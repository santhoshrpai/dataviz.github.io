{

"totalTopics":"10",


"totalLines":"51",

"topics":[{"topicID":1,"strength":88,"labels":[
{"phrase":"Android","prob":0.2},
{"phrase":"mobile","prob":0.2},
{"phrase":" Java","prob":0.2},
{"phrase":"google","prob":0.2},
      {"phrase":"https","prob":0.2}]},

{"topicID":2,"strength":7,"labels":[{"phrase":"Loops","prob":0.367826820806},{"phrase":"function","prob":0.261955329161},{"phrase":"While","prob":0.260373021189},{"phrase":"init","prob":0.0552599084916},{"phrase":"remove","prob":0.0545849203523}]},
{"topicID":3,"strength":8,"labels":[{"phrase":"Servlets","prob":0.170144009755},{"phrase":"Tomcat","prob":0.170144009755},{"phrase":"Deploy","prob":0.164927995123},{"phrase":"invoke","prob":0.164927995123},{"phrase":"Util","prob":0.164927995123},{"phrase":"Apache","prob":0.164927995123}]},
{"topicID":4,"strength":80,"labels":[{"phrase":"Multithreading","prob":0.166666666667},{"phrase":"write","prob":0.166666666667},{"phrase":"produce","prob":0.166666666667},{"phrase":"complete","prob":0.166666666667},{"phrase":"execute","prob":0.166666666667},{"phrase":"single","prob":0.166666666667}]},
{"topicID":5,"strength":20,"labels":[{"phrase":"Variables","prob":0.2},{"phrase":"size","prob":0.2},{"phrase":"number","prob":0.2},{"phrase":"character","prob":0.2},{"phrase":"byte","prob":0.2}]},
{"topicID":6,"strength":7,"labels":[{"phrase":"JSON","prob":0.256645974355},{"phrase":"document","prob":0.186113412342},{"phrase":"parse","prob":0.186113412342},{"phrase":"null","prob":0.186034061379},{"phrase":"Annotation","prob":0.185093139582}]},
{"topicID":7,"strength":7,"labels":[{"phrase":"Inheritance","prob":0.409374558127},{"phrase":"implement","prob":0.409374558127},{"phrase":"interface","prob":0.0604169612483},{"phrase":"subclass","prob":0.0604169612483},{"phrase":"instantiate","prob":0.0604169612483}]},
{"topicID":8,"strength":17,"labels":[{"phrase":"Database","prob":0.227184151917},{"phrase":"table","prob":0.227184151917},{"phrase":"store","prob":0.227184151917},{"phrase":"save","prob":0.225811891145},{"phrase":"save","prob":0.0926356531028}]},
{"topicID":9,"strength":10,"labels":[{"phrase":"Eclipse","prob":0.298665400282},{"phrase":"library","prob":0.17533364993},{"phrase":"include","prob":0.17533364993},{"phrase":"maven","prob":0.17533364993},{"phrase":"source","prob":0.17533364993}]},
{"topicID":10,"strength":8,"labels":[{"phrase":"Network","prob":0.267809809442},{"phrase":"page","prob":0.218946001648},{"phrase":"response","prob":0.218946001648},{"phrase":"browse","prob":0.177124396867},{"phrase":"move","prob":0.117173790394}]}],

"author":"blank",
"date":"Time is here",
"author":"user",
"title":"StackOverFlow Java",
"sent": [
         {"linePolarity":0,"systemtopicid":1, "systemlabel":"Android", "topicid":1, "goldlabel":"This is the header","sent":"Hierarchy of the Q&A","sentimentwords":[]}]
,"children":[
  {
    "author": "gnasher729",
    "parent": "",
    "title": "\"Why does int i = 1024 * 1024 * 1024 * 1024 compile without error?\"",
    "commentid": "q_96439",
    "date": "Jul 10, 2014 5:17:41 AM",
    "children": [
      {
        "date": "Jul 10, 2014 5:25:18 AM",
        "commentid": "a_118325",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8559999999999999,
            "sent": " there's nothing wrong with that statement; you're just multiplying 4 numbers and assigning it to an int  there just happens to be an overflow. this is different than assigning a single  literal   which would be bounds-checked at compile-time.    it is the out-of-bounds  literal  that causes the error  not the  assignment :         by contrast a   literal would compile fine:         note that  in fact  the result  is  still computed at compile-time because   is a   constant expression  :         becomes:         notice that the result ( ) is simply loaded and stored  and no multiplication takes place.       from  jls ?3.10.1  (thanks to @chrisk for bringing it up in the comments):         it is a compile-time error if a decimal literal of type   is larger than   (2 31 )  or if the decimal literal   appears anywhere other than as the operand of the unary minus operator ( ?15.15.4 ).     "
          }
        ],
        "parent": "q_96439",
        "author": "arshajii"
      },
      {
        "date": "Jul 11, 2014 6:43:13 AM",
        "commentid": "a_118326",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8559999999999999,
            "sent": "   and   do not have the same value in java.     actually     isn't even a value (although   is) in java. the compiler literally does not know what it is  or how to use it. so it whines.      is a valid int in java  and a valid   multiplied by another valid    is always a valid  . even if it's not the same value that you would intuitively expect because the calculation will overflow.    example    consider the following code sample:         would you expect this to generate a compile error? it becomes a little more slippery now.  what if we put a loop with 3 iterations and multiplied in the loop?     the compiler is allowed to optimize  but it can't change the behaviour of the program while it's doing so.         some info on how this case is actually handled:     in java and many other languages  integers will consist of a fixed number of bits.   calculations that don't fit in the given number of bits will  overflow ; the calculation is basically performed  modulus  2^32 in java  after which the value is converted back into a  signed  integer.     other languages or api's use a dynamic number of bits (  in java)  raise an exception or set the value to a magic value such as not-a-number.  "
          }
        ],
        "parent": "q_96439",
        "author": "Cruncher"
      },
      {
        "date": "Jul 10, 2014 9:24:15 AM",
        "commentid": "a_118327",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8559999999999999,
            "sent": "     i have no idea why the second variant produces no error.      the behaviour that you suggest -- that is   the production of diagnostic message when a computation produces a value that is larger than the largest value that can be stored in an integer  -- is a  feature . for you to use any feature  the feature must be thought of  considered to be a good idea  designed  specified  implemented  tested  documented and shipped to users.    for java  one or more of the things on that list did not happen  and therefore you don't have the feature. i don't know which one; you'd have to ask a java designer.    for c#  all of those things did happen -- about fourteen years ago now -- and so the corresponding program in c# has produced an error since c# 1.0.  "
          }
        ],
        "parent": "q_96439",
        "author": "Eric Lippert"
      },
      {
        "date": "Jul 10, 2014 11:35:26 PM",
        "commentid": "a_118328",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8559999999999999,
            "sent": " in addition to arshajii's answer i want to show one more thing:    it is not the  assignment  that causes the error but simply the use of the  literal . when you try         you'll notice it also causes a compile-error since the right hand side still is an  -literal and out of range.    so operations with  -values (and that's including assignments) may overflow without a compile-error (and without a runtime-error as well)  but the compiler just can't handle those too-large literals.  "
          }
        ],
        "parent": "q_96439",
        "author": "piet.t"
      },
      {
        "date": "Jul 11, 2014 9:34:17 AM",
        "commentid": "a_118329",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8559999999999999,
            "sent": "  a:  because it is not an error.      background:  the multiplication   will lead to an overflow. an overflow is very often a bug. different programming languages produce different behavior when overflows happen. for example  c and c++ call it \\\"undefined behavior\\\" for signed integers  and the behavior is defined unsigned integers (take the mathematical result  add   as long as the result is negative  subtract   as long as the result is greater than  ).     in the case of java  if the result of an operation with   values is not in the allowed range  conceptually java adds or subtracts 2^32 until the result is in the allowed range. so the statement is completely legal and not in error. it just doesn't produce the result that you may have hoped for.    you can surely argue whether this behavior is helpful  and whether the compiler should give you a warning. i'd say personally that a warning would be very useful  but an error would be incorrect since it is legal java.  "
          }
        ],
        "parent": "q_96439",
        "author": "gnasher729"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.8559999999999999,
        "sent": " the limit of   is from -2147483648 to 2147483647.    if i input          then eclipse will prompt a red underline under \\\"2147483648\\\".    but if i do this:         it will compile fine.         maybe it's a basic question in java  but i have no idea why the second variant produces no error.  "
      }
    ]
  },
  {
    "author": "ajb",
    "parent": "",
    "title": "\"Why does this random value have a 25/75 distribution instead of 50/50?\"",
    "commentid": "q_2695",
    "date": "Dec 23, 2014 10:57:41 AM",
    "children": [
      {
        "date": "Dec 23, 2014 11:05:37 AM",
        "commentid": "a_3249",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9079999999999999,
            "sent": " because nextdouble works like this: ( source )           makes   random bits.    now why does this matter? because about half the numbers generated by the first part (before the division) are less than    and therefore their significand doesn't entirely fill the 53 bits that it could fill  meaning the least significant bit of the significand is always zero for those.       because of the amount of attention this is receiving  here's some extra explanation of what a   in java (and many other languages) really looks like and why it mattered in this question.    basically  a   looks like this: ( source )         a very important detail not visible in this picture is that numbers are \\\"normalized\\\" 1  such that the 53 bit fraction starts with a 1 (by choosing the exponent such that it is so)  that 1 is then omitted. that is why the picture shows 52 bits for the fraction (significand) but there are effectively 53 bits in it.    the normalization means that if in the code for   the 53rd bit is set  that bit is the implicit leading 1 and it goes away  and the other 52 bits are copied literally to the significand of the resulting  . if that bit is not set however  the remaining bits must be shifted left until it becomes set.    on average  half the generated numbers fall into the case where the significand was  not  shifted left at all (and about half those have a 0 as their least significant bit)  and the other half is shifted by at least 1 (or is just completely zero) so their least significant bit is always 0.    1: not always  clearly it cannot be done for zero  which has no highest 1. these numbers are called denormal or subnormal numbers  see  wikipedia:denormal number .  "
          }
        ],
        "parent": "q_2695",
        "author": "harold"
      },
      {
        "date": "Dec 23, 2014 11:10:37 AM",
        "commentid": "a_3250",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9079999999999999,
            "sent": " from the  docs :        the method nextdouble is implemented by class random as if by:           but it also states the following (emphasis mine):        [in early versions of java  the result was incorrectly calculated as:             this might seem to be equivalent  if not better  but in fact it introduced a large nonuniformity because of the bias in the rounding of floating-point numbers:  it was three times as likely that the low-order bit of the significand would be 0 than that it would be 1 ! this nonuniformity probably doesn't matter much in practice  but we strive for perfection.]      this note has been there since java 5 at least (docs for java &lt;= 1.4 are behind a loginwall  too lazy to check). this is interesting  because the problem apparently still exists even in java 8. perhaps the \\\"fixed\\\" version was never tested?  "
          }
        ],
        "parent": "q_2695",
        "author": "Thomas"
      },
      {
        "date": "Dec 23, 2014 11:28:25 AM",
        "commentid": "a_3251",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9079999999999999,
            "sent": " this result doesn't surprise me given how floating-point numbers are represented.  let's suppose we had a very short floating-point type with only 4 bits of precision.  if we were to generate a random number between 0 and 1  distributed uniformly  there would be 16 possible values:         if that's how they looked in the machine  you could test the low-order bit to get a 50/50 distribution.  however  ieee floats are represented as a power of 2 times a mantissa; one field in the float is the power of 2 (plus a fixed offset).  the power of 2 is selected so that the \\\"mantissa\\\" part is always a number >= 1.0 and &lt; 2.0.  this means that  in effect  the numbers other than   would be represented like this:         (the   before the binary point is an implied value; for 32- and 64-bit floats  no bit is actually allocated to hold this  .)    but looking at the above should demonstrate why  if you convert the representation to bits and look at the low bit  you will get zero 75% of the time.  this is due to all values less than 0.5 (binary  )  which is half the possible values  having their mantissas shifted over  causing 0 to appear in the low bit.  the situation is essentially the same when the mantissa has 52 bits (not including the implied 1) as a   does.     (actually  as @sneftel suggested in a comment  we  could  include more than 16 possible values in the distribution  by generating:         but i'm not sure it's the kind of distribution most programmers would expect  so it probably isn't worthwhile.  plus it doesn't gain you much when the values are used to generate integers  as random floating-point values often are.)  "
          }
        ],
        "parent": "q_2695",
        "author": "ajb"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.9079999999999999,
        "sent": "  edit:  so basically what i'm trying to write is a 1 bit hash for  .    i want to map a   to   or   with a 50/50 chance. for that i wrote code that picks some random numbers  (just as an example  i want to use this on data with regularities and still get a 50/50 result)   checks their last bit and increments   if it is 1  or   if it is 0.     however  this code constantly results in 25%   and 75%  . why is it not 50/50? and why such a weird  but straight-forward (1/3) distribution?         example output:       "
      }
    ]
  },
  {
    "author": "La-comadreja",
    "parent": "",
    "title": "\"Why does Java think that the product of all numbers from 10 to 99 is 0?\"",
    "commentid": "q_44097",
    "date": "Oct 14, 2014 11:34:01 PM",
    "children": [
      {
        "date": "Oct 15, 2014 2:10:03 AM",
        "commentid": "a_53342",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " here is what the program does at each step:         note that numbers change their sign in an unexpected manner indicating that there is an integer overflow. also note that java does not clamp the result of an overflow to some min/max value.    keeping in mind that   data type  is a 32-bit signed    two's complement  integer  here is an explanation of each operation:           is the actual decimal result   is the internal representation of the decimal result had we used a bigger integer;   will store the lower 32 bits   is the result represented by the two's complement representation      if you are wondering where the 0 comes from  look closely at the binary representation of numbers above. you will notice that:      multiplying any number by an even number results in an even number   multiplying an even number by an even number shifts the bits towards left and adds zero bits towards right   multiplying an even number by an odd number does not change the number of rightmost zero bits      repeat the multiplication enough times and the number will zero out starting from the right.  "
          }
        ],
        "parent": "q_44097",
        "author": "Salman A"
      },
      {
        "date": "Oct 15, 2014 2:32:02 PM",
        "commentid": "a_53343",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " computer multiplication is really happening modulo 2^32. once you have accumulated enough powers of two in the multiplicand  then all values will be 0.    here we have all the even numbers in the series  along with the maximum power of two that divides the number  and the cumulative power of two         the product up to 42 is equal to x * 2^32 = 0 (mod 2^32).  the sequence of the powers of two is related to gray codes (among other things)  and appears as  https://oeis.org/a001511 .    edit: to see why other responses to this question are incomplete  consider the fact that the same program  restricted to odd integers only  would  not  converge to 0  despite all the overflowing.  "
          }
        ],
        "parent": "q_44097",
        "author": "user295691"
      },
      {
        "date": "Oct 14, 2014 11:36:11 PM",
        "commentid": "a_53344",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " it looks like an  integer overflow .    take a look at this         output:         output no longer be a   value. then you will get wrong value because of the overflow.         if it overflows  it goes back to the minimum value and continues from   there. if it underflows  it goes back to the maximum value and   continues from there.      more  info      edit .     let's change your code as follows         out put:       "
          }
        ],
        "parent": "q_44097",
        "author": "Ruchira Gayan Ranaweera"
      },
      {
        "date": "Oct 15, 2014 12:49:34 PM",
        "commentid": "a_53345",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " it's because of integer overflow. when you multiply many even numbers together  the binary number gets a lot of trailing zeroes. when you have over 32 trailing zeroes for an    it rolls over to  .    to help you visualize this  here are the multiplications in hex calculated on a number type that won't overflow. see how the trailing zeroes slowly grow  and note that an   is made up of the last 8 hex-digits. after multiplying by 42 (0x2a)  all 32 bits of an   are zeroes!       "
          }
        ],
        "parent": "q_44097",
        "author": "Tim S."
      },
      {
        "date": "Oct 14, 2014 11:38:31 PM",
        "commentid": "a_53346",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " somewhere in the middle you get   as the product. so  your entire product will be 0.    in your case :         every time you multiply the current value of   with the number you get   as output.  "
          }
        ],
        "parent": "q_44097",
        "author": "TheLostMind"
      },
      {
        "date": "Oct 15, 2014 5:13:18 AM",
        "commentid": "a_53347",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " since many of the existing answer point to implementation details of java and debug output  lets have a look at the math behind binary multiplication to really answer the why.    the comment of @kasperd goes in the right direction. suppose you do not multiply directly with the number but with the prime factors of that number instead. than a lot of numbers will have 2 as a prime factor. in binary this is equal to a left shift. by commutativity we can multiply with prime factors of 2 first. that means we just do a left shift.    when having a look at binary multiplication rules  the only case where a 1 will result in a specific digit position is when both operand values are one.    so the effect of a left shift is that the lowest bit position of a 1 when further multiplying the result is increased.    since integer contains only the lowest order bits  they all will be set to 0 when the prime factor 2 is cotnained often enough in the result.    note that two's complement representation is not of interest for this analysis  since the sign of the multiplication result can be computed independently from the resulting number. that means if the value overflows and becomes negative  the lowest order bits are represented as 1  but during multiplication they are treated again as being 0.  "
          }
        ],
        "parent": "q_44097",
        "author": "SpaceTrucker"
      },
      {
        "date": "Oct 14, 2014 11:38:41 PM",
        "commentid": "a_53348",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " if i run this code what i get all -            integer overflow cause -         produce 0 cause -       "
          }
        ],
        "parent": "q_44097",
        "author": "Subhrajyoti Majumder"
      },
      {
        "date": "Oct 15, 2014 9:44:26 AM",
        "commentid": "a_53349",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " eventually  the calculation overflows  and eventually that overflow leads to a product of zero; that happens when   and  .  try this code out to verify it for yourself (or run the code  here ):         once it's zero  it of course stays zero.  here's some code that will produce a more accurate result (you can run the code  here ):       "
          }
        ],
        "parent": "q_44097",
        "author": "threed"
      },
      {
        "date": "Oct 16, 2014 8:01:06 AM",
        "commentid": "a_53350",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " it is an integer overflow.    the int data type is 4 bytes  or 32 bits.  therefore  numbers larger than 2^(32 - 1) - 1 (2 147 483 647) cannot be stored in this data type.  your numerical values will be incorrect.    for very large numbers  you will want to import and use the class           note: for numerical values that are still too large for the int data type  but small enough to fit within 8 bytes (absolute value less than or equal to 2^(64 - 1) - 1)  you should probably use the   primitive.      hackerrank's practice problems (www.hackerrank.com)  such as the algorithms practice section  ( https://www.hackerrank.com/domains/algorithms/warmup ) include some very good large-number questions that give good practice about how to think about the appropriate data type to use.  "
          }
        ],
        "parent": "q_44097",
        "author": "La-comadreja"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.6680000000000001,
        "sent": " the following block of codes gives the output as 0.         please can somebody explain why this happens?  "
      }
    ]
  },
  {
    "author": "EJP",
    "parent": "",
    "title": "\"Why is Cloneable not deprecated?\"",
    "commentid": "q_43390",
    "date": "Oct 16, 2014 12:48:42 AM",
    "children": [
      {
        "date": "Oct 16, 2014 4:31:07 AM",
        "commentid": "a_52450",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.5880000000000001,
            "sent": " there is a  bug  submitted in 1997 to  java bug database  about adding   method to    so it would no longer be useless. it was closed with resolution \\\"won't fix\\\" and justification was as follows:        sun's technical review committee (trc) considered this issue at length   and recommended against taking  any action other than improving the   documentation of the current cloneable interface .  here is the full   text of the recommendation:        the existing java object cloning apis are problematic.  there is a   protected \\\"clone\\\" method on java.lang.object and there is an interface   java.lang.cloneable.  the intention is that if a class wants to allow   other people to clone it  then it should support the cloneable   interface and override the default protected clone method with a   public clone method.  unfortunately  for reasons conveniently lost in   the mists of time  the cloneable interface does not define a clone   method.        this combination results in a fair amount of confusion.  some classes   claim to support cloneable  but accidentally forget to support the   clone method.  developers are confused about how cloneable is supposed   to work and what clone is supposed to do.        unfortunately  adding a \\\"clone\\\" method to cloneable would be an   incompatible change.  it won't break binary compatibility  but it will   break source compatibility.  anecdotal evidence suggests that in   practice there are a number of cases where classes support the   cloneable interface but fail to provide a public clone method.  after   discussion  trc unanimously recommended that we should not modify the   existing cloneable interface  because of the compatibility impact.        an alternative proposal was to add a new interface   java.lang.publiclycloneable to reflect the original intended purpose   of cloneable.  by a 5 to 2 majority  trc recommended against this.    the main concern was that this would add yet more confusion (including   spelling confusion!) to an already confused picture.         trc unanimously recommended that we should add additional   documentation to the existing cloneable interface  to better describe   how it is intended to be used and to describe \\\"best practices\\\" for   implementors.      so  although this is not directly about  deprecated   the reason for not making cloneable \\\"deprecated\\\" is that technical review comitee decided that  modifying existing documentation will be sufficient enough  to make this interface useful. and so they did. until java 1.4    was documented as follows:        a class implements the cloneable interface to indicate to the   object.clone() method that it is legal for that method to make a   field-for-field copy of instances of that class.         attempts to clone instances that do not implement the cloneable   interface result in the exception clonenotsupportedexception being   thrown.         the interface cloneable declares no methods.      since java 1.4 (which was released in february 2002) up to current edition (java 8) it looks like this:        a class implements the cloneable interface to indicate to the   object.clone() method that it is legal for that method to make a   field-for-field copy of instances of that class.  invoking object's   clone method on an instance that does not implement the cloneable   interface results in the exception clonenotsupportedexception being   thrown.         by convention  classes that implement this interface should override   object.clone (which is protected) with a public method. see   object.clone() for details on overriding this method.         note that this interface does not contain the clone method. therefore    it is not possible to clone an object merely by virtue of the fact   that it implements this interface. even if the clone method is invoked   reflectively  there is no guarantee that it will succeed.    "
          }
        ],
        "parent": "q_43390",
        "author": "Kao"
      },
      {
        "date": "Oct 16, 2014 4:30:52 PM",
        "commentid": "a_52451",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.5880000000000001,
            "sent": " the short answer to \\\"why isn't   deprecated?\\\" (or indeed  why isn't   deprecated  for any  ) is that there hasn't been much attention paid to deprecating them.    most things that have been deprecated recently were deprecated because there is a specific plan to remove them. for example  the   and   methods of  logmanager  were  deprecated in java se 8  with the intention of removing them in java se 9. (the reason is that they unnecessarily complicated module interdependencies.) indeed  these apis have already been  removed from early jdk 9  development builds. (note that similar property change listener calls were also removed from  ; see  jdk-8029806 .)    no such similar plan exists to for   and  .    a longer answer would involve discussing further questions  such as what one might expect to happen to these apis  what costs or benefits would accrue the platform if they were deprecated  and what is being communicated to developers when an api is deprecated. i explored this topic in my recent javaone talk   debt and deprecation . (slides available at that link; video forthcoming.) it turns out that the jdk itself hasn't been very consistent in its usage of deprecation. it's been used to mean several different things  including for example        this is dangerous and you should be aware of the risks of using it (example:      and  ).     this is going to be removed in a future release    this is obsolete and it's a good idea for you to use something different (example: many of the methods in  )      all of these are distinct meanings  and different subsets of them apply to different things that are deprecated. and some subset of them apply to things that aren't deprecated (but that maybe should be deprecated).      and   are \\\"broken\\\" in the sense that they have design flaws and are difficult to use correctly. however    is still the best way to copy arrays  and cloning has some limited usefulness to make copies of instances of classes that are carefully implemented. removing cloning would be an incompatible change that would break a lot of things. a cloning operation could be reimplemented a different way  but it would probably be slower than  .    however  for most things a copy constructor is preferable to cloning. so perhaps marking   as \\\"obsolete\\\" or \\\"superseded\\\" or something similar would be appropriate. this would tell developers that they probably want to look elsewhere  but it would not signal that the cloning mechanism might be removed in a future release. unfortunately  no such marker exists.    as things stand  \\\"deprecation\\\" seems to imply eventual removal -- despite the fact that a vanishingly small number of deprecated features have ever been removed -- and so deprecation doesn't seem warranted for the cloning mechanism. perhaps in the future an alternative marking can be applied that directs developers to use alternative mechanisms instead.     update     i've added some additional history to the  bug report . frank yellin  an early jvm implementor and co-author of the jvm specification  made some comments in response to the \\\"lost in the mists of time\\\" comment in the trc recommendation quoted in the  other answer . i've quoted the relevant portions here; the full message is in the bug report.        cloneable has no methods for the same reason that serializable doesn't. cloneable indicates a property of the class  rather than specifically saying anything about the methods that the class supported.        prior to reflection  we needed a native method to make a shallow copy of an object.  hence object.clone() was born.  it was also clear that many classes would want to override this method  and that not every class would want to be cloned.  hence cloneable was born to indicate the programmer's intention.        so  in short.  the purpose of cloneable was not to indicate that you had a public clone() method.  it was to indicate that you were willing to be cloned using object.clone()  and it was up to the implementation to decide whether or not to make clone() public.    "
          }
        ],
        "parent": "q_43390",
        "author": "Stuart Marks"
      },
      {
        "date": "Oct 16, 2014 1:49:53 AM",
        "commentid": "a_52452",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.5880000000000001,
            "sent": "     why it is not deprecated yet?      because the jcp hasn't seen fit to do so  and may never do so. ask them. you're asking in the wrong place.        what are the reasons behind keeping this thing in java api      no-one will ever remove anything from the java api  because of the backwards compatibility requirement. the last time that happened was the change in the awt event model between 1.0 and 1.1 in 1996/7.  "
          }
        ],
        "parent": "q_43390",
        "author": "EJP"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.5880000000000001,
        "sent": " it is commonly understood that   interface in java is broken. there are many reasons for this  which i will not mention;  others  already did it. it is also the position of  java architects  themselves.    my question is therefore: why has is not been deprecated yet? if the core java team have decided that it is broken  then they must also have considered deprecation. what are their reasons against doing so (in java 8 it is  still not deprecated )?   "
      }
    ]
  },
  {
    "author": "maaartinus",
    "parent": "",
    "title": "\"Why does the Java API use int instead of short or byte?\"",
    "commentid": "q_19173",
    "date": "Nov 25, 2014 2:11:41 AM",
    "children": [
      {
        "date": "Nov 25, 2014 2:44:29 AM",
        "commentid": "a_23269",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.28400000000000025,
            "sent": " some of the reasons have already been pointed out. for example  the fact that   \\\"...(almost) all operations on byte  short will promote these primitives to int\\\"  . however  the obvious next question would be:  why  are these types promoted to  ?    so to go one level deeper: the answer may simply be related to the java virtual machine instruction set. as summarized in the  table in the java virtual machine specification    all  integral arithmetic operations  like adding  dividing and others  are only available for the type   and the type    and  not  for the smaller types.      (an aside: the smaller types (  and  ) are basically only intended for  arrays . an  array  like   will take 1000 bytes  and an array like   will take 4000 bytes)      now  of course  one could say that  \\\"...the obvious next question would be:  why  are these instructions only offered for   (and  )?\\\" .     one reason is mentioned in the jvm spec mentioned above:        if each typed instruction supported all of the java virtual machine's run-time data types  there would be more instructions than could be represented in a byte      additionally  the java virtual machine can be considered as an abstraction of a real processor. and introducing dedicated  arithmetic logic unit  for smaller types would not be worth the effort: it would need additional transistors  but it still could only execute one addition in one clock cycle. the dominant architecture when the jvm was designed was 32bits  just right for a 32bit  . (the operations that involve a 64bit   value are implemented as a special case).       (note: the last paragraph is a bit oversimplified  considering possible vectorization etc.  but should give the basic idea without diving too deep into processor design topics)        edit: a short addendum  focussing on the example from the question  but in an more general sense: one could also ask whether it would not be beneficial to store  fields  using the smaller types. for example  one might think that memory could be saved by storing   as a  . but here  the java class file format comes into play: all the  fields in a class file  occupy at least one \\\"slot\\\"  which has the size of one   (32 bits). (the \\\"wide\\\" fields    and    occupy two slots). so explicitly declaring a field as   or   would not save any memory either.   "
          }
        ],
        "parent": "q_19173",
        "author": "Marco13"
      },
      {
        "date": "Nov 25, 2014 2:23:16 AM",
        "commentid": "a_23270",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.28400000000000025,
            "sent": " (almost) all operations on      will promote these primitives to    for example  you cannot write:         arithmetics are easier and straightforward when using    no need to cast.     in terms of space  it makes a  very  little difference.   and   would complicate things  i don't think this micro optimization worth it since we are talking about a fixed amount of variables.    i think that   is relevant and useful when you program for embedded devices or dealing with files/networks. also these primitives are limited  what if the calculations might exceed their limits in the future? try to think about an extension for   class that might evolve bigger numbers.     also note that in a 64-bit processors  locals will be saved in registers and won't use any resources  so using      and other primitives won't make any difference at all. moreover  many java implementations align variables *  (and objects).         *    and   occupy the same space as   if they are  local  variables   class  variables or even  instance  variables. why? because in (most) computer systems  variables  addresses are  aligned   so for example if you use a single byte  you'll actually end up with two bytes - one for the variable itself and another for the padding.    on the other hand  in arrays    take 1 byte    take 2 bytes and   take four bytes  because in arrays only the start and maybe the end of it has to be aligned. this will make a difference in case you want to use  for example     then you'll really note a performance difference.  "
          }
        ],
        "parent": "q_19173",
        "author": "Maroun Maroun"
      },
      {
        "date": "Nov 25, 2014 2:25:29 AM",
        "commentid": "a_23271",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.28400000000000025,
            "sent": " because arithmetic operations are easier when using integers compared to shorts. assume that the constants were indeed modeled by   values. then you would have to use the api in this manner:         notice the explicit casting. short values are implicitly promoted to   values when they are used in arithmetic operations. (on the operand stack  shorts are even expressed as ints.) this would be quite cumbersome to use which is why   values are often preferred for constants.    compared to that  the gain in storage efficiency is minimal because there only exists a fixed number of such constants. we are talking about 40 constants. changing their storage from   to   would safe you  . see  this answer  for further reference.  "
          }
        ],
        "parent": "q_19173",
        "author": "Rafael Winterhalter"
      },
      {
        "date": "Nov 25, 2014 9:27:58 AM",
        "commentid": "a_23272",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.28400000000000025,
            "sent": " if you used the philosophy where integral constants are stored in the smallest type that they fit in  then java would have a serious problem: whenever programmers write code using integral constants  they have to pay careful attention to their code to check if the type of the constants matter  and if so look up the type in the documentation and/or do whatever type conversions are needed.    so now that we've outlined a serious problem  what benefits could you hope to achieve with that philosophy? i would be unsurprised if the  only  runtime-observable effect of that change would be what type you get when you look the constant up via reflection. (and  of course  whatever errors are introduced by lazy/unwitting programmers not correctly accounting for the types of the constants)    weighing the pros and the cons is very easy: it's a bad philosophy.  "
          }
        ],
        "parent": "q_19173",
        "author": "Hurkyl"
      },
      {
        "date": "Nov 25, 2014 12:25:03 PM",
        "commentid": "a_23273",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.28400000000000025,
            "sent": " the design complexity of a virtual machine is a function of how many kinds of operations it can perform.  it's easier to having four implementations of an instruction like \\\"multiply\\\"--one each for 32-bit integer  64-bit integer  32-bit floating-point  and 64-bit floating-point--than to have  in addition to the above  versions for the smaller numerical types as well.  a more interesting design question is why there should be four types  rather than fewer (performing all integer computations with 64-bit integers and/or doing all floating-point computations with 64-bit floating-point values).  the reason for using 32-bit integers is that java was expected to run on many platforms where 32-bit types could be acted upon just as quickly as 16-bit or 8-bit types  but operations on 64-bit types would be noticeably slower.  even on platforms where 16-bit types would be faster to work with  the extra cost of working with 32-bit quantities would be offset by the simplicity afforded by  only  having 32-bit types.    as for performing floating-point computations on 32-bit values  the advantages are a bit less clear.  there are some platforms where a computation like   could be performed most quickly by converting all operands to a higher-precision type  adding them  and then converting the result back to a 32-bit floating-point number for storage.  there are other platforms where it would be more efficient to perform all computations using 32-bit floating-point values.  the creators of java decided that all platforms should be required to do things the same way  and that they should favor the hardware platforms for which 32-bit floating-point computations are faster than longer ones  even though this severely degraded pc both the speed and precision of floating-point math on a typical pc  as well as on many machines without floating-point units.  note  btw  that depending upon the values of b  c  and d  using higher-precision intermediate computations when computing expressions like the aforementioned   will sometimes yield results which are significantly more accurate than would be achieved of all intermediate operands were computed at   precision  but will sometimes yield a value which is a tiny bit less accurate.  in any case  sun decided everything should be done the same way  and they opted for using minimal-precision   values.    note that the primary advantages of smaller data types become apparent when large numbers of them are stored together in an array; even if there were no advantage to having individual variables of types smaller than 64-bits  it's worthwhile to have arrays which can store smaller values more compactly; having a local variable be a   rather than an   saves seven bytes; having an array of 1 000 000 numbers hold each number as a   rather than a   waves 7 000 000 bytes.  since each array type only needs to support a few operations (most notably read one item  store one item  copy a range of items within an array  or copy a range of items from one array to another)  the added complexity of having more array types is not as severe as the complexity of having more types of directly-usable discrete numerical values.  "
          }
        ],
        "parent": "q_19173",
        "author": "supercat"
      },
      {
        "date": "Nov 28, 2014 4:48:16 AM",
        "commentid": "a_23274",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.28400000000000025,
            "sent": " actually  there'd be a small advantage. if you have a         then on a typical jvm it needs as much space as a class containing a single  . the memory consumption gets rounded to a next multiple of 8 or 16 bytes (iirc  that's configurable)  so the cases when there are real saving are rather rare.    this class would be slightly easier to use if the corresponding   methods returned a  . but there are no such   methods  only   which must returns an   because of other fields. each operation on smaller types promotes to    so you need a lot of casting.    most probably  you'll either give up and switch to an   or write setters like         then the type of   doesn't matter  anyway.  "
          }
        ],
        "parent": "q_19173",
        "author": "maaartinus"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": 0.28400000000000025,
        "sent": " why does the  java api  use   if   or even   would be sufficient?    example: the     field in calendar uses  .     edit:     aspects of interest:      storage   calculations      if the difference is too minimal  then why do these datatypes (short  int) exist at all? legacy?  "
      }
    ]
  },
  {
    "author": "Ira Baxter",
    "parent": "",
    "title": "\"What makes Java easier to parse than C?\"",
    "commentid": "q_45564",
    "date": "Oct 12, 2014 2:58:53 PM",
    "children": [
      {
        "date": "Oct 12, 2014 3:41:07 PM",
        "commentid": "a_55147",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.031999999999999806,
            "sent": "  parsing c++ is getting hard.  parsing java is getting to be just as hard.     see this  so answer discussing why c (and c++) is \\\"hard\\\" to parse .  the short summary is that c and c++  grammars  are inherently ambiguous; they will give you multiple parses and you  must  use context to resolve the ambiguities. people then make the mistake of assuming you have to resolve ambiguities as you  parse; not so  see below.  if you insist on resolving ambiguities as you parse  your parser gets more complicated and that much harder to build; but that complexity is a self-inflicted wound.    iirc   java 1.4's \\\"obvious\\\" lalr(1) grammar was not ambiguous  so it was \\\"easy\\\" to parse.  i'm not so sure that modern java hasn't got at least long distance local ambiguities; there's always the problem of deciding whether \\\"...>>\\\" closes off two templates or is a \\\"right shift operator\\\".  i suspect  modern java does not parse with lalr(1) anymore .     but one can get past the parsing problem by using strong parsers (or weak parsers and context collection hacks as c and c++ front ends mostly do now)  for both languages. c and c++ have the additional complication of having a preprocessor; these are more complicated in practice than they look.  one claim is that the c and c++ parsers are so hard they have to be be written by hand.    it isn't true; you can build java and c++ parsers just fine with glr parser generators.      but parsing isn't really where the problem is.      once you parse  you will want to do something with the ast/parse tree.  in practice  you need to know  for every identifier  what its definition is and where it is used  (\\\"name and type resolution\\\"  sloppily  building symbol tables).   this turns out to be a lot more work than getting the parser right  compounded  by inheritance  interfaces  overloading and templates  and the confounded by the fact that the semantics for all this is written in informal natural language spread across tens to hundreds of pages of the language standard.  c++ is really bad here.  java 7 and 8 are getting to be pretty awful from this point of view. (and symbol tables aren't all you need; see my bio for a longer essay on \\\"life after parsing\\\").    most folks struggle with the pure parsing part (often never finishing; check so itself for the many  many questions about to how to build working parsers for real langauges)  so they don't ever see life after parsing.  and then we get folk theorems about what is hard to parse and no signal about what happens after that stage.     fixing c++ syntax won't get you anywhere.     regarding changing the c++ syntax: you'll find you need to patch a lot of places to take care of the variety of local and real ambiguities in any c++ grammar.  if you insist  the  following list might be a good starting place .   i contend there is no point in doing this if you are not the c++ standards committee; if you did so  and built a compiler using that  nobody sane would use it.   there's too much invested in existing c++ applications to switch for convenience of the guys building parsers; besides  their pain is over and existing parsers work fine.    you may want to write your own parser.  ok  that's fine; just don't expect the rest of the community to let you change the language they must use to make it easier for you.  they all want it easier for them  and that's to use the language as documented and implemented.  "
          }
        ],
        "parent": "q_45564",
        "author": "Ira Baxter"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.031999999999999806,
        "sent": " i'm acquainted with the fact that  c and c++ are context-sensitive   and in particular you need a \\\"lexer hack\\\" in c. on the other hand  i'm under the impression that you can parse java with only 2 tokens of look-ahead  despite considerable similarity between the two languages.     what would you have to change about c to make it more tractable to parse?     i ask because all of the examples i've seen of c's context-sensitivity are technically allowable but awfully weird. for example          could be calling the void function   with argument  . or  it could be declaring   to be an object of type    but you could just as easily get rid of the parantheses. in part  this weirdness occurs because the \\\"direct declarator\\\" production rule for the  c grammar  fulfills the dual purpose of declaring both functions and variables.    on the other hand  the  java grammar  has separate production rules for variable declaration and function declaration. if you write         then you know it's a variable declaration and   can unambiguously be parsed as a typename. this might not be valid code if the class   hasn't been defined somewhere in the current scope  but that's a job for semantic analysis that can be performed in a later compiler pass.    i've seen it said that c is hard to parse because of typedef  but you can declare your own types in java too. which c grammar rules  besides    are at fault?  "
      }
    ]
  },
  {
    "author": "Tarun Varshney",
    "parent": "",
    "title": "\"Why doesn&#39;t RecyclerView have onItemClickListener()? and How RecyclerView is different from Listview?\"",
    "commentid": "q_89996",
    "date": "Jul 22, 2014 3:43:39 AM",
    "children": [
      {
        "date": "Jul 24, 2014 5:00:40 AM",
        "commentid": "a_110350",
        "sent": [
          {
            "topicid": 1,
            "systemtopicid": 1,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Android",
            "linePolarity": 1.04,
            "sent": " since the introduction of listview  onitemclicklistener has been problematic. the moment you have a click listener for any of the internal elements the callback would not be triggered but it wasn't notified or well documented (if at all) so there was a lot of confusion and so questions about it.    given that recyclerview takes it a step further and doesn't have a concept of a row/column  but rather an arbitrarily laid out amount of children  they have delegated the onclick to each one of them  or to programmer implementation.    think of recyclerview not as a listview 1:1 replacement but rather as a more flexible component for complex use cases. and as you say  your solution is what google expected of you. now you have an adapter who can delegate onclick to an interface passed on the constructor  which is the correct pattern for both listview and recyclerview.         and then on your adapter         now look into that last piece of code:   the signature already suggest different view types. for each one of them you'll require a different viewholder too  and subsequently each one of them can have a different set of clicks. or you can just create a generic viewholder that takes any view and one onclicklistener and applies accordingly. or delegate up one level to the orchestrator so several fragments/activities have the same list with different click behaviour. again  all flexibility is on your side.    it is a really needed component and fairly close to what our internal implementations and improvements to listview were until now. it's good that google finally acknowledges it.  "
          }
        ],
        "parent": "q_89996",
        "author": "MLProgrammer-CiM"
      },
      {
        "date": "Dec 25, 2014 11:27:47 PM",
        "commentid": "a_110351",
        "sent": [
          {
            "topicid": 1,
            "systemtopicid": 1,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Android",
            "linePolarity": 1.04,
            "sent": "    how recyclerview is different from listview?      one difference is that there is   class with recyclerview by which you can manage your  like-          horizontal   or vertical scrolling by          gridlayout by            staggered gridlayout by         like for horizontal scrolling for recyclerview-       "
          }
        ],
        "parent": "q_89996",
        "author": "Tarun Varshney"
      }
    ],
    "sent": [
      {
        "topicid": 1,
        "systemtopicid": 1,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Android",
        "linePolarity": 1.04,
        "sent": " i don't know whether i am asking the right question. i was exploring recyclerview and i was surprised to see that recyclerview does not have onitemclicklistener(). because recyclerview extends  android.view.viewgroup . and listview extends  android.widget.abslistview .  however i solved my problem by writing onclick in my recyclerview.adapter:         but still i want to know why google removed onitemclicklistener()? is there a performance issue or something else?  "
      }
    ]
  },
  {
    "author": "James_pic",
    "parent": "",
    "title": "\"What is a possible use case of BigInteger&#39;s .isProbablePrime()?\"",
    "commentid": "q_8732",
    "date": "Dec 11, 2014 11:39:57 AM",
    "children": [
      {
        "date": "Dec 11, 2014 11:46:44 AM",
        "commentid": "a_10721",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -1.2,
            "sent": " yes  this method can be used in cryptography.   rsa encryption  involves the finding of huge prime numbers  sometimes on the order of 1024 bits (about 300 digits).  the security of rsa depends on the fact that factoring a number that consists of 2 of these prime numbers multiplied together is extremely difficult and time consuming.  but for it to work  they must be prime.    it turns out that proving these numbers prime is difficult too.  but the  miller-rabin primality test   one of the primality tests uses by    either detects that a number is composite or gives no conclusion.  running this test   times allows you to conclude that there is a 1 in 2 n  odds that this number is really composite.  running it   times yields the acceptable risk of 1 in 2 100  that this number is composite.  "
          }
        ],
        "parent": "q_8732",
        "author": "rgettman"
      },
      {
        "date": "Dec 11, 2014 2:40:01 PM",
        "commentid": "a_10722",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -1.2,
            "sent": " if the test tells you an integer is  not prime   you can certainly believe that 100%.    it is only the other side of the question  if the test tells you an integer is \\\"a probable prime\\\"  that you may entertain doubt.  repeating the test with varying \\\"bases\\\" allows the probability of falsely succeeding at \\\"imitating\\\" a prime (being a strong pseudo-prime with respect to multiple bases) to be made as small as desired.    the usefulness of the test lies in its speed and simplicity.  one would not necessarily be satisfied with the status of \\\"probable prime\\\" as a final answer  but one would definitely avoid wasting time on almost all composite numbers by  using this routine before bringing in the big guns of primality testing .    the comparison to the difficulty of factoring integers is something of a red herring.  it is known that the primality of an integer can be determined in polynomial time  and indeed there is a proof that an extension of miller-rabin test to sufficiently many bases is definitive (in detecting primes  as opposed to probable primes)  but this assumes the generalized riemann hypothesis  so it is not quite so certain as the (more expensive)  aks primality test .  "
          }
        ],
        "parent": "q_8732",
        "author": "hardmath"
      },
      {
        "date": "Dec 12, 2014 10:43:13 AM",
        "commentid": "a_10723",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -1.2,
            "sent": " the standard use case for     is in cryptography.  specifically  certain cryptographic algorithms  such as  rsa   require randomly chosen large primes.  importantly  however  these algorithms don't really require these numbers to be  guaranteed  to be prime &mdash; they just need to be prime with a  very  high probability.    how high is very high?  well  in a crypto application  one would typically call   with an argument somewhere between 128 and 256.  thus  the probability of a non-prime number passing such a test is less than one in 2 128  or 2 256 .    let's put that in perspective: if you had 10 billion computers  each generating 10 billion probable prime numbers per second (which would mean less than one clock cycle per number on any modern cpu)  and the primality of those numbers was tested with    you would  on average  expect one non-prime number to slip in  once in every 100 billion years .    that is  that would be the case  if those 10 billion computers could somehow all run for hundreds of billions of years without experiencing  any  hardware failures.  in practice  though   it's a lot more likely for a random cosmic ray to strike your computer at just the right time and place to flip the return value  of   from false to true  without causing any other detectable effects  than it is for a non-prime number to actually pass the probabilistic primality test at that certainty level.    of course  the same risk of random cosmic rays and other hardware faults also applies to deterministic primality tests like  aks .  thus  in practice  even these tests have a (very small) baseline false positive rate due to random hardware failures (not to mention all other possible sources of errors  such as implementation bugs).    since it's easy to push the intrinsic false positive rate of the  miller&ndash;rabin primality test  used by   far below this baseline rate  simply by repeating the test sufficiently many times  and since  even repeated so many times  the miller&ndash;rabin test is still much faster in practice than the best known deterministic primality tests like aks  it remains the standard primality test for cryptographic applications.    (besides  even if you happened to accidentally select a strong pseudoprime as one of the factors of your rsa modulus  it would not generally lead to a catastrophic failure.  typically  such pseudoprimes would be products of two (or rarely more) primes of approximately half the length  which means that you'd end up with a  multi-prime rsa key .  as long as none of the factors were  too  small (and if they were  the primality test should've caught them)  the rsa algorithm will still work just fine  and the key  although somewhat weaker against certain types of attacks than normal rsa keys of the same length  should still be reasonably secure if you didn't needlessly skimp on the key length.)  "
          }
        ],
        "parent": "q_8732",
        "author": "Ilmari Karonen"
      },
      {
        "date": "Dec 11, 2014 11:45:42 AM",
        "commentid": "a_10724",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -1.2,
            "sent": " a possible use case is in testing primality of a given number (at test which in itself has many uses). the   algorithm will run much faster than an exact algorithm  so if the number fails    then one need not go to the expense of running the more expensive algorithm.  "
          }
        ],
        "parent": "q_8732",
        "author": "Ted Hopp"
      },
      {
        "date": "Dec 11, 2014 11:52:40 AM",
        "commentid": "a_10725",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -1.2,
            "sent": "  finding  probable primes is an important problem in cryptography. it turns out that a reasonable strategy for finding a probable k-bit prime is to repeatedly select a random k-bit number  and test it for probable primality using a method like  .    for further discussion  see  section 4.4.1 of the handbook of applied cryptography .    also see  on generation of probable primes by incremental search  by brandt and damg?rd.  "
          }
        ],
        "parent": "q_8732",
        "author": "NPE"
      },
      {
        "date": "Dec 12, 2014 7:21:10 AM",
        "commentid": "a_10726",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -1.2,
            "sent": " algorithms such as rsa key generation rely on being able to determine whether a number is prime or not.    however  at the time that the   method was added to the jdk (february 1997)  there was no proven way to deterministically decide whether a number was prime in a reasonable amount of time. the best known approach at that time was the  miller-rabin algorithm  - a probabilistic algorithm that would sometimes give false positives (i.e  would report non-primes as primes)  but could be tuned to reduce the likelihood of false positives  at the expense of modest increases in runtime.    since then  algorithms have been discovered that can deterministically decide whether a number is prime reasonably quickly  such as the  aks algorithm  that was discovered in august 2002. however  it should be noted that these algorithms are still not as fast as miller-rabin.    perhaps a better question is why no   method has been added to the jdk since 2002.  "
          }
        ],
        "parent": "q_8732",
        "author": "James_pic"
      }
    ],
    "sent": [
      {
        "topicid": 8,
        "systemtopicid": 8,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Database",
        "linePolarity": -1.2,
        "sent": "  the method biginteger.isprobableprime()  is quite strange; from the documentation  this will tell whether a number is prime with a probability of    where   is the integer argument.    it has been present in the jdk for quite a long time  so it means it must have uses. my limited knowledge in computer science and algorithms (and maths) tells me that it does not really make sense to know whether a number is \\\"probably\\\" a prime but not exactly a prime.    so  what is a possible scenario where one would want to use this method? cryptography?  "
      }
    ]
  },
  {
    "author": "usr",
    "parent": "",
    "title": "\"Is (x - x) always positive zero for doubles  or sometimes negative zero?\"",
    "commentid": "q_87987",
    "date": "Jul 25, 2014 1:06:24 AM",
    "children": [
      {
        "date": "Jul 25, 2014 1:08:31 AM",
        "commentid": "a_107854",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": 0.0,
            "sent": "   can be   or  . there are no other values it can take in ieee 754 arithmetics in round-to-nearest (and in java  the rounding mode is  always round-to-nearest ). the subtraction of two identical finite values is  defined  as producing   in this rounding mode.  mark dickinson   in comments below  cites the ieee 754 standard as saying  section 6.3:        when the sum of two operands with opposite signs (or the difference of two operands with like signs) is exactly zero  the sign of that sum (or difference) shall be +0 in all rounding-direction attributes except roundtowardnegative [...].      this  page  shows that in particular   and   are both  .    infinities and nan both produce nan when subtracted from themselves.  "
          }
        ],
        "parent": "q_87987",
        "author": "Pascal Cuoq"
      },
      {
        "date": "Jul 25, 2014 12:09:11 PM",
        "commentid": "a_107855",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": 0.0,
            "sent": " the smt solver z3 supports ieee floating point arithmetic. let's ask z3 to find a case where  . it immediately finds   as well as  . excluding those  there is no   that satisfies that equation.         z3 implements ieee floating point arithmetic by converting all operations to boolean circuits and using the standard sat solver to find a model. barring any bugs in that translation or the sat solver the result is perfectly precise.    proof for...      ... :  http://rise4fun.com/z3/7h4    ... :  http://rise4fun.com/z3/opl4w       note the counterexample for the rounding mode  :  http://rise4fun.com/z3/t845 . for a certain   the result of   is negative zero. such a case can hardly be found by humans. yet  with an smt solver it is easy to find. we can change   to   so that z3 uses ieee equality comparison semantics instead of exact equality. after that change  there is again no counter-example because   according to ieee.    i tried making the rounding mode a variable. that would work in theory but z3 has a bug here. for now we have to manually specify a hard-coded rounding mode. if we could make it a variable we could ask z3 to prove this statement for  all  rounding modes in one query.  "
          }
        ],
        "parent": "q_87987",
        "author": "usr"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": 0.0,
        "sent": " when   is a    is   guaranteed to be    or might it sometimes be   (depending on the sign of  )?  "
      }
    ]
  },
  {
    "author": "KisHan SarsecHa Gajjar",
    "parent": "",
    "title": "\"Do &quot;nothing&quot; while &quot;condition&quot;\"",
    "commentid": "q_98540",
    "date": "Jul 7, 2014 4:30:21 AM",
    "children": [
      {
        "date": "Jul 7, 2014 4:49:22 AM",
        "commentid": "a_120983",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.9079999999999999,
            "sent": " if you read the comments at top of the file  just below the class declaration  there is a section which explains the use of this construct:       "
          }
        ],
        "parent": "q_98540",
        "author": "MicSim"
      },
      {
        "date": "Jul 7, 2014 7:44:25 AM",
        "commentid": "a_120984",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.9079999999999999,
            "sent": "   makes extensive use of   from   and most of the occurrences of   in   can &mdash; as mentioned by other answers &mdash; be explained by this comment under the heading style notes:             the choice to use write a  -loop with an empty body as   seems however to be a mostly stylistic choice. this is perhaps clearer in    which happened to be updated in java 8.    in the java 7   you can find this:         while much of the code around it has also changed  it is clear that the replacement in java 8 is this:         the first version has the weakness that if the lone semicolon happened to be deleted it would change the meaning of the program depending on the following line.    as seen below  bytecode generated by   and   is slightly different  but not in any way that should affect anything when run.    java code:         generated code:       "
          }
        ],
        "parent": "q_98540",
        "author": "Erik Vesteraas"
      },
      {
        "date": "Jul 8, 2014 1:19:10 AM",
        "commentid": "a_120985",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.9079999999999999,
            "sent": " leaving aside any potential performance benefits  there is a clear readability benefit.    with   the trailing semicolon is not always obvious at first glance  you may be confused into thinking that the following statement or statements are inside the loop. for example:         it would be very easy to misread the above as having the if statement inside the loop  and even if you did read it correctly it would be easy to think that it was a programming mistake and the if should be inside the loop.    with   though it is immediately at a glance clear that there is no body to the loop.  "
          }
        ],
        "parent": "q_98540",
        "author": "Tim B"
      },
      {
        "date": "Jul 7, 2014 4:46:01 AM",
        "commentid": "a_120986",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.9079999999999999,
            "sent": " if you will read comment above the code  it is mentioned that...    if the caller is not a    this method is behaviorally equivalent to         so it is just another form to implement above code in else part...!!    in  style notes  it is mentioned that          there are several occurrences of the unusual \\\"do {} while    (!cas...)\\\"  which is the simplest way to force an update of a    cas'ed variable.      and if you will see implementation of  managedlocker#isreleasable   it is updating the lock and returns   if blocking is unnecessary.      interpretation :      blank while loops are used to provide an interrupt until some condition reset to true/false.    here    is a blocker/interrupt until   will be   when   is  . loop will continue execution while   is not releasable ( ) and   is not blocked !! execution will be out of loop as soon as   will set to true.    note that    does not update cas variable  but it guarantee that program will wait until variable gets updated (force to wait until variable gets updated).    "
          }
        ],
        "parent": "q_98540",
        "author": "KisHan SarsecHa Gajjar"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.9079999999999999,
        "sent": " while browsing the code for the java 8 version of forkjoinpool(which has a few interesting changes from java 7) i ran across this construct ( here ):         i'm struggling with why you would write it like this instead of just         is it just a semantics/readability choice  since you could read the first construct as  ? or is there some additional benefit i'm missing?  "
      }
    ]
  },
  {
    "author": "Leo",
    "parent": "",
    "title": "Explain the syntax of Collections.&lt;String&gt;emptyList()",
    "commentid": "q_908",
    "date": "Dec 28, 2014 10:18:20 PM",
    "children": [
      {
        "date": "Dec 28, 2014 10:53:44 PM",
        "commentid": "a_1037",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.0,
            "sent": " that line creates an empty list of strings by calling a static method with a generic type parameter.    inside the   class  there is a static method   declared like:         this has a generic type parameter  . we call call this method by using:         and   is infered to be a   because of the type of  .    we can also specify the type of   by putting it in angle brackets when calling  . this may be needed if we want a more specific type than is inferred:           is not correct because that placement is only valid when creating instances of generic classes  not calling methods. when using   there are two possible type parameters  the ones before the class name are for the constructor only  and the ones after the class name are for the whole instance  so with the class:         we can call its constructor where   is   and   is   like:         or by using type inference:          see also:        generic methods     type inference     "
          }
        ],
        "parent": "q_908",
        "author": "fgb"
      },
      {
        "date": "Dec 28, 2014 10:53:26 PM",
        "commentid": "a_1038",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.0,
            "sent": "\"     what is  ? why isn't it   or  ?          is a jdk class.        this class consists exclusively of static methods that operate on or   return collections. it contains polymorphic algorithms that operate on   collections  \\\"\"wrappers\\\"\"  which return a new collection backed by a   specified collection  and a few other odds and ends.      it's not generic and cannot be instantiated.        why is   placed before the method name  ?          is a generic method. here  we are explicitly specifying a type argument   .        (isn't   correct for generic?)      no  in java  generic type arguments for methods come before the method name.        what does the statement mean?      we are invoking the static   method and assigning its return value to a variable of type  .  \""
          }
        ],
        "parent": "q_908",
        "author": "Sotirios Delimanolis"
      },
      {
        "date": "Dec 28, 2014 10:53:51 PM",
        "commentid": "a_1039",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.0,
            "sent": "\"  in a nutshell  this creates an empty  immutable list of strings.     let's look at the expression bit by bit.        is the name of a class. from the  javadoc :        this class consists exclusively of static methods that operate on or return collections. it contains polymorphic algorithms that operate on collections  \\\"\"wrappers\\\"\"  which return a new collection backed by a specified collection  and a few other odds and ends.          is the name of a static method defined in the   class ( javadoc ). it is a generic method  and the   in   specifies the generic type argument.    the method returns a    which in this case is  : a list of strings. more specifically  it returns an  empty    immutable  list of strings.  \""
          }
        ],
        "parent": "q_908",
        "author": "NPE"
      },
      {
        "date": "Dec 28, 2014 10:53:25 PM",
        "commentid": "a_1040",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.0,
            "sent": " firstly    is a helper library of static methods that operate on various types of collections. if you've done any sort of c++ programming  it's very synonymous to the   library of functions.    in this case  you're invoking the static method    which returns an empty list of a particular type. since these methods still require a type  but  ' methods can refer to many types  you have to specify the type upon invocation.    in order to call a generic  method   you must call it with the parameter list (i.e.  )  before  the method name  but  after  the dot.       "
          }
        ],
        "parent": "q_908",
        "author": "Qix"
      },
      {
        "date": "Dec 28, 2014 10:50:54 PM",
        "commentid": "a_1041",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.0,
            "sent": " 1.collection is an interface and collections is a static class consists exclusively of static methods that operate on or return collections. hence we cannot use   or  .    2.  before emptylist(used to get the empty list that is immutable) denotes that only string values can be added to it. like:          3.the statement means that it will create a immutable empty list of type list.    you can check out this link:  difference between java collection and collections   "
          }
        ],
        "parent": "q_908",
        "author": "Leo"
      }
    ],
    "sent": [
      {
        "topicid": 10,
        "systemtopicid": 10,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Network",
        "linePolarity": 0.0,
        "sent": " i just studied about generic programming  the   interface  and    so i can understand the statement below.         but i don't understand the next statement which i saw while surfing the web.           what is  ? why isn't it   or  ?   why is   placed before the method name  ?      (isn't   correct for generic?)      what does the statement mean?    "
      }
    ]
  },
  {
    "author": "August",
    "parent": "",
    "title": "\"Why does one long string take MORE space than lots of small strings?\"",
    "commentid": "q_16763",
    "date": "Nov 28, 2014 2:54:34 PM",
    "children": [
      {
        "date": "Nov 28, 2014 4:01:53 PM",
        "commentid": "a_20291",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": -0.28400000000000003,
            "sent": " to expand on august's answer  i figured i should explain exactly how this is compiled and what takes up space at the binary level.    to simplify  i'll use the following example with shorter strings. the important part is that there are some duplicate strings.         now there are three important things to understand when compiling this code.       constant string concatenation is done at compile time.  in fact  this is a special case of compile time simplification of constant expressions. you can find the exact rules for what is considered a constant expression in the java language specification.    array brace initializers are syntactic sugar . the code is equivalent to creating an array and assigning the elements one by one. (note that this is specific to java bytecode. dalvik (i.e. android) does have special shorthand instructions for array initialization)    inline initializers are syntactic sugar . the code is equivalent to manually initializing the fields in the static initializer method.       edit : one minor detail is that in the special case of static final fields initialized to a constant expression inline  the field is initialized using the constantvalue attribute instead of in the static initializer and all uses are inlined. so in the case of    #3 will actually result in different bytecode  but since the constant pool entries for the string are the same  the filesize taken up by the strings won't change.    put them together  and the above code is equivalent to the following.         now this desugared code still shows the duplicate strings multiple times in the array example. to understand the classfile size behavior  you have to understand what it is compiled to.    when you access a constant string  the bytecode contains a load constant instruction (  or  )  which is a one byte opcode followed by an index into the class's constant pool. the constant pool is a seperate section of the classfile where a list of constants is stored. obviously  the compiler will only store each constant once.     so the bytecode for stringarray looks something like this (stripping away a few details that aren't relevant here). note that there are only 3 unique strings stored in the constant pool. (actually there are a lot more constant pool entries relating to other parts of the classfile  but they're not important here).         whereas longstring looks something like         so with the first version  duplicate strings can be stored only once  whereas with the second version  the entire string has to be stored. so is the first version always better? not so fast. it has the advantage of not storing duplicated strings  but as you may have noticed  there is a big per element overhead in the array initialization. which one is better depends on how long your strings are and how many duplicates you have.    p.s. at the binary level  constant strings are encoded with a modified utf8 encoding. the upshot is that characters 1-127 are one byte each but null characters are two bytes. so you can save some space by offsetting everything 1.  "
          }
        ],
        "parent": "q_16763",
        "author": "Antimony"
      },
      {
        "date": "Nov 28, 2014 3:18:05 PM",
        "commentid": "a_20292",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": -0.28400000000000003,
            "sent": " you have  18  duplicate strings in your array (  class). since each array entry is a separate string literal  there will only be constant pool entries for  unique  strings. however  in your second example (  class)  all the duplicates are kept and concatenated at compile-time.    it's a bit hard to look at the class file you posted  because of the size of the strings. i used a shorter version  but it still has the same concept:                 if we open up the class file    you can see the entire string:         but if you look at the class file    duplicates have been removed:          the space between   and   is    which marks a     entry of length  .     in your case  reusing strings would save 2 304 characters.   "
          }
        ],
        "parent": "q_16763",
        "author": "August"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": -0.28400000000000003,
        "sent": " here is some code for a dfa  implemented as an array of strings:         i thought i could save some space by using one long string instead of a string array:         as it turns out  i was wrong. eclipse kepler generates the following class files:         how come the second class is even bigger than the first class? do very long strings incur some space penalty i am not aware of?  "
      }
    ]
  },
  {
    "author": "Ruchira Gayan Ranaweera",
    "parent": "",
    "title": "\"Infinite loop breaks method signature without compilation error\"",
    "commentid": "q_81842",
    "date": "Aug 6, 2014 12:24:30 AM",
    "children": [
      {
        "date": "Aug 6, 2014 12:26:40 AM",
        "commentid": "a_100178",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.0,
            "sent": " the final   of the method is unreachable - you only get a compilation error if it's possible to get to the end of the method without returning a value.    this is more useful for cases where the end of the method is unreachable due to an exception  e.g.         the rule for this is in the  jls section 8.4.7 :        if a method is declared to have a return type (?8.4.5)  then a compile-time error occurs if the body of the method can complete normally (?14.1).      your method can't complete normally  hence there's no error. importantly  it's not just that it can't complete normally  but the specification  recognizes  that it can't complete normally. from  jls 14.21 :        a   statement can complete normally iff at least one of the following is true:            the   statement is reachable and the condition expression is not a constant expression (?15.28) with value  .     there is a reachable   statement that exits the   statement.          in your case  the condition expression  is  a constant with value    and there aren't any   statements (reachable or otherwise) so the   statement can't complete normally.  "
          }
        ],
        "parent": "q_81842",
        "author": "Jon Skeet"
      },
      {
        "date": "Aug 6, 2014 12:28:27 AM",
        "commentid": "a_100179",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.0,
            "sent": "      to more clarification try this         it says   statement  "
          }
        ],
        "parent": "q_81842",
        "author": "Ruchira Gayan Ranaweera"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -1.0,
        "sent": " i am wondering why is the following code allowed in java  without getting compilation error? in my opinion  this code breaks method signature by not returning any  . could someone explain what i'm missing here?         "
      }
    ]
  },
  {
    "author": "Dima",
    "parent": "",
    "title": "\"Why are arrays Objects  but can not be used as a base class?\"",
    "commentid": "q_6394",
    "date": "Dec 16, 2014 5:42:03 AM",
    "children": [
      {
        "date": "Dec 16, 2014 5:53:08 AM",
        "commentid": "a_7769",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.948,
            "sent": " java was a compromise between non-object languages and very slow languages of that time where everything was an object (think about  smalltalk ).    even in more recent languages  having a  fast  structure at the language level for arrays (and usually maps) is considered a strategic goal. most people wouldn't like the weight of an inheritable object for arrays  and certainly nobody wanted this before jvm advances like jit.     that's why arrays  while being objects  weren't designed as class instances ( \\\"an object is a class instance or an array\\\" ). there would be little benefit in having the ability to override a method on an array  and certainly not a great-enough one to counterbalance the need to check for the right method to apply (and in my opinion not a great-one enough to counterbalance the increased difficulty in code reading  similar to what happens when you override operators).  "
          }
        ],
        "parent": "q_6394",
        "author": "dystroy"
      },
      {
        "date": "Dec 16, 2014 5:52:04 AM",
        "commentid": "a_7770",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.948,
            "sent": " i came across   under the hood - objects and arrays   which explains almost anything you need to know about how jvm handles arrays. in jvm  arrays are handled with special  bytecodes   not like other objects we are familiar with.        in the jvm instruction set  all objects are instantiated and accessed   with the same set of opcodes   except  for arrays. in java   arrays are   full-fledged objects   and  like any other object in a java program    are created dynamically. array references can be used anywhere a   reference to type object is called for  and any method of object can   be invoked on an array. yet  in the java virtual machine   arrays are   handled with special bytecodes .        as with any other object  arrays  cannot be declared as local   variables ; only array references can. array objects themselves always   contain either an array of primitive types or an array of object   references. if you declare an array of objects  you get an array of   object references. the objects themselves must be explicitly created   with new and assigned to the elements of the array.      arrays are  dynamically  created objects  and they serve as a container that hold a (constant) number of objects of the same type. it looks like arrays are not like any other object  and that's why they are treated differently.  "
          }
        ],
        "parent": "q_6394",
        "author": "Maroun Maroun"
      },
      {
        "date": "Dec 16, 2014 5:53:22 AM",
        "commentid": "a_7771",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.948,
            "sent": " i'd like to point out this  article . it seems as though arrays and objects follow different opcodes. i can't honestly summarize it more than that however it seems  arrays are simply not treated as   like we're normally used to so they don't inherit   methods.    full credits to the author of that post as it's a very interesting read  both short &amp; detailed.       upon further digging into the topic via multiple sources i've decided to give a more elaborate version of my previous answer.    the first thing to note that instantiation of  objects  and  arrays  are very different within the jvm  their follow their respective bytecode.     object:       instantiation follows a simple opcode   which is a combination of two operands -   &amp;  . once instantiated the jvm pushes the  reference  to this object onto the  . this occurs for all objects irrespective of their types.        arrays:       opcodes  (regarding instantiation of an array)  however are divided into three different codes.          - pops length  allocates new array of primitive types of type indicated by atype  pushes objectref of new array        opcode is used when creating arrays that involve primitive datatypes (               ) rather than object references.          - pops length  allocates a new array of objects of class indicated by indexbyte1 and indexbyte2  pushes objectref of new array        opcode is used when creating arrays of object references          - pops dimensions number of array lengths  allocates a new multidimensional array of class indicated by indexbyte1 and indexbyte2  pushes objectref of new array        instruction is used when allocating multi-dimensional arrays       object  can  be a class instance or an array.     take from  oracle docs          a class instance is explicitly created by a class instance creation expression        but         an array is explicitly created by an array creation expression       this goes hand in hand with the information regarding the opcodes. arrays are simply not developed to be class interfaces but are instead  explicitly  created by array creation expression thus naturally wouldn't  implicitly  be able to inherit and/or override  .    as we have seen  it has nothing to do with the fact that arrays may hold primitive datatypes. after giving it some thought though  it isn't very common to come across situations where one might want to   or   however was still a very interesting question to try and answer.       resources:     oracle-docs chapter 4.3.1      oracle-docs chapter 15.10.1      artima - underthehood   "
          }
        ],
        "parent": "q_6394",
        "author": "Juxhin"
      },
      {
        "date": "Dec 16, 2014 5:53:54 AM",
        "commentid": "a_7772",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.948,
            "sent": " there are many classes in standard java library that you cannot subclass  arrays aren't the only example. consider    or    or any of the \\\"primitive wrappers\\\"  like    or  . there are optimizations that jvm does based on knowing the exact structure of these objects when it deals with them (like unboxing the primitives  or manipulating array memory at byte level). if you could override anything  it would not be possible  and affect the performance of  the programs very badly.   "
          }
        ],
        "parent": "q_6394",
        "author": "Dima"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.948,
        "sent": " the java language specification specifies that         in the java programming language arrays are objects (?4.3.1)  are dynamically created  and may be assigned to variables of type object (?4.3.2). all methods of class   may be invoked on an array.      so  considering arrays are objects ? why did the java designers make the decision not to allow inherit and override from it  for example      or  ?    the current syntax wouldn't allow creating anonymous classes with an array as the base class  but i don't think  that  was the reason for their decision.  "
      }
    ]
  },
  {
    "author": "tobias_k",
    "parent": "",
    "title": "\"What does the -&gt; &lt;- operator do?\"",
    "commentid": "q_88353",
    "date": "Jul 24, 2014 8:46:30 AM",
    "children": [
      {
        "date": "Jul 24, 2014 8:52:08 AM",
        "commentid": "a_108356",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " there is no   operator. that first   is just lambda syntax  as introduced in java 8  and that second   is a delusive concatenation of 'smaller than'   and 'unary minus'  .    you can read it as    i.e. it tests whether   is smaller than    which is the case for all (well   most ) negative numbers  hence the name  .            just for completeness: this test is not only (intentionally?) hard to understand  but -- as pointed out in the comments -- it also fails for   (which is  ). instead  you should probably just use the much simpler  .  "
          }
        ],
        "parent": "q_88353",
        "author": "tobias_k"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": 0.6680000000000001,
        "sent": " i recently came upon the following code:         what is this  some sort of reverse double lambda?  "
      }
    ]
  },
  {
    "author": "Rahul Tripathi",
    "parent": "",
    "title": "\"Java Primitives range calculation\"",
    "commentid": "q_93450",
    "date": "Jul 15, 2014 10:39:25 PM",
    "children": [
      {
        "date": "Jul 15, 2014 10:41:35 PM",
        "commentid": "a_114588",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.0,
            "sent": " the compiler will  in this case  evaluate the calculation (because it contains only constants) and try to assign the result to the variable. this calculation is done with type   and only converted to   on assignment  if at all possible.    in your case  the first calculation is too large to fit into a   (1073741824). the second one will overflow the   and end up in a range that   supports (0). so the assignment works in that case.    mind you  you probably don't ever want to rely on these things in code.  "
          }
        ],
        "parent": "q_93450",
        "author": "Joey"
      },
      {
        "date": "Jul 15, 2014 10:46:56 PM",
        "commentid": "a_114589",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.0,
            "sent": " you are facing the problem as your number is  wrapping around .in the first case it does not wrap around and hence it overflows the range of short. but in the second case it wraps around after the calculationa and hence it comes in the range of short and so you dont have the compile time error.    a loss of precision means that you are losing information of the given value.( the short data type is a 16-bit signed two's complement integer. it has a minimum value of -32 768 and a maximum value of 32 767 (inclusive). ) in your first case the range of short is crossed(1073741824) and hence you are loosing the information.        a narrowing conversion of a signed integer to an integral type t   simply discards all but the n lowest order bits  where n is the number   of bits used to represent type t.       edit:-     from   jls ?3.10.1  (very correctly mentioned in  this  similar question)        it is a compile-time error if a decimal literal of type int is larger   than 2147483648 (2 31 )  or if the decimal literal 2147483648 appears   anywhere other than as the operand of the unary minus operator   ( ?15.15.4 ).    "
          }
        ],
        "parent": "q_93450",
        "author": "Rahul Tripathi"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.0,
        "sent": " in java when we declare          it will gives compile time error but           compiles fine.  why this happened?   "
      }
    ]
  },
  {
    "author": "Chris K",
    "parent": "",
    "title": "\"Why can&#39;t you add an int and a char in some cases?\"",
    "commentid": "q_98577",
    "date": "Jul 7, 2014 3:16:50 AM",
    "children": [
      {
        "date": "Jul 7, 2014 3:25:06 AM",
        "commentid": "a_121034",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": 0.0,
            "sent": " to understand this  lets consider what the compiler does at each step for both possibilities.  lets start with:         the compiler converts '4' to an int.  so it becomes         which the compiler then turns into         ch is a char  and the compiler is allowed to convert 54 to a char as it can prove that there is no loss in the conversion.     now lets consider the second version:         ch has no known value at compile time.  thus this becomes         now the compiler cannot prove that the result of this (an int) is storable within the range of a char.  so it will  not  automatically narrow it  and reports it as an error.    edit1:    if the compiler can prove that the variable will never change  and is inlineable.  then the second form can be turned into the first.   subir pointed out that adding 'final' makes this possible.  although if a compiler was to perform change analysis then it is technically capable of figuring this out without the final keyword  but final does make it easier for the compiler and readers of the code.    edit2:     narrowing of int to char is covered in the  java language spec   the link was kindly provided by jon skeet.  "
          }
        ],
        "parent": "q_98577",
        "author": "Chris K"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": 0.0,
        "sent": " why does         work  but         doesn't?  "
      }
    ]
  },
  {
    "author": "falsarella",
    "parent": "",
    "title": "\"Are the bit patterns of NaNs really hardware-dependent?\"",
    "commentid": "q_85007",
    "date": "Jul 30, 2014 7:56:13 PM",
    "children": [
      {
        "date": "Jul 30, 2014 10:57:52 PM",
        "commentid": "a_104172",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.17199999999999993,
            "sent": " this is what  ?2.3.2 of the jvm 7 spec  has to say about it:        the elements of the double value set are exactly the values that can be represented   using the double floating-point format defined in the ieee 754 standard  except   that there is only one nan value (ieee 754 specifies 2 53 -2 distinct nan values).      and  ?2.8.1 :        the java virtual machine has no signaling nan value.      so technically there is only one nan. but  ?4.2.3 of the jls  also says (right after your quote):        for the most part  the java se platform treats nan values of a given type as though collapsed into a single canonical value  and hence this specification normally refers to an arbitrary nan as though to a canonical value.        however  version 1.3 of the java se platform introduced methods enabling the programmer to distinguish between nan values: the float.floattorawintbits and double.doubletorawlongbits methods. the interested reader is referred to the specifications for the float and double classes for more information.      which i take to mean exactly what you and  candiedorange  propose: it is dependent on the underlying processor  but java treats them all the same.    but it gets better: apparently  it is entirely possible that your nan values are silently converted to different nans  as described in    :        note that this method may not be able to return a double nan with exactly same bit pattern as the long argument. ieee 754 distinguishes between two kinds of nans  quiet nans and signaling nans. the differences between the two kinds of nan are generally not visible in java. arithmetic operations on signaling nans turn them into quiet nans with a different  but often similar  bit pattern. however  on some processors merely copying a signaling nan also performs that conversion. in particular  copying a signaling nan to return it to the calling method may perform this conversion. so longbitstodouble may not be able to return a double with a signaling nan bit pattern. consequently  for some long values  doubletorawlongbits(longbitstodouble(start)) may not equal start. moreover  which particular bit patterns represent signaling nans is platform dependent; although all nan bit patterns  quiet or signaling  must be in the nan range identified above.      for reference  there is a table of the hardware-dependant nans  here . in summary:         so  to verify this you would really need one of these processors and go try it out. also any insights on how to interpret the longer values for the power and alpha architectures are welcome.  "
          }
        ],
        "parent": "q_85007",
        "author": "jmiserez"
      },
      {
        "date": "Jul 30, 2014 8:49:38 PM",
        "commentid": "a_104173",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.17199999999999993,
            "sent": " the way i read the jls here the exact bit value of a nan is dependent on who/what made it and since the jvm didn't make it  don't ask them.  you might as well ask them what an \\\"error code 4\\\" string means.       the hardware produces different bit patterns meant to represent different kinds of nan's.  unfortunately the different kinds hardware produce different bit patterns for the same kinds of nan's.  fortunately there is a standard pattern that java can use to at least tell that it is some kind of nan.      it's like java looked at the \\\"error code 4\\\" string and said  \\\"we don't know what 'code 4' means on your hardware  but there was the word 'error' in that string  so we think it's an error.\\\"    the jls tries to give you a chance to figure it out on your own though:    \\\"however  version 1.3 of the java se platform introduced methods enabling the programmer to distinguish between nan values: the float.floattorawintbits and double.doubletorawlongbits methods. the interested reader is referred to the specifications for the float and double classes for more information.\\\"    which looks to me like a c++ reinterpret cast.  it's java giving you a chance to analyze the nan yourself in case you happen to know how its signal was encoded.  if you want to track down the hardware specs so you can predict what different events should produce which nan bit patterns you are free to do so but you are outside the uniformity the jvm was meant to give us.  so expect it might change from hardware to hardware.    when testing if a number is nan we check if it's equal to itself since it's the only number that isn't.  this isn't to say that the bits are different.  before comparing the bits the jvm tests for the many bit patterns that say it's a nan.  if it's any of those patterns then it reports that it's not equal  even if the bits of the two operands really are the same (and even if they're different).      back in 1964  when pressed to give an exact definition for pornography  u.s. supreme court justice stewart famously said  ?i know it when i see it?.  i think of java as doing the same thing with nan's.  java can't tell you anything that a \\\"signaling\\\" nan might be signaling cause it doesn't know how that signal was encoded.  but it can look at the bits and tell it's a nan of some kind since that pattern follows one standard.      if you happen to be on hardware that encodes all nan's with uniform bits you'll never prove that java is doing anything to make nan's have uniform bits.  again  the way i read the jls  they are outright saying you are on your own here.    i can see why this feels flaky.  it is flaky.  it's just not java's fault.  i'll lay odds that some where out there some enterprising hardware manufactures came up with wonderfully expressive signaling nan bit patterns but they failed to get it adopted as a standard widely enough that java can count on it.  that's what's flaky.  we have all these bits reserved for signalling what kind of nan we have and can't use them because we don't agree what they mean.  having java beat nan's into a uniform value after the hardware makes them would only destroy that information  harm performance  and the only payoff is to not seem flaky.  given the situation  i'm glad they realized they could cheat their way out of the problem and define nan as not being equal to anything.  "
          }
        ],
        "parent": "q_85007",
        "author": "CandiedOrange"
      },
      {
        "date": "Jul 30, 2014 10:43:01 PM",
        "commentid": "a_104174",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.17199999999999993,
            "sent": " here is a program demonstrating different nan bit patterns:         output:         regardless of what the hardware does  the program can itself create nans that may not be the same as e.g.    and may have some meaning in the program.  "
          }
        ],
        "parent": "q_85007",
        "author": "Patricia Shanahan"
      },
      {
        "date": "Aug 5, 2014 5:32:37 PM",
        "commentid": "a_104175",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.17199999999999993,
            "sent": " the only other   value that i could generate with normal arithmetic operations so far is the same but with the sign changed:         output:       "
          }
        ],
        "parent": "q_85007",
        "author": "falsarella"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.17199999999999993,
        "sent": " i was reading about floating-point nan values in the java language specification (i'm boring). a 32-bit   has this bit format:           is the sign bit    are the exponent bits  and   are the mantissa bits. a nan value is encoded as an exponent of all 1s and the mantissa bits are not all 0 (which would be +/- infinity). this means that there are lots of different possible nan values (having different   and   bit values).    on this   jls ?4.2.3  says:        ieee 754 allows multiple distinct nan values for each of its single and double floating-point formats. while each hardware architecture returns a particular bit pattern for nan when a new nan is generated  a programmer can also create nans with different bit patterns to encode  for example  retrospective diagnostic information.      the text in the jls seems to imply that the result of  for example     has a hardware-dependent bit pattern  and depending on whether that expression was computed as a compile time constant  the hardware it is dependent on might be the hardware the java program was compiled on or the hardware the program was run on. this all seems  very  flaky if true.    i ran the following test:         the output on my machine is:         the output indicates that the exponent bits are 1 as expected. the upper bit of the mantissa is also 1  which for nans apparently indicates a 'quiet nan' as opposed to a 'signalling nan' ( https://en.wikipedia.org/wiki/nan#floating_point ). the sign bit and the rest of the mantissa bits are 0. the output also indicates that there was no difference between the nans generated on my machine and the constant nans from the float and double classes.    my question is  is that output guaranteed in java  regardless of the cpu of the compiler or vm  or is it all genuinely unpredictable? the jls is mysterious about this.    if that output is guaranteed for    are there any arithmetic ways of producing nans that have other (possibly hardware-dependent?) bit patterns? (i know i could use  /  to encode other nans deliberately  but i would like to know if other values can occur from normal arithmetic.)       a followup question: i've noticed that  float.nan  and  double.nan  specify their exact bit pattern  but in the source ( float    double ) they are generated by 0.0/0.0. if that result is really dependent on the hardware of the compiler  the spec is wrong  right  and can't actually make that guarantee?  "
      }
    ]
  },
  {
    "author": "thkala",
    "parent": "",
    "title": "\"JIT not optimizing loop that involves Integer.MAX_VALUE\"",
    "commentid": "q_76669",
    "date": "Aug 15, 2014 5:26:56 AM",
    "children": [
      {
        "date": "Aug 15, 2014 5:32:50 AM",
        "commentid": "a_93580",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.3600000000000001,
            "sent": " i have not dug up the java language specification  but i'd guess that it has to do with this difference:         never overflows. once   reaches   it is incremented to   and then the loop terminates.       contains an integer overflow. once   reaches    it is incremented by one causing an overflow and  then  the loop terminates.       i assume that the jit compiler is \\\"reluctant\\\" to optimize-out loops with such corner conditions - there was a  whole bunch of bugs  w.r.t. loop optimization in integer overflow conditions  so that reluctance is probably quite warranted.    there may also be some hard requirement that does not allow integer overflows to be optimized-out  although i somehow doubt that since integer overflows are not directly detectable or otherwise handled in java.  "
          }
        ],
        "parent": "q_76669",
        "author": "thkala"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.3600000000000001,
        "sent": " while writing an answer to  another question   i noticed a strange border case for jit optimization.     the following program is  not  a \\\"microbenchmark\\\" and  not  intended to reliably measure an execution time (as pointed out in the answers to the other question). it is solely intended as an  mcve  to reproduce the issue:         it basically runs the same loop     where the limit   is once set to    and once to  .     when executing this on win7/64 with jdk 1.7.0_21 and          the timing results are as follows:         obviously  for the case of    the jit does what one could expect: it detects that the loop is useless  and completely eliminates it. however  it does  not  remove the loop when it is running up to  .     this observation is confirmed by a look at the jit assembly output when starting with          the log contains the following assembly for the method that runs up to  :         one can clearly see the loop  with the comparison to   and the jump back to  . in contrast to that  the assembly for the case where it is running up to  :         so my question is: what is so special about the   that prevents the jit from optimizing it in the same way as it does for  ? my guess would be that has to do with the   instruction  which is intended for  signed  arithmetic  but that alone is not really a convincing reason. can anybody explain this  and maybe even give a pointer to the openjdk hotspot code where this case is treated?    (an aside: i hope that the answer will also explain the different behavior between   and   that was asked for in the other question  assuming that the reason for the missing optimization is (obviously)  actually  caused by the   loop limit)  "
      }
    ]
  },
  {
    "author": "ganbustein",
    "parent": "",
    "title": "\"Why does =+ not cause a compile error?\"",
    "commentid": "q_10739",
    "date": "Dec 8, 2014 8:43:50 PM",
    "children": [
      {
        "date": "Dec 8, 2014 8:45:16 PM",
        "commentid": "a_13014",
        "sent": [
          {
            "topicid": 1,
            "systemtopicid": 1,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Android",
            "linePolarity": 0.0,
            "sent": " there's no compilation error because   is a valid (albeit fairly useless)  unary operator  in the same way that   is:         the relevant section in the java language specification is  unary plus operator + (?15.15.3 ) . it specifies that invoking the unary   operation results in  unary numeric promotion (?5.6.1)  of the operand. this means that:             if the operand is of compile-time type          or    it is subjected to unboxing conversion   ( ?5.1.8 ).   the result is then promoted to a value of type   by a widening   primitive conversion   ( ?5.1.2 )   or an identity conversion   ( ?5.1.1 ).       otherwise  if the operand is of compile-time type       or    it is subjected to unboxing conversion   ( ?5.1.8 ).       otherwise  if the operand is of compile-time type       or    it is promoted to a value of type   by a widening   primitive conversion   ( ?5.1.2 ).       otherwise  a unary numeric operand remains as is and is not converted.             in any case  value set conversion   ( ?5.1.13 )   is then applied.      in short: numeric primitive wrapper types are unboxed  and integer types smaller than   are widened to  .  "
          }
        ],
        "parent": "q_10739",
        "author": "Robby Cornelissen"
      },
      {
        "date": "Dec 8, 2014 9:10:42 PM",
        "commentid": "a_13015",
        "sent": [
          {
            "topicid": 1,
            "systemtopicid": 1,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Android",
            "linePolarity": 0.0,
            "sent": " there may be a bug lurking here. the writer may have intended to write      in the original version of c    and   were synonyms. if you meant    you had to be careful to leave a space between the   and the  . same with all the other operators.   multiplied a by p.   de-referenced the pointer p and assigned the result to a.    then they came to their senses  and started giving warnings for   where   was probably intended  and now no longer accept   at all.    but old habits die hard. an old-school c programmer might might still absent-mindedly use old-school syntax  even when writing in a language other than c.    on the other hand  the   in   is an initialization  not an assignment  and it would be bizarre for a programmer to think in terms of incrementing a variable that is only just now being given its initial value.  "
          }
        ],
        "parent": "q_10739",
        "author": "ganbustein"
      }
    ],
    "sent": [
      {
        "topicid": 1,
        "systemtopicid": 1,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Android",
        "linePolarity": 0.0,
        "sent": " came across someone mistakenly using    instead of    in their code and it didn't show up as a compile error.    is this because         is the same as         ?  "
      }
    ]
  },
  {
    "author": "Sotirios Delimanolis",
    "parent": "",
    "title": "\"Is it possible to use multiple view resolvers in Spring?\"",
    "commentid": "q_83801",
    "date": "Aug 1, 2014 9:53:06 PM",
    "children": [
      {
        "date": "Aug 1, 2014 10:00:52 PM",
        "commentid": "a_102568",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": 0.18000000000000016,
            "sent": " you absolutely can.   has a single method      which returns        the view object  or  null if not found  (optional  to allow for viewresolver chaining)      your   beans are registered. when a view name is returned by a handler  spring will iterate through each    invoking their   method with the given name. if the method returns non   that   is used. if   is returned  then it continues iterating.    so the implementation has to return   if spring is to skip it.    there are some implementations that never return  . this seems to be the case with your custom   classes. if the   returns a    even if that   will eventually fail to be rendered  spring will use it.    you either need to fix that or order your   beans. for example  you can order them with the   interface. have your classes implement that interface and return an appropriate value.  "
          }
        ],
        "parent": "q_83801",
        "author": "Sotirios Delimanolis"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": 0.18000000000000016,
        "sent": " i have multiple view resolvers in my spring configuration and i wanted to use different view resolvers depending on conditions.     example for urls started with  . i wanted to use birt view resolver and for ajax calls use tiles resolver and so on.    i tried setting order property but all views are resolved by tilesviewresolver       "
      }
    ]
  },
  {
    "author": "gexicide",
    "parent": "",
    "title": "\"Can there be a (Java 7) FileSystem for which a Path .isAbsolute() but has a null root?\"",
    "commentid": "q_16555",
    "date": "Nov 29, 2014 5:24:20 AM",
    "children": [
      {
        "date": "Dec 15, 2014 8:47:00 AM",
        "commentid": "a_20055",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": -0.3640000000000001,
            "sent": " well  there are some obscure things with file systems. i made a few enterprise search crawlers  and somewhere down the road you will notice some strange file system things going on with paths. btw: these are all implementations of custom (overridden) file systems  so no standard ones  and you can definitely argue for hours on what of those things are good ideas and what are not... still  i don't think you'll encounter any of these cases with the standard file systems.    here goes a few examples of strange things:     files in container file systems (ole2  zip  tar  etc): c:\\\\foo\\\\bar\\\\blah.zip\\\\myfile     in this case  you can decide what item is 'the root':      'c:\\\\' ? that's not the root of the zip file containing the file...   'c:\\\\foo\\\\bar\\\\blah.zip' ? it might be the root of the file  but by doing that it might break your application.   'blah.zip' ? might be the root of the zip file - but regardless this might probably break your application as well.   '/' ? as in the '/' folder in the zip file? it might be possible  but that will give you a serious headache in the long run.       'graph' like structures like http:       the fact that you have '/foo/bar' doesn't imply that '/foo' or even '/' exists. (suppose that meets your criterium). the only thing you can do is walk the graph...   note that protocols like webdav are http based and can give you a similar headache. i have some examples here of custom webdav file systems that don't have a 'root' folder  but do have absolute paths.       still  you can argue that the top-most common path (if that exists...) that you can reach is the root or that there is a root - but you simply cannot reach it (even though it's really non-existent).     samba/netbios     if you see a complete samba (windows networking) network as a single file system  then you basically end up with a 'root' containing all workgroups  a workgroup containing all computers  a computer containing all shares  and then the files in the share.    however... the root and the workgroups don't really exist. they are things that are made up from a broadcast protocol (which is also quite unreliable if you have a network of over 1000 computers). from a crawler perspective  it makes all the sense in the world to treat the 'root' and 'workgroup' directories completely different from the (reliable) rest.     however     these scenario's describe only paths where the root is unreachable  unreliable or something else. theoretically  i suppose that in any url you can think of  there is always a root. after all  it's made up as a string of characters defining a hierarchy  which therefore by definition has a start.  "
          }
        ],
        "parent": "q_16555",
        "author": "atlaste"
      },
      {
        "date": "Dec 15, 2014 8:50:45 AM",
        "commentid": "a_20056",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": -0.3640000000000001,
            "sent": " a question of semantics    from my understanding of the subject  an absolute path can only be absolute if it can be traced back to it's root. as such there should  never  be an absolute path without a root. ultimately this just comes down to semantics and although we can find definitions that define the absolute path such (egs below);       \\\"the absolute path contains the root directory and all other subdirectories\\\"     \\\"absolute path is a path that points to the same location on one file system regardless of the present working directory or combined paths. as such it must always contain the root directory.\\\"       the only real question left after this point is whether the definition by the java api follows suit. the only place i can find reference to the definition of an absolute path (with reference to the root element) from an official oracle source is from inside the official java tutorial.  the official java tutorials say         an absolute path always contains the root element       if this statement is to be believed  then no file system (no matter how obscure) can contain a path that the java api will consider absolute  unless it also considers it to contain a root .     you could argue that in some non-heirarchical file systems you might fall into some issues deciding whether a file can be it's own root. however  by this definition in the  path api  (emphasis mine)  a path should not represent a non-hierarchical element;        a path represents a path that is  hierarchical  and composed of a   sequence of directory and file name elements    "
          }
        ],
        "parent": "q_16555",
        "author": "Rudi"
      },
      {
        "date": "Dec 15, 2014 8:56:56 AM",
        "commentid": "a_20057",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": -0.3640000000000001,
            "sent": " the definition    the interface states the following about roots:        a root component  that identifies a file system hierarchy  may also be present.      so as you see  the comment seems to imply that roots are used for  file system hierarchies . ow we have to reason about what an absolute path is. the interface tells us the following:        an absolute path is complete in that it doesn't need to be combined with other path information in order to locate a file.      so  as you see  there is  no word about roots in the definition about absolute paths . the only restriction is that we have to be able to locate the file without further information.    hierarchical file systems    most file system are hierarchical  i.e.  they are trees (or graphs if we consider links) or forests. the root in a tree is a node that is not the child of another node (excluding links). windows file systems are  for example  forests  as they have many roots (    ...). linux has usually only one root which is  . roots are very important as without them it would be hard to start locating a file.  in such file systems  you can usually rely on each absolute path having a root.     non-hierarchical file systems     as long as we have a hierarchical file system  we can anticipate a root in an absolute path  but what if we don't have one? then  an absolute path might not contain a root.     an example that comes to my mind: distributed file systems like  chord . these are often not hierarchical so the meaning of roots is usually undefined. instead  a file hash identifies a file (sha-1 in chord). so a valid chord path might look like this:          this is an absolute path. one can retrieve the associated file without further information  so the path is absolute. however  i see no root. we  could  define the whole hash to be its own root (then each file would be its own root)  but nobody can guarantee that every person that implements a chord file system will agree to this. so there might be reasonable implementations that do not treat these hashes as roots. in such a file system  each path would be absolute  but none would contain a root.    if i would implement a non-hierarchical file system  i would always return   as root  as imho a root is not a defined concept in a non-hierarchical file system. since i think like this  other devs might think so as well. consequently  you may not assume that every absolute path has a root.    note that distributed file systems are quite common in many areas  so this is not merely a corner case that will never be implemented. i think you have to anticipate it.    conclusion      the interface does not mandate that each absolute path must have a root   there are reasonable file systems in which having no root makes sense   an oracle tutorial as mentioned in the comments is no contract for the interface. you should not rely on this      so there will be people implementing file systems without roots; you should anticipate this.  "
          }
        ],
        "parent": "q_16555",
        "author": "gexicide"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": -0.3640000000000001,
        "sent": " the javadoc for     says:        tells whether or not this path is absolute.     an absolute path is complete in that it doesn't need to be combined with other path information in order to locate a file.         returns:      if  and only if  this path is absolute      the javadoc for     says:        returns the root component of this path as a path object  or null if this path does not have a root component.         returns:    a path representing the root component of this path  or        ok  so  i am at a loss here; are there any filesystems out there for which a path may be absolute without a root at all?       edit: note that there can be paths which have a root but are not absolute. for instance  these on windows systems:       ;    .      but i am asking for the  reverse  here: no root and absolute.  "
      }
    ]
  },
  {
    "author": "Daniel Figueroa",
    "parent": "",
    "title": "\"Arrays should not be statically initialized by an array initializer. Why?\"",
    "commentid": "q_22792",
    "date": "Nov 19, 2014 4:41:00 AM",
    "children": [
      {
        "date": "Nov 19, 2014 4:58:40 AM",
        "commentid": "a_27616",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.6680000000000001,
            "sent": " it's an interesting question  and this decision is groundless imho. (i hope somebody else will answer this thread if there is a legit reason behind this design decision).    moreover  google shows how to format those static initializers in their good practice formatting guide  https://google-styleguide.googlecode.com/svn/trunk/javaguide.html#s4.8.3.1-array-initializers  without saying anything about how bad it is to use those constructs ...    i guess that the person behind that rule just had a tooth against that style of programming :)  "
          }
        ],
        "parent": "q_22792",
        "author": "Lo&#239;c Gammaitoni"
      },
      {
        "date": "Nov 19, 2014 5:03:37 AM",
        "commentid": "a_27617",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.6680000000000001,
            "sent": " i'd think that is because it is a special syntax that only works when initializing  .         the last form   is legal wether it is when creating the array or later on. so instead of mixing forms of initializing arrays you'd be better of using the same form always. imho that makes for clearer code  following the principle of least surprise.    strangely though the  google code style does  not prohibit this form of initialization which is very clear in this  example .  "
          }
        ],
        "parent": "q_22792",
        "author": "Daniel Figueroa"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.6680000000000001,
        "sent": " this is one of the rules from googles static analyser codepro analytix:      summary     arrays should not be statically initialized by an array initializer.     description     this audit rule checks for array variables that are initialized (either in the initializer or in an assignment statement) using an array initializer.     example     the following array declaration would be flagged because of the use of an array initializer:         now  i can disable it if i don't like it  that's not a problem. but i'm wondering why would this be a problem  and what would be the solution to keep that code from being flagged by the audit rule?  "
      }
    ]
  },
  {
    "author": "lbalazscs",
    "parent": "",
    "title": "Why there is no BooleanConsumer in Java 8?",
    "commentid": "q_484",
    "date": "Dec 29, 2014 7:34:48 PM",
    "children": [
      {
        "date": "Dec 30, 2014 2:13:23 AM",
        "commentid": "a_567",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.5,
            "sent": "   and   are needed to avoid the overhead autoboxing every value. it is much more efficent to be working on raw primitives. however  for boolean and byte every possible object is cached so there is little reason to avoid using   or    "
          }
        ],
        "parent": "q_484",
        "author": "Peter Lawrey"
      },
      {
        "date": "Dec 29, 2014 8:32:22 PM",
        "commentid": "a_568",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.5,
            "sent": "\" as other answers indicate there is no great reason to avoid    but then there's no great reason to avoid   either  so a different explanation is required for this.    a similar question is why can't you switch on a   value. the answer is that there's no need because you could always use   or  .    a   would really be nothing more than an   construct because the   method for a   could always be written in this form:         if you needed to pass such code around as data  you could just pass a pair of  s representing \\\"\"do something\\\"\" and \\\"\"do something else\\\"\". in many cases  you would only need one of the  s because one of the two blocks above would be empty.    in the same way  there is no need for a   because it would be nothing more than a pair of  s and there is no need for a a   because it would be nothing more than a pair of  s.    in contrast to this  it is not possible to break a   into two simpler objects.   \""
          }
        ],
        "parent": "q_484",
        "author": "pbabcdefp"
      },
      {
        "date": "Dec 29, 2014 7:56:10 PM",
        "commentid": "a_569",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.5,
            "sent": "\" you can write your own booleanconsumer  but in order to make it really useful  you would need to write your own booleanstream  too. there is an intstream  longstream  and doublestream  but no \\\"\"booleanstream\\\"\" (or \\\"\"shortstream\\\"\"  \\\"\"floatstream\\\"\" etc). it seems that these primitives were judged not to be important enough.    you can always use boolean objects instead of boolean primitives  and a boolean consumer to consume the values. example code:         mybooleanfunction returns a boolean  but using it in a map creates a stream of booleans (because we are in the generic  non-primitive stream. again  we have maptoint  maptolong  maptodouble to create an intstream etc  but no maptoboolean).    if you don't need stream support  you can still write and use a \\\"\"booleanconsumer\\\"\" in order to provide a type for some behavior  put i would prefer to see a functional interface with a more specific and descriptive name.  \""
          }
        ],
        "parent": "q_484",
        "author": "lbalazscs"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.5,
        "sent": "\" i'm afraid that this is somewhat a silly question.    is there anybody can tell me why there is no   opposite to    ?    is there any reason other than \\\"\"because simply there isn't\\\"\"?    should i create my own one? or am i missing something else?         update    where to use? i'm writing a library that uses much of consumers and suppliers. i successfully wrote a line with   and i encountered a situation that expecting a consumer accepting a boolean value which is from a method result. say  ?   \""
      }
    ]
  },
  {
    "author": "Evgeniy Dorofeev",
    "parent": "",
    "title": "\"What exactly does the final keyword guarantee regarding concurrency?\"",
    "commentid": "q_14725",
    "date": "Dec 2, 2014 9:23:57 AM",
    "children": [
      {
        "date": "Dec 2, 2014 9:48:26 AM",
        "commentid": "a_17922",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8240000000000001,
            "sent": " what you are saying is true.    marking a field as final forces the compiler to complete initialization of the field before the constructor completes. there is no such guarantee however for non-final fields. this might seem weird  however there are many things done by the compiler and jvm for optimization purposes such as reordering instructions  that cause such stuff to occur.    the final keyword has many more benefits. from the java concurecncy in practice:        final fields can't be modified (although the objects they refer to can be modified if they are mutable)  but they       also have special semantics under the java memory model. it is the use of final fields that makes possible the guarantee       of initialization safety (see section 3.5.2) that lets immutable objects be freely accessed and shared without       synchronization.      the books says then:         to publish an object safely  both the reference to the object and the object's state must be made visible to other       threads at the same time. a properly constructed object can be safely published by:            initializing an object reference from a static initializer;     storing a reference to it into a volatile field or atomicreference;      storing a reference to it into a final field of a properly constructed object;  or     storing a reference to it into a field that is properly guarded by a lock.        "
          }
        ],
        "parent": "q_14725",
        "author": "javaHunter"
      },
      {
        "date": "Dec 2, 2014 3:16:09 PM",
        "commentid": "a_17923",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8240000000000001,
            "sent": " i think your question is answered by jls right below the part you quote  in  section 17.5.1: semantics of final fields :        given a write  w   a freeze  f   an action  a  (that is not a read of a final field)  a read  r 1   of the final field frozen by  f   and a read  r 2   such that  hb ( w    f )   hb ( f    a )   mc ( a    r 1  )  and  dereferences ( r 1     r 2  )  then when determining which values can be seen by  r 2    we consider  hb ( w    r 2  ).      let's break it down in terms of the question:       w : is the write to   by thread 1    f : is the freeze action that freezes      a : publish of the   object reference    r 1  : a read of   (frozen by  f ) by thread 2    r 2  : a reaf of   by thread 2      we note that       hb ( w    f ): the write to   happens before the freeze of      hb ( f    a ): the   reference is published after the constructor is finished (i.e. after the freeze)    mc ( a    r 1  ): thread 2 reads the   reference before reading      dereferences ( r 1     r 2  ): thread 2 dereferences the value of   by accessing        the following sentence...        \\\"then when determining which values can be seen by  r 2    we consider  hb ( w    r 2  )\\\"      ...then says that        when determining which values can be seen by the read of   we consider that the write to   by thread 1  happens before  the read of  .      i.e. thread 2 is guaranteed to see the value   for  .        a relevant quote  by bill pugh:        the ability to see the correctly constructed value for the field is nice  but if the field itself is a reference  then you also want your code to see the up to date values for the object (or array) to which it points. if your field is a final field  this is also guaranteed. so  you can have a final pointer to an array and not have to worry about other threads seeing the correct values for the array reference  but incorrect values for the contents of the array. again  by \\\"correct\\\" here  we mean \\\"up to date as of the end of the object's constructor\\\"  not \\\"the latest value available\\\".      in particular this is a direct answer to the example @supercat brought up regarding illsynchronized sharing of   references.  "
          }
        ],
        "parent": "q_14725",
        "author": "aioobe"
      },
      {
        "date": "Dec 2, 2014 10:20:34 AM",
        "commentid": "a_17924",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8240000000000001,
            "sent": " it's worth mentioning that   here serves the same purpose as   in terms of visibility of values to threads. that said  you cannot use both   and   on a field as they are redundant to each other. back to your question. as others have pointed out your assumption is wrong as jls only guarantees the visibility of the reference to b  not the non-final fields defined in b. however  you can make b behaves in the way you want it to behave. one solution is to declare   as   if it can't be  .  "
          }
        ],
        "parent": "q_14725",
        "author": "neurite"
      },
      {
        "date": "Dec 2, 2014 9:54:16 AM",
        "commentid": "a_17925",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8240000000000001,
            "sent": "      the above line only guarantees that b will be initialized when you access b from an instance of a. now the detail of b's initialization or any instantiation of b is completely dependent on how b is defined and in this case it may get initialized as per jls or may not.    so  if you do a a = new a(); from one thread and somehow manage to read a.b from another thread  it is guaranteed you will not see null if a is not null but b.bnotfinal may be still zero.  "
          }
        ],
        "parent": "q_14725",
        "author": "yadab"
      },
      {
        "date": "Dec 2, 2014 9:39:00 PM",
        "commentid": "a_17926",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8240000000000001,
            "sent": " \\\"jsr 133 (java memory model) faq  jeremy manson and brian goetz  february 2004\\\" describes how does   field work.       how can final fields appear to change their values?     how do final fields work under the new jmm?        quote:        the goals of jsr 133 include:            a new guarantee of  initialization safety  should be provided. if an object is properly constructed (which means that references to it do not escape during construction)  then all threads which see a reference to that object will also see the values for its final fields that were set in the constructor  without the need for synchronization.         "
          }
        ],
        "parent": "q_14725",
        "author": "yohjp"
      },
      {
        "date": "Dec 9, 2014 1:01:06 PM",
        "commentid": "a_17927",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8240000000000001,
            "sent": "  there is a memory barrier inserted after a final assignment  (it's a store fence)  this is how you guarantee that the other threads will see the value that you assign. i like the jls and how it says that things are done with happens-before/after and guarantees of the final  but to me  the memory barrier and it's effects are much simpler to grasp.      you should really read this :  memory barriers   "
          }
        ],
        "parent": "q_14725",
        "author": "Eugene"
      },
      {
        "date": "Dec 2, 2014 9:46:35 AM",
        "commentid": "a_17928",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8240000000000001,
            "sent": " see this broken double-checked locking           does not guarantee that   points to an initialized instance of  . jvm is allowed to first create and publish an instance of   and then assign 1 to  .  "
          }
        ],
        "parent": "q_14725",
        "author": "Evgeniy Dorofeev"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.8240000000000001,
        "sent": " i think i've read that the final keyword on a field guarantees that if thread 1 instantiates the object containing the field  then thread 2 will always see the initialized value of that field if thread 2 has a reference to the object (provided it was properly constructed). it also says in the jls that         [thread 2] will also see versions of any object or array referenced by   those final fields that are at least as up-to-date as the final fields   are.  (section 17.5 of jls)       that implies that if i have class a         and class b         then anotfinal is not guaranteed to be initialized by the time thread 2 gets a reference to class a  but field bnotfinal is  because b is an object referenced by a final field  as specified in the jls.    do i have this right?    edit:    a scenario where this could happen would be if we have two threads concurrently executing geta() on the same instance of a class c       "
      }
    ]
  },
  {
    "author": "user270349",
    "parent": "",
    "title": "\"Does a lambda expression create an object on the heap every time it&#39;s executed?\"",
    "commentid": "q_5815",
    "date": "Dec 17, 2014 4:20:59 AM",
    "children": [
      {
        "date": "Dec 17, 2014 4:27:20 AM",
        "commentid": "a_7009",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.5,
            "sent": " it is equivalent but not identical. simply said  if a lambda expression does not captures values  it will be a singleton that is re-used on every invocation.    the behavior is not exactly specified. the jvm is given a big freedom how to implement it. currently  oracle?s jvm creates (at least) one instance per lambda expression (i.e. doesn?t share instance between different identical expressions) but creates singleton for all expression which don?t capture values.    you may read  this answer  for more details. there  i not only gave a more detailed description but also testing code to observe the current behavior.       this is covered by the the java? language specification  chapter ? 15.27.4. run-time evaluation of lambda expressions ?    summarized:        these rules are meant to offer flexibility to implementations of the java programming language  in that:             a new object need not be allocated on every evaluation.       objects produced by different lambda expressions need not belong to different classes (if the bodies are identical  for example).       every object produced by evaluation need not belong to the same class (captured local variables might be inlined  for example).       if an \\\"existing instance\\\" is available  it need not have been created at a previous lambda evaluation (it might have been allocated during the enclosing class's initialization  for example).         "
          }
        ],
        "parent": "q_5815",
        "author": "Holger"
      },
      {
        "date": "Dec 17, 2014 4:27:56 AM",
        "commentid": "a_7010",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.5,
            "sent": " when an instance representing the lambda is created sensitively depends on the exact contents of your lambda's body. namely  the key factor is what the lambda  captures  from the lexical environment. if it doesn't capture any state which is variable from creation to creation  then an instance will not be created each time the for-each loop is entered. instead a synthetic method will be generated at compile time and the lambda use site will just receive a singleton object that delegates to that method.    further note that this aspect is implementation-dependent and you can expect future refinements and advancements on hotspot towards greater efficiency. there are general plans to e.g. make a lightweight object without a full corresponding class  which has just enough information to forward to a single method.    here is a good  accessible in-depth article on the topic:     http://www.infoq.com/articles/java-8-lambdas-a-peek-under-the-hood   "
          }
        ],
        "parent": "q_5815",
        "author": "Marko Topolnik"
      },
      {
        "date": "Dec 17, 2014 4:24:35 AM",
        "commentid": "a_7011",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.5,
            "sent": " you are passing a new instance to the   method. every time you do that you create a new object but not one for every loop iteration. iteration is done inside   method using the same 'callback' object instance until it is done with the loop.    so the memory used by the loop does not depend on the size of the collection.        isn't this equivalent to the 'old syntax' snippet?      yes. it has slight differences at a very low level but i don't think you should care about them. lamba expressions use the invokedynamic feature instead of anonymous classes.  "
          }
        ],
        "parent": "q_5815",
        "author": "user270349"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.5,
        "sent": " when i iterate over a collection using the new syntactic sugar of java8  such as         isn't this equivalent to the 'old syntax' snippet below?         does this mean a new anonymous   object is created on the heap everytime i iterate over a collection? how much heap space does this take? what performance implications does it have? does it mean i should rather use the old style for loops when iterating over large multi-level data structures?  "
      }
    ]
  },
  {
    "author": "edTarik",
    "parent": "",
    "title": "\"Why does this Java code with &quot;+ +&quot; compile?\"",
    "commentid": "q_108605",
    "date": "Jun 18, 2014 12:12:12 AM",
    "children": [
      {
        "date": "Jun 18, 2014 12:18:32 AM",
        "commentid": "a_133247",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.3999999999999999,
            "sent": " it is  the unary plus   twice. it is not a prefix increment because there is a space. java does consider whitespace under many circumstances.    the unary plus basically does nothing  it just promotes the operand.    for example  this doesn't compile  because the unary plus causes the   to be promoted to  :       "
          }
        ],
        "parent": "q_108605",
        "author": "Radiodef"
      },
      {
        "date": "Jun 18, 2014 12:17:27 AM",
        "commentid": "a_133248",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.3999999999999999,
            "sent": " in this case  java considers the   operator as a unary operator and hence the result is  .  just try with the following code:       "
          }
        ],
        "parent": "q_108605",
        "author": "user3694267"
      },
      {
        "date": "Jun 18, 2014 12:17:16 AM",
        "commentid": "a_133249",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.3999999999999999,
            "sent": " the  'es are the  unary plus operator   indicating sign of the value to the right of it  not the  addition operator  used when adding two values.    consider this line          which is equivalent to this         which again is equivalent to this         and is similar (in form) to          which probably is more straightforwardly understandable.    and by the way    is not equivalent  nor evaluated by the parser as the  increment operator   !  "
          }
        ],
        "parent": "q_108605",
        "author": "Anders R. Bystrup"
      },
      {
        "date": "Jun 18, 2014 12:22:54 AM",
        "commentid": "a_133250",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.3999999999999999,
            "sent": "   and   are both operators. they are elements of the language. technically they can be referred to as  tokens .    is not a single token in java and is broken down into two individual tokens during the parsing stages of compilation.      can exist in two forms: (i) as a  unary  operator; for example   (which is essentially a no-op which only adds to the confusion) or (ii) as a  binary  operator (meaning it acts on two thing) to add two terms; for example  .    since they have an intrinsic value  you can also regard   and   as both being  expressions . you should also note that the unary   has a very high operator precedence so it will bind tightly to the expression immediately after it.    so what is happening in your code is the the compiler is binding the   just to the left of the expression 10 to produce another expression with value 10. then that binds the second leftmost unary   to produce  yet again  10.    in summary: without the space    is a single token. with any space between them  the two   act as two unary operators.  "
          }
        ],
        "parent": "q_108605",
        "author": "Bathsheba"
      },
      {
        "date": "Jun 18, 2014 12:23:05 AM",
        "commentid": "a_133251",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.3999999999999999,
            "sent": " it's just the unary plus. if you compile   you will get the following bytecode:         which is exactly equivalent to    "
          }
        ],
        "parent": "q_108605",
        "author": "manouti"
      },
      {
        "date": "Jun 18, 2014 12:25:47 AM",
        "commentid": "a_133252",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.3999999999999999,
            "sent": " do not confound the preincrement operator   and the unary operator   repeated twice.       "
          }
        ],
        "parent": "q_108605",
        "author": "fluminis"
      },
      {
        "date": "Jun 18, 2014 2:35:09 PM",
        "commentid": "a_133253",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.3999999999999999,
            "sent": " so everyone has said that it is a unary plus that promotes its operand to an integer value. i noticed a few comments about how java allows useless constructs. well  so does every other language  including formal mathematics.    this is not a java specific thing. it is an impossible task for compiler writers to try to \\\"predict\\\" every useless thing a programmer could potentially write within a grammar  and try to restrict those by adding semantic checks in the compiler. plus  \\\"useless\\\" is a subjective term. one man's useless is another man's learning tool. you leave that up to the programmer. if he wants to write useless code  so be it. as long as it produces  correct  code  we should be satisfied.    code is like math. it may be useful  or not. but it just  is   just like the number 42. it is neither useful or useless  depending on context. it is just 42.     what we don't need is any extra semantic code within the java compiler to restrict programmers. there is enough bondage in the language already.   "
          }
        ],
        "parent": "q_108605",
        "author": "mrjoltcola"
      },
      {
        "date": "Jun 18, 2014 12:16:13 AM",
        "commentid": "a_133254",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.3999999999999999,
            "sent": " 10 is a literal not a variable. as such  10 cannot be incremented.       "
          }
        ],
        "parent": "q_108605",
        "author": "edTarik"
      }
    ],
    "sent": [
      {
        "topicid": 3,
        "systemtopicid": 3,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Servlets",
        "linePolarity": -0.3999999999999999,
        "sent": " i'm curious why this simple program could be compiled by java using intellij (java 7).         the output is still 10.  "
      }
    ]
  },
  {
    "author": "CommuSoft",
    "parent": "",
    "title": "\"Difference between java enum with no values and utility class with private constructor\"",
    "commentid": "q_36020",
    "date": "Oct 28, 2014 2:33:52 PM",
    "children": [
      {
        "date": "Oct 28, 2014 3:31:46 PM",
        "commentid": "a_43352",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8879999999999999,
            "sent": " using   for something that's not actually an enum is ugly and confusing. i'd say that's enough reason not to do it.     the \\\"utility class\\\" pattern is perfectly legitimate. if your tools don't like it  that's a problem with the tools.  "
          }
        ],
        "parent": "q_36020",
        "author": "Mike Baranczak"
      },
      {
        "date": "Oct 28, 2014 3:07:18 PM",
        "commentid": "a_43353",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8879999999999999,
            "sent": " one benefit is that you're absolutely guaranteed that no instances can be created  even from inside the class.    a disadvantage is that this goes beyond the usual intent of enums. however  we already do this for singletons implemented with enums.    this bit from \\\"effective java\\\" by joshua bloch about such singletons also applies to utility classes:        ...you get an ironclad guarantee that there can be no instances besides the declared constants. the jvm makes this guarantee  and you can depend on it.      disclaimer: i have not used this pattern  and am not recommending for or against it.   "
          }
        ],
        "parent": "q_36020",
        "author": "Andy Thomas"
      },
      {
        "date": "Oct 28, 2014 3:15:53 PM",
        "commentid": "a_43354",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8879999999999999,
            "sent": " the following pattern in utility classes also provides ironclad guarantee that there can be no instances:         besides  you have no additional irrelevant static methods  provided by enums.  "
          }
        ],
        "parent": "q_36020",
        "author": "ursa"
      },
      {
        "date": "Oct 28, 2014 3:11:49 PM",
        "commentid": "a_43355",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8879999999999999,
            "sent": " the only difference is that you can still call the constructor  inside your class :         but since you're the designer of that class  it wouldn't make any sense to break your own code contracts.    in general  most software design books i've ever read  are against using  static methods . unless they are really utility methods: in the sense they will  never ever  require any state. and even then  it's only a small effort to implement a  singleton  pattern such that  when the time would come  you can assign state to it:         and calling it with  . it would be way harder to perform an  introduce state  transformation later on in the coding process.  thus static methods are harder to maintain.     the reason why instances are useful is that you can use them in a lot of patterns like  strategy  if at one occasion it depends on something what should be done ... by using   methods  one makes the methods less dynamic because java doesn't support method pointers (for good reasons).  thus non-static methods are more dynamic and useful.     furthermore some security researchers argue that it is harder to analyze code with   modifiers since they can be accessed from anywhere and the side effects are less predictable (for instance in an automatic security analysis tool): say you have an class that is not fully implemented  then you can still analyze the fields to know which methods it can access and thus analyze the possible side effects (network usage  file io ...). this can generate a list of possible hazards per class that should be verified. at least if i understood the phd dissertation of one of my fellow researchers correct.  thus non-static methods allow more modifier analysis.     to conclude: java was built on the principle of object-oriented programming. this means that the \\\" c lass world\\\" is used by the  c ompiler  and the \\\" i nstance world\\\" by the  i nterpreter/runtime. i agree there are a lot of conflicts between the two words. but   methods are in many/some cases a mistake to resolve such conflicts.  "
          }
        ],
        "parent": "q_36020",
        "author": "CommuSoft"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.8879999999999999,
        "sent": " a common thing to do to utility classes is to  give them a private constructor :         but unfortunately  some tools don't like that private constructor. they may warn that it's never called within the class  that it's not covered by tests  that the block doesn't contain a comment  etc.    a lot of those warnings go away if you do this instead:         my question is: besides the unending hatred of future developers  what important differences are there between an enum with no values and a class with a private constructor in java?    note that i am  not  asking  what's the advantage of a java enum versus a class with public static final fields? . i'm not deciding between whether a list of things should be a bunch of constants or an enum  i'm deciding between putting a bunch of functions in a constructor-less class or a value-less enum.    also note that i don't actually want to do this. i just want to know the trade-offs as part of general language knowledge.    for example  using an enum pollutes the autocomplete with useless methods like  . what other downsides are there? upsides?  "
      }
    ]
  },
  {
    "author": "jdphenix",
    "parent": "",
    "title": "\"C++ and PHP vs C# and Java - unequal results\"",
    "commentid": "q_76746",
    "date": "Aug 15, 2014 1:11:18 AM",
    "children": [
      {
        "date": "Aug 15, 2014 1:29:53 AM",
        "commentid": "a_93673",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -1.2,
            "sent": " the c++ standard states:         with respect to an indeterminately-sequenced function call  the operation of a compound assignment is a single evaluation. [ note: therefore  a function call shall not intervene between the lvalue-to-rvalue conversion and the side effect associated with any single compound assignment operator. ?end note ]        ?5.17 [expr.ass]      hence  as in the same evaluation you use   and a function with a side effect on    the result is undefined  because:        if a side effect on a scalar object is unsequenced relative to either another side effect on the same scalar object or a value computation using the value of the same scalar object  the behavior is undefined.        ?1.9 [intro.execution]      it happens to be 11 on many compilers  but there is no guarantee that a c++ compiler won't give you 1 as for the other languages.     if you're still skeptical  another analysis of the standard leads to the same conclusion:  the standard also says in the same section as above:          the behavior of an expression of the form   is equivalent to   except that   is evaluated only once.      in you case   except that   is evaluated only once.  as there is no guarantee on the order of evaluation  in    you cannot take for granted that first f is evaluated and then  .       addendum:   i'm not a java expert  but the java rules clearly specify the order of evaluation in an expression  which is guaranteed to be from left to right in section 15.7 of  java specifications . in section  15.26.2. compound assignment operators  the java specs also say that   is equivalent to  .      in your java programm this means again that your expression is equivalent to   and first   is evaluated  then  .  so the side effect of   is not taken into account in the result.      so your java compiler doesn't have a bug. it just complies with the specifications.    "
          }
        ],
        "parent": "q_76746",
        "author": "Christophe"
      },
      {
        "date": "Aug 15, 2014 2:23:44 AM",
        "commentid": "a_93674",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -1.2,
            "sent": "   edit:  thanks to comments by deduplicator and user694733  here is a modified version of my original answer.        the c++ version has  undefined  unspecified  behaviour.    there is a subtle difference between \\\"undefined\\\" and \\\"unspecified\\\"  in that the former allows a program to do  anything  (including crashing) whereas the latter allows it to choose from a set of particular allowed behaviours without dictating which choice is correct.    except of very rare cases  you will always want to avoid both.       a good starting point to understand whole issue are the c++ faqs  why do some people think x = ++y + y++ is bad?     what?s the value of i++ + i++?  and  what?s the deal with ?sequence points?? :         between the previous and next sequence point  a scalar object shall   have its stored  value modified at most once  by the evaluation of an   expression.      (...)        basically  in c and c++  if you read a variable twice in an expression   where you also write it  the result is  undefined .      (...)        at certain specified points in the execution sequence called sequence   points  all side effects of previous evaluations shall be complete and   no side effects of subsequent evaluations shall have taken place. (...)   the ?certain specified points? that are called  sequence points are  (...)    after evaluation of all a function?s parameters  but before the first   expression within the function is executed.      in short  modifying a variable twice between two consecutive sequence points yields undefined behaviour  but a function call introduces an intermediate sequence point (actually  two intermediate sequence points  because the return statement creates another one).    this means the fact that you have a function call in your expression \\\"saves\\\" your   line from being undefined and turns it into \\\"only\\\" unspecified.    both 1 and 11 are possible and correct outcomes  whereas printing 123  crashing or sending an insulting e-mail to your boss are not allowed behaviours; you'll just never get a guarantee whether 1 or 11 will be printed.       the following example is slightly different. it's seemingly a simplification of the original code but really serves to highlight the difference between undefined and unspecified behaviour:         here the behaviour is indeed undefined  because the function call has gone away  so both modifications of   occur between two consecutive sequence points. the compiler is allowed by the c++ language specification to create a program which prints 123  crashes or sends an insulting e-mail to your boss.    (the e-mail thing of course is just a very common humorous attempt at explaining how  undefined  really means  anything goes . crashes are often a more realistic result of undefined behaviour.)    in fact  the   (just like the return statement in your original code) is a red herring. the following yields undefined behaviour  too:         this  may  print 20 (it does so on my machine with vc++ 2013) but the behaviour is still undefined.    (note: this applies to built-in operators. operator overloading changes the behaviour back to  specified   because overloaded operators copy the  syntax  from the built-in ones but have the  semantics  of functions  which means that an overloaded   operator of a custom type that appears in an expression is actually a  function call . therefore  not only are sequence points introduced but the entire ambiguity goes away  the expression becoming equivalent to    which has guaranteed order of argument evaluation. this is probably irrelevant to your question but should be mentioned anyway.)    in contrast  the java version          must  print 10. this is because java has neither undefined nor unspecified behaviour with regards to evaluation order. there are no sequence points to be concerned about. see  java language specification 15.7. evaluation order :        the java programming language guarantees that the operands of   operators appear to be evaluated in a specific evaluation order    namely  from left to right.      so in the java case     interpreted from left to right  means that first something is added to  0   and that something is  0 + 10 . hence  0 + (0 + 10) = 10 .    see also example 15.7.1-2 in the java specification.    going back to your original example  this also means that the more complex example with the static variable has defined and specified behaviour in java.       honestly  i don't know about c# and php but i would guess that both of them have some guaranteed evaluation order as well. c++  unlike most other programming languages (but like c) tends to allow much more undefined and unspecified behaviour than other languages. that's not good or bad. it's a  tradeoff between robustness and efficiency . choosing the right programming language for a particular task or project is always a matter of analysing tradeoffs.    in any case  expressions with such side effects are  bad programming style in all four languages .    one final word:        i found a little bug in c# and java.      you should not assume to find bugs in  language specifications  or  compilers  if you don't have many years of professional experience as a software engineer.  "
          }
        ],
        "parent": "q_76746",
        "author": "Christian Hackl"
      },
      {
        "date": "Aug 15, 2014 1:43:21 AM",
        "commentid": "a_93675",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -1.2,
            "sent": " as christophe has already written  this is basically an undefined operation.    so why does c++ and php does it one way  and c# and java the other way?    in this case (which may be different for different compilers and platforms)  the order of evaluation of arguments in c++ is inverted compared to c# - c# evaluates arguments in order of writing  while the c++ sample does it the other way around. this boils down to the default calling conventions both use  but again - for c++  this is an undefined operation  so it may differ based on other conditions.    to illustrate  this c# code:         will produce   on output  rather than  .    that's simply because c# evaluates \\\"in order\\\"  so in your example  it first reads   and then calls    while in mine  it first calls   and then reads  .    now  this still might be unrealiable. il (.net's bytecode) has   as pretty much any other method  but optimizations by the jit compiler  might  result in a different order of evaluation. on the other hand  since c# (and .net)  does  define the order of evaluation / execution  so i guess a compliant compiler should  always  produce this result.    in any case  that's a lovely unexpected outcome you've found  and a cautionary tale - side-effects in methods can be a problem even in imperative languages :)    oh  and of course -   means something different in c# vs. c++. i've seen that mistake made by c++ers coming to c# before.     edit :    let me just expand a bit on the \\\"different languages\\\" issue. you've automatically assumed  that c++'s result is the correct one  because when you're doing the calculation manually  you're doing the evaluation in a certain order - and you've determined this order to comply with the results from c++. however  neither c++ nor c# do analysis on the expression - it's simply a bunch of operations over some values.    c++  does  store   in a register  just like c#. it's just that c# stores it  before  evaluating the method call  while c++ does it  after . if you change the c++ code to do   instead  just like i've done in c#  i expect you'll get the   on output.    the most important part is that c++ (and c) simply didn't specify an explicit order of operations  probably because it wanted to exploit architectures and platforms that do either one of those orders. since c# and java were developed in a time when this doesn't really matter anymore  and since they could learn from all those failures of c/c++  they specified an explicit order of evaluation.  "
          }
        ],
        "parent": "q_76746",
        "author": "Luaan"
      },
      {
        "date": "Aug 15, 2014 1:51:43 AM",
        "commentid": "a_93676",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -1.2,
            "sent": " according to the java language specification:          jls 15.26.2  compound assignment operators         a compound assignment expression of the form       is equivalent to         where       is the type of         except that    1   is evaluated   only once.      this small program demonstrates the difference  and exhibits expected behavior based on this standard.          in order          is assigned to value of  .      is assigned to the value of the   result of retrieving    which is    and    applying the addition operator to   with   the return value of invoking    which is     and assigning the result of   to  .      i also declared   to show how changing the order of invocation changes the result.         is assigned to value of the   the return value of invoking    with is    and   applying the addition operator to that value with    the value of   (which is 10  a side effect of     and assigning the result of   to  .       in java  this is  not  undefined behavior.     edit:     to address your point regarding local copies of variables. that is correct  but it has nothing to do with  . java saves the result of evaluating each side (left side first)  then evaluates result of performing the operator on the saved values.   "
          }
        ],
        "parent": "q_76746",
        "author": "jdphenix"
      }
    ],
    "sent": [
      {
        "topicid": 2,
        "systemtopicid": 2,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Loops",
        "linePolarity": -1.2,
        "sent": " i found something a little strange in c# and java. let's look at this c++ code:         in a console you will see x = 11 ( look at the result here - ideone c++ ).    now let's look at the same code on c#:         in a console you will see 1 (not 11!) (look at the result here -  ideone c#  i know what you thinking now - \\\"how that is possible?\\\"  but let's go to the following code.    java code:         result the same as in c# (x = 1  look at the result  here ).    and for the last time let's look at the php code:         result is 11 (look at the result  here ).    i have a little theory - these languages (c# and java) are making a local copy of static variable x on the stack (are they ignoring the  static  keyword?). and that is reason why result in those languages is 1.    is somebody here  who have other versions?  "
      }
    ]
  },
  {
    "author": "TheLostMind",
    "parent": "",
    "title": "\"Does verification of byte code happen twice?\"",
    "commentid": "q_70250",
    "date": "Aug 27, 2014 11:04:43 PM",
    "children": [
      {
        "date": "Aug 27, 2014 11:10:23 PM",
        "commentid": "a_85697",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3640000000000001,
            "sent": " you may understand the byte code verification using this diagram which is in detail explained in  oracle docs          you will find that the byte code verification happens only once not twice        the illustration shows the flow of data and control from java language   source code through the java compiler  to the class loader and   bytecode verifier and hence on to the java virtual machine  which   contains the interpreter and runtime system. the important issue is   that the java class loader and the bytecode verifier make no   assumptions about the primary source of the bytecode stream--the code   may have come from the local system  or it may have travelled halfway   around the planet. the bytecode verifier acts as a sort of gatekeeper:   it ensures that code passed to the java interpreter is in a fit state   to be executed and can run without fear of breaking the java   interpreter. imported code is not allowed to execute by any means   until after it has passed the verifier's tests. once the verifier is   done  a number of important properties are known:            there are no operand stack overflows or underflows     the types of the parameters of all bytecode instructions are known to    always be correct     object field accesses are known to be legal--private  public  or    protected            while all this checking appears excruciatingly detailed  by the time   the bytecode verifier has done its work  the java interpreter can   proceed  knowing that the code will run securely. knowing these   properties makes the java interpreter much faster  because it doesn't   have to check anything. there are no operand type checks and no stack   overflow checks. the interpreter can thus function at full speed   without compromising reliability.       edit:-     from oracle docs  section 5.3.2 :        when the loadclass method of the class loader l is invoked with the   name n of a class or interface c to be loaded  l must perform one of   the following two operations in order to load c:            the class loader l can create an array of bytes representing c as the    bytes of a classfile structure (?4.1); it then must invoke the   method    defineclass of class classloader. invoking defineclass   causes the    java virtual machine to derive a class or interface   denoted by n    using l from the array of bytes using the algorithm   found in ?5.3.5.     the class loader l can delegate the loading of c to some other class    loader l'. this is accomplished by passing the argument n   directly or    indirectly to an invocation of a method on l'   (typically the    loadclass method). the result of the invocation is   c.          as correctly commented by holger  trying to explain it more with the help of an  example :         the corresponding byte code would be          note that most of the instructions in jvm are typed.    now you should note that proper operation of the jvm is not guaranteed unless the code meets at least the following conditions:      type correctness: the arguments of an instruction are always of the types expected by the instruction.   no stack over?ow or under?ow: an instruction never pops an argument o? an empty stack  nor pushes a result on a full stack (whose size is equal to the maximal stack size declared for the method).   code containment: the program counter must always point within the code for the method  to the beginning of a valid instruction encoding (no falling o? the end of the method code; no branches into the middle of an instruction encoding).   register initialization: a load from a register must always follow at least one store in this register; in other terms  registers that do ot correspond to method parameters are not initialized on method entrance  and it is an error to load from an uninitialized register.   object initialization: when an instance of a class c is created  one of the initialization methods for class c (corresponding to the constructors for this class) must be invoked before the class instance can be used.       the purpose of byte code verification is to check these condition once and for all   by static analysis of the byte code at load time. byte code that passes verfification can then be executed faster.    also to note that byte code verification purpose is to shift the verfification listed above from run time to load time.     the above explanation has been taken from  java bytecode veri?cation: algorithms and formalizations    "
          }
        ],
        "parent": "q_70250",
        "author": "Rahul Tripathi"
      },
      {
        "date": "Aug 27, 2014 11:11:20 PM",
        "commentid": "a_85698",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3640000000000001,
            "sent": " no.     from the  jvm spec 4.10 :         even though a compiler for the java programming language must only produce class files that satisfy all the static and structural constraints in the previous sections  the java virtual machine has no guarantee that any file it is asked to load was generated by that compiler or is properly formed.      and then proceeds specify the verification process.     and  jvm spec 5.4.1 :        verification (?4.10) ensures that the binary representation of a class or interface is structurally correct (?4.9). verification may cause additional classes and interfaces to be loaded (?5.3) but need not cause them to be verified or prepared.       the section specifying linking references ?4.10 - not as a separate process but part of loading the classes.     the jvm and jls are great documents when you have a question like this.   "
          }
        ],
        "parent": "q_70250",
        "author": "jdphenix"
      },
      {
        "date": "Aug 27, 2014 11:12:06 PM",
        "commentid": "a_85699",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3640000000000001,
            "sent": "  no such two time verification      no   as far as verification is concerned look closely that how the program written in java goes through various phases in the following image you will see that there is  no such two time verification  but the code is verified just once.            edit  ? the programmer writes the program (preferably on a notepad) and saves it as a ?.java? file  which is then further used for compilation  by the compiler.      compile  ? the compiler here takes the    ?.java? file  compiles it and looks for any possible errors in the    scope of the program. if it finds any error  it reports them to the    programmer. if no error is there  then the program is converted into    the bytecode and saved as a ?.class? file.      load  ? now the major purpose of the component called ?class loader? is to load the byte code in the jvm. it doesn?t execute the code yet  but just loads it into the jvm?s memory.       verify  ? after loading the    code  the jvm?s subpart called ?byte code verifier? checks the    bytecode and verifies it for its authenticity. it also checks if the    bytecode has any such code which might lead to some malicious    outcome. this component of the jvm ensures security.      execute  ? the next component is the execution engine. the execution engine interprets the code line by line using the just in time (jit) compiler. the jit compiler does the execution pretty fast but consumes extra cache memory.     "
          }
        ],
        "parent": "q_70250",
        "author": "nobalG"
      },
      {
        "date": "Aug 27, 2014 11:15:48 PM",
        "commentid": "a_85700",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3640000000000001,
            "sent": " the spec lists 4 phases in bytecode verification. these steps are functionally distinct  not to be mistaken with repeating the same thing. just like a multi-pass compiler uses each pass to setup for the next pass  phases are not repetition  but are orchestrated for a single overall purpose  each phase accomplishes certain tasks.    unless the bytecode is changed  there is no reason to verify it twice.    the verification is described here.     http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.10   "
          }
        ],
        "parent": "q_70250",
        "author": "mrjoltcola"
      },
      {
        "date": "Aug 28, 2014 8:28:12 AM",
        "commentid": "a_85701",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3640000000000001,
            "sent": " verification of  code  happens  twice . once during  compilation  (compilation fails if the code has flaws  threats) and again after  the class is loaded  into memory during execution (actual byte-code verification happens here). yes  this happens  along with the process of loading classes (by class loaders)   but the class loaders themselves might not act as verifiers. its the jvm (or rather the verifier present in the jvm) that does the verification.   "
          }
        ],
        "parent": "q_70250",
        "author": "TheLostMind"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.3640000000000001,
        "sent": " so i am a little confused regarding the verification of bytecode that happens inside a jvm. according to the book by  deitel and deitel   a java program goes through five phases (edit  compile  load  verify and execute) (chapter 1). the bytecode verifier verifies the bytecode during the 'verify' stage. nowhere does the book mention that the bytecode verifier is a part of the classloader.    however according to   docs of oracle    the classloader performs the task of loading  linking and initialization  and during the process of linking it has to verify the bytecode.     now  are the bytecode verification that deitel and deitel talks about  and the bytecode verification that   this oracle document   talks about  the same process?    or does bytecode verification happen twice  once during the linking process and the other by the bytecode verifier?    picture describing phases of a java program as mentioned in book by dietel and dietel.(i borrowed this pic from one of the answers below by  nobalg  :) )    "
      }
    ]
  },
  {
    "author": "Bathsheba",
    "parent": "",
    "title": "\"Java classes and static blocks\"",
    "commentid": "q_89682",
    "date": "Jul 22, 2014 11:56:20 AM",
    "children": [
      {
        "date": "Jul 22, 2014 11:58:58 AM",
        "commentid": "a_109988",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.5,
            "sent": " in the \\\"detailed initialization procedure\\\" for classes   section 12.4.2 of the jls  states:        next  execute either the class variable initializers and static initializers of the class  or the field initializers of the interface   in textual order   as though they were a single block.      this means that it's as if the first example was:         and the second example was:         the last assignment \\\"wins\\\"  explaining your output.  "
          }
        ],
        "parent": "q_89682",
        "author": "rgettman"
      },
      {
        "date": "Jul 22, 2014 11:59:28 AM",
        "commentid": "a_109989",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.5,
            "sent": " static blocks and static variables are initialized in the order in which they appear in the source. if your code is:         the result is 100.  "
          }
        ],
        "parent": "q_89682",
        "author": "Zhenxiao Hao"
      },
      {
        "date": "Jul 22, 2014 12:18:08 PM",
        "commentid": "a_109990",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.5,
            "sent": " besides answering the question of how the code is executed in what order  i am guessing you also want to know why a static block can refer to a static variable that has not been textually declared/executed yet.    while  section 12.4.2 of the jls  does explain that static blocks and static variable are executed in the textual order that they appear   section 8.3.3 of the jls  explains when you can reference what  and you can see that the condition of   fails  allowing your static block in the second example to refer to a static variable that has not textually in order been declared/executed yet.  "
          }
        ],
        "parent": "q_89682",
        "author": "NESPowerGlove"
      },
      {
        "date": "Jul 22, 2014 11:59:02 AM",
        "commentid": "a_109991",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.5,
            "sent": " statics are evaluated in the order they appear in the program.  "
          }
        ],
        "parent": "q_89682",
        "author": "Bathsheba"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.5,
        "sent": "      on running above code the output comes as 100 because when i called hello class  static block is executed first setting the value of b to 100 and displaying it. but when i write this code:         here the output comes as 10. i am expecting answer as 100 because once the static block is executed it gave b the value as 100. so when in main()  i called  hello.b  it should have referred to b (=100). how is the memory allocated to b in both the codes?  "
      }
    ]
  },
  {
    "author": "Pier-Alexandre Bouchard",
    "parent": "",
    "title": "\"One plus plus two compiles unexpectedly\"",
    "commentid": "q_2704",
    "date": "Dec 23, 2014 10:34:56 AM",
    "children": [
      {
        "date": "Dec 23, 2014 10:38:03 AM",
        "commentid": "a_3259",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": " java is interpreting the working   as 1 plus positive 2.  see the  unary operator section .  "
          }
        ],
        "parent": "q_2704",
        "author": "ryanyuyu"
      },
      {
        "date": "Dec 23, 2014 10:38:37 AM",
        "commentid": "a_3260",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": " from the specification  on  lexical translations          the longest possible translation is used at each step  even if the   result does not ultimately make a correct program while another   lexical translation would.  there is one exception: if lexical   translation occurs in a type context (?4.11) and the input stream has   two or more consecutive > characters that are followed by a non->   character  then each > character must be translated to the token for   the numerical comparison operator >.      (also known as  maximal munch .)    the   is interpreted as a  postfix increment operator  which cannot be applied to an integer literal  thus the compiler error.    while          each character is interpreted separately.   is an integer literal    is the  additive operator     is the  unary plus operator   and   is an integer literal. the whole expression is equivalent to         which is easier to read.  "
          }
        ],
        "parent": "q_2704",
        "author": "Sotirios Delimanolis"
      },
      {
        "date": "Dec 23, 2014 10:50:03 AM",
        "commentid": "a_3261",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": " in java/c++/c   is not same as  .  /  are the  increment/decrement operator . the first case does not work because it does not apply to literals (  or  ).  even then it would not be a valid statement  neither   nor   are valid statement in java. the second example works because it is interpreted as  . the java lexer ignores white space. ?in the same way this is valid :         or         ?it works only because   is a   operator. it does not work for strings as below :         similarly it is not valid with multiplication       "
          }
        ],
        "parent": "q_2704",
        "author": "fastcodejava"
      },
      {
        "date": "Dec 23, 2014 11:23:50 AM",
        "commentid": "a_3262",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": " the message is:         the   statement is parsed as   followed by   followed by  . this is interpreted as   and   creating a syntax error (and not an   error; in fact  you will get the same error if you used variables e.g.  ).       the   statement on the other hand is parsed as   followed by   followed by   which compiles as expected. the space between the two operators separates the two operators.  "
          }
        ],
        "parent": "q_2704",
        "author": "Salman A"
      },
      {
        "date": "Dec 24, 2014 6:33:04 AM",
        "commentid": "a_3263",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": " sometimes it's easier to see a problem using variables.    your snippet could be rewritten as:         now  you can easily see that the second   operator is used to indicate a positive value. you could also have written  .    the   operator could be used to negate an expression.      and   are unary operators.  "
          }
        ],
        "parent": "q_2704",
        "author": "Pier-Alexandre Bouchard"
      }
    ],
    "sent": [
      {
        "topicid": 2,
        "systemtopicid": 2,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Loops",
        "linePolarity": 0.0,
        "sent": " so  i expect this not to compile  and it doesn't:         but this does:         what gives? shouldn't it also not compile?    also  this question is very hard to search for because of the operators..  "
      }
    ]
  },
  {
    "author": "FrobberOfBits",
    "parent": "",
    "title": "\"Why is the diamond case with its common ancestor used to explain Java multiple inheritance issue  instead of two unrelated parent classes?\"",
    "commentid": "q_45247",
    "date": "Oct 13, 2014 4:45:49 AM",
    "children": [
      {
        "date": "Oct 13, 2014 4:56:33 AM",
        "commentid": "a_54765",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.076,
            "sent": " the problem with diamond inheritance is not so much   shared behaviour  but  shared state  . as you can see  java in fact has always supported multiple inheritance  but only  multiple inheritance of type .    with only three classes the problem is resolved relatively easily by introducing a simple construct like   or  . and while you're only looking at overridden methods  it indeed doesn't matter whether you have a common ancestor or just the basic three classes.    however if   and   have a common ancestor  the state of which they both inherit  then you're in serious trouble. do you store two separate copies of the state of this common ancestor? that would be more like composition than inheritance. or do you only store one that is shared by both   and    causing strange interactions when they manipulate their inherited shared state?         note how the above wouldn't be so big a problem if class   didn't exist and both   and   declared their own   field. there would still be a problem of clashing names  but that could be resolved with some namespacing construct (  and   maybe  as we do with inner classes?). the true diamond problem on the other hand is more than a naming clash  it's a matter of how to maintain class invariants when two unrelated superclasses of   (  and  ) share the same state they both inherit from  . this is why all four classes are needed to demonstrate the full extent of the problem.    shared behaviour in multiple inheritance doesn't exhibit the same problem. so much so that the recently introduced  default methods  do exactly that. this means that multiple inheritance of implementations is allowed now too. there is still some complication around the resolution of which implementation to call but since interfaces are stateless  the biggest bugbear is avoided.  "
          }
        ],
        "parent": "q_45247",
        "author": "biziclop"
      },
      {
        "date": "Oct 13, 2014 5:00:34 AM",
        "commentid": "a_54766",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.076,
            "sent": " java does not support multiple inheritance because the designers of the language designed java this way. other languages like c++ support multiple inheritance just fine  so it is not a technical issue but just a design criteria.    the problem with multiple inheritance is that it is not always clear which method from which class you are calling to  and which instance variables you are accessing to. different people interpret it differently and the java designers believed at that time that it was better to skip multiple inheritance altogether.    c++ solves the diamond shaped class issue with   virtual inheritance  :        virtual inheritance is a technique used in object-oriented   programming  where a particular base class in an inheritance hierarchy   is declared to share its member data instances with any other   inclusions of that same base in further derived classes. for example    if class a is normally (non-virtually) derived from class x (assumed   to contain data members)  and class b likewise  and class c inherits   from both classes a and b  it will contain two sets of the data   members associated with class x (accessible independently  often with   suitable disambiguating qualifiers). but if class a is virtually   derived from class x instead  then objects of class c will contain   only one set of the data members from class x. the best-known language   that implements this feature is c++.      contrary to java  in c++ you can disambiguate which instance method to call by prefixing the call with the name of the class:       "
          }
        ],
        "parent": "q_45247",
        "author": "vz0"
      },
      {
        "date": "Oct 13, 2014 5:00:43 AM",
        "commentid": "a_54767",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.076,
            "sent": " that is just one difficulty that you have to solve for multiple inheritance in a language.  since there exist languages that do have multiple inhertance (e.g. common lisp  c++  eiffel)  it is obviously not insurmountable.    common lisp defines the exact strategy for priorizing (ordering) the inheritance graph  so there is no ambiguity in the rare cases where it does matter in practice.    c++ uses virtual inheritance (i have not yet put the effort in to understand what that means).    eiffel allows to specify exactly how you want to inherit  possibly renaming methods in the subclass.    java just skipped multiple inheritance.  (i personally think that it is difficult not to be  insulting to its designers while trying to rationalize this decision.  of course  it is hard to think about a thing when the language you think  in  does not support it.)  "
          }
        ],
        "parent": "q_45247",
        "author": "Svante"
      },
      {
        "date": "Oct 13, 2014 7:04:49 AM",
        "commentid": "a_54768",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.076,
            "sent": " the diamond problem with four classes is simpler than three classes problem described in the question.    the problem with three classes adds another issue that has to be solved first: the naming conflict caused by two unrelated   methods with the same signature. it's not actually hard to solve but it adds unnecessary complexity. it would probably be allowed in java (like it is already allowed to implement multiple unrelated interface methods with the same signature)  but there could be languages that simply prohibit multiple inheritance of similar methods without a common ancestor.     by adding a fourth class which defines    it is clear that both   and   are implementing the  same    method.    so the diamond problem is more clear because it shows the problem using simple inheritance instead of just using methods with the same signature. it is well known and probably applies to a wider range of programming languages  so the variation of it with three classes (which adds the additional naming conflict issue) doesn't make much sense and therefore hasn't been adopted.  "
          }
        ],
        "parent": "q_45247",
        "author": "kapep"
      },
      {
        "date": "Oct 14, 2014 11:10:22 AM",
        "commentid": "a_54769",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.076,
            "sent": " multiple inheritance is no problem if you find a sane solution to method resolution. when you invoke a method on an object  method resolution is the act of picking which class'es version of the method should be used. for this  the inheritance graph is linearized. then  the first class in this sequence that provides an implementation of the requested method is chosen.    to illustrate the following examples of conflict resolution strategies  we will be using this diamond inheritance graph:            the most flexible strategy is requiring the programmer to  explicitly choose  the implementation when creating an ambiguous class  by explicitly overriding the conflicting method. a variant of this is banning multiple inheritance. if a programmer wants to inherit behaviour from multiple classes  composition will have to be used  and a number of proxy methods to be written. however  naively explicitly resolved inheritance conflicts has the same shortcomings as?      depth first search   which might create the linearization  . but this way    is considered before   although    overrides   ! this can't be what we wanted. an example of a language using dfs is perl.     use a  clever algorithm  that guarantees that if   is a subclass of    it will always come before   in the linearization. such an algorithm will not be able to unravel all inheritance graphs  but it provides sane semantics in most cases: if a class overrides a method  it will always be preferred over the overridden method. this algorithm exists and is called   c3  . this would create the linearization  . c3 was first presented in 1996. unfortunately  java was published in 1995 ? so c3 was not known when java was initially designed.      use composition  not inheritance ? revisited.  some solutions to multiple inheritance suggest getting rid of the ?class inheritance? bit  and instead propose other units of composition. one example is  mixins   which ?copy &amp; paste? method definitions into your class. this is incredibly crude.    the idea of mixins has been refined into   traits   (presented 2002  also too late for java). traits are a more general case of both classes and interfaces. when you ?inherit? a trait  the definitions are embedded into your class  so that this does not complicate method resolution. unlike mixins  traits provide more nuanced strategies to resolve conflicts. especially  the order in which traits are composed matters. traits play a prominent role in perl's ?moose? object system (called  roles ) and in  scala .       java prefers single inheritance for another reason: if each class can only have one superclass  we don't have to apply complicated method resolution algorithms ? the inheritance chain already is the method resolution order. this makes methods a bit more efficient.    java 8 introduced default methods which look similar to traits. however  the rules of java's method resolution make interfaces with default methods much less capable than traits. still  a step in the direction of adapting modern solutions to the multiple-inheritance problem.    in most multiple inheritance method resolution schemes  the order of superclasses does matter. that is  there's a difference between   and  . because the order can be used for simple disambiguation  a three-class example does not sufficiently demonstrate the problems associated with multiple inheritance. you need a full four-class diamond problem for that  as it shows how naive depth-first search leads to an unintuitive method resolution order.  "
          }
        ],
        "parent": "q_45247",
        "author": "amon"
      },
      {
        "date": "Oct 13, 2014 4:52:28 AM",
        "commentid": "a_54770",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.076,
            "sent": " the diamond problem that you cite is one reason (and a very good one) not to support multiple inheritance.  there are other reasons too  which you can  read a bit about here .       a lot of the reasons boil down to complexity (it's quite complex to do it right)  the relatively rare legitimate need to use it  and all sorts of other problems with dispatching (aside from just the diamond problem) that it creates.  "
          }
        ],
        "parent": "q_45247",
        "author": "FrobberOfBits"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -1.076,
        "sent": " this question might sound weird to java people but if you try to explain this  it would be great.    in these days i am clearing some of java's very basic concept.  so i come to inheritance and interface topic of java.     while reading this i found that java does not support multiple inheritance and also understood that  what i am not able to understand that why everywhere diamond figure issue(at least 4 class to create diamond) is discussed to explain this behavior  can't we understand this issue using 3 classes only.    say  i have class a and class b  these two classes are different (they are not child class of common class) but they have one common method and they look like :-         ok now say if java supports multiple inheritance and if there is one class which is the subclass of a and b like this :-         then compiler will not be able to find which method to call whether from a or b and that is why java does not support multiple inheritance. so is there any thing wrong with this concept ?     when i read about this topic i was able to understand diamond issue  but i am not able to understand why people are not giving example with three class (if this is valid one  because we used only 3 classes to demonstrate issue so its easy to understand by comparing it to diamond issue.)    let me know whether this example does not fit to explain issue or this can also be  referred to understand issue.      edit:  i got one close vote here stating that question is not clear.  here is main question :-    can i understand why \\\"java does not support multiple inheritance\\\" with 3 classes only as described above or i must need to have 4 classes (diamond structure) to understand the issue.  "
      }
    ]
  },
  {
    "author": "Teddy",
    "parent": "",
    "title": "\"Why doesn&#39;t Java have true multidimensional arrays?\"",
    "commentid": "q_46009",
    "date": "Oct 11, 2014 12:17:52 PM",
    "children": [
      {
        "date": "Oct 11, 2014 2:53:21 PM",
        "commentid": "a_55670",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9040000000000001,
            "sent": "     but it seems that this is really not what one might have expected.      why?    consider that the form   means \\\"array of type t\\\"  then just as we would expect   to mean \\\"array of type int\\\"  we would expect   to mean \\\"array of type array of type int\\\"  because there's no less reason for having   as the   than  .    as such  considering that one can have arrays of any type  it follows just from the way   and   are used in declaring and initialising arrays (and for that matter       and  )  that without some sort of special rule banning arrays of arrays  we get this sort of use \\\"for free\\\".    now consider also that there are things we can do with jagged arrays that we can't do otherwise:      we can have \\\"jagged\\\" arrays where different inner arrays are of different sizes.   we can have null arrays within the outer array where appropriate mapping of the data  or perhaps to allow lazy building.   we can deliberately alias within the array so e.g.   is the same array as  . (this can allow for massive savings with some data-sets  e.g. many unicode properties can be mapped for the full set of 1 112 064 code points in a small amount of memory because leaf arrays of properties can be repeated for ranges with matching patterns).   some heap implementations can handle the many smaller objects better than one large object in memory.      there are certainly cases where these sort of multi-dimensional arrays are useful.    now  the default state of any feature is unspecified and unimplemented. someone needs to decide to specify and implement a feature  or else it wouldn't exist.    since  as shown above  the array-of-array sort of multidimensional array will exist unless someone decided to introduce a special banning array-of-array feature. since arrays of arrays are useful for the reasons above  that would be a strange decision to make.    conversely  the sort of multidimensional array where an array has a defined rank that can be greater than 1 and so be used with a set of indices rather than a single index  does not follow naturally from what is already defined. someone would need to:      decide on the specification for the declaration  initialisation and use would work.   document it.   write the actual code to do this.   test the code to do this.   handle the bugs  edge-cases  reports of bugs that aren't actually bugs  backward-compatibility issues caused by fixing the bugs.      also users would have to learn this new feature.    so  it has to be worth it. some things that would make it worth it would be:      if there was no way of doing the same thing.   if the way of doing the same thing was strange or not well-known.   people would expect it from similar contexts.   users can't provide similar functionality themselves.      in this case though:      but there is.   using strides within arrays was already known to c and c++ programmers and java built on its syntax so that the same techniques are directly applicable   java's syntax was based on c++  and c++ similarly only has direct support for multidimensional arrays as arrays-of-arrays. (except when statically allocated  but that's not something that would have an analogy in java where arrays are objects).   one can easily write a class that wraps an array and details of stride-sizes and allows access via a set of indices.      really  the question is not \\\"why doesn't java have true multidimensional arrays\\\"? but \\\"why should it?\\\"    of course  the points you made in favour of multidimensional arrays are valid  and some languages do have them for that reason  but the burden is nonetheless to argue a feature in  not argue it out.        (i hear a rumour that c# does something like this  although i also hear another rumour that the clr implementation is so bad that it's not worth having... perhaps they're just rumours...)      like many rumours  there's an element of truth here  but it is not the full truth.    .net arrays can indeed have multiple ranks. this is not the only way in which it is more flexible than java. each rank can also have a lower-bound other than zero. as such  you could for example have an array that goes from -3 to 42 or a two dimensional array where one rank goes from -2 to 5 and another from 57 to 100  or whatever.    c# does not give complete access to all of this from its built-in syntax (you need to call   for lower bounds other than zero)  but it does for allow you to use the syntax   for a two-dimensional array of      for a three-dimensional array  and so on.    now  the extra work involved in dealing with lower bounds other than zero adds a performance burden  and yet these cases are relatively uncommon. for that reason single-rank arrays with a lower-bound of 0 are treated as a special case with a more performant implementation. indeed  they are internally a different sort of structure.    in .net multi-dimensional arrays with lower bounds of zero are treated as multi-dimensional arrays whose lower bounds just happen to be zero (that is  as an example of the slower case) rather than the faster case being able to handle ranks greater than 1.    of course  .net  could  have had a fast-path case for zero-based multi-dimensional arrays  but then all the reasons for java not having them apply  and  the fact that there's already one special case  and special cases suck  and then there would be two special cases and they would suck more. (as it is  one can have some issues with trying to assign a value of one type to a variable of the other type).    not a single thing above shows clearly that java couldn't possibly have had the sort of multi-dimensional array you talk of; it would have been a sensible enough decision  but so also the decision that was made was also sensible.  "
          }
        ],
        "parent": "q_46009",
        "author": "Jon Hanna"
      },
      {
        "date": "Oct 11, 2014 12:54:28 PM",
        "commentid": "a_55671",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9040000000000001,
            "sent": " this should be a question to james gosling  i suppose. the initial design of java was about oop and simplicity  not about speed.    if you have a better idea of how multidimensional arrays should work  there are several ways of bringing it to life:      submit a  jdk?enhancement?proposal .   develop a new jsr through  java community process .   propose a new  project .       upd . of course  you are not the first to question the problems of java arrays design.  for instance  projects  sumatra  and  panama  would also benefit from  true  multidimensional arrays.     \\\"arrays 2.0\\\"  is john rose's talk on this subject at jvm language summit 2012.  "
          }
        ],
        "parent": "q_46009",
        "author": "apangin"
      },
      {
        "date": "Oct 11, 2014 2:19:55 PM",
        "commentid": "a_55672",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9040000000000001,
            "sent": " to me it looks like you sort of answered the question yourself:        ... an incentive to write it as a flat array  even if that makes the unnatural and hard to read.      so write it as a flat array which is easy to read. with a trivial helper like         and similar setter and possibly a  -equivalent  you can pretend you're working with a 2d array. it's really no big deal. you can't use the array notation and everything gets  verbose and ugly . but that seems to be the java way. it's exactly the same as with   or  . you can't use braces for accessing a    that's a very similar case.    now the question is how important all those features are? would more people be happy if they could write    or    or use 2d arrays without the boilerplate  or what? all of this would be nice and you could go further  e.g.  array slices.  but there's no real problem. you have to choose between verbosity and inefficiency as in many other cases.  imho  the effort spent on this feature can be better spent elsewhere. your 2d arrays are a new best as....     java actually has no 2d primitive arrays  ...     it's mostly a syntactic sugar  the underlying thing is array of objects.         as arrays are reified  the current implementation needs hardly any support. your implementation would open a can of worms:      there are currently 8 primitive types  which means 9 array types  would a 2d array be the tenth? what about 3d?   there is a single special object header type for arrays. a 2d array could need another one.   what about  ? clone it for 2d arrays?   many other features would have be adapted  e.g. serialization.      and what would         be? an old-style 2d  ? what about interoperability?    i guess  it's all doable  but there are simpler and more important things missing from java. some people need 2d arrays all the time  but many can hardly remember when they used any array at all.  "
          }
        ],
        "parent": "q_46009",
        "author": "maaartinus"
      },
      {
        "date": "Oct 11, 2014 3:19:55 PM",
        "commentid": "a_55673",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9040000000000001,
            "sent": " i am unable to reproduce the performance benefits you claim. specifically  the test program:         run on my workstation with             prints             that is  we observe a mere 3% difference between the one-dimensional and the multi-dimensional code you provided. this difference drops to 1% if we use idiomatic java (specifically  an enhanced for loop) for traversal (probably because bounds checking is performed on the same array object the loop dereferences  enabling the just in time compiler to elide bounds checking more completely).    performance therefore seems an inadequate justification for increasing the complexity of the language. specifically  to support true multi dimensional arrays  the java programming language would have to distinguish between arrays of arrays  and multidimensional arrays.  likewise  programmers would have to distinguish between them  and be aware of their differences. api designers would have to ponder whether to use an array of arrays  or a multidimensional array. the compiler  class file format  class file verifier  interpreter  and just in time compiler would have to be extended. this would be particularly difficult  because multidimensional arrays of different dimension counts would have an incompatible memory layout (because the size of their dimensions must be stored to enable bounds checking)  and can therefore not be subtypes of each other. as a consequence  the methods of class java.util.arrays would likely have to be duplicated for each dimension count  as would all otherwise polymorphic algorithms working with arrays.    to conclude  extending java to support multidimensional arrays would offer negligible performance gain for most programs  but require non-trivial extensions to its type system  compiler and runtime environment. introducing them would therefore have been at odds with the design goals of the java programming language  specifically that it be  simple .  "
          }
        ],
        "parent": "q_46009",
        "author": "meriton"
      },
      {
        "date": "Oct 12, 2014 5:33:26 AM",
        "commentid": "a_55674",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9040000000000001,
            "sent": " since this question is to a great extent about performance  let me contribute a proper jmh-based benchmark. i have also changed some things to make your example both simpler and the performance edge more prominent.     in my case i compare a 1d array with a 2d-array  and use a very short inner dimension. this is the worst case for the cache.    i have tried with both   and   accumulator and saw no difference between them. i submit the version with  .         the difference in performance is  larger  than what you have measured:         that's a factor above three. (note that the timing is reported  per array element .)    i also note that there is no warmup involved: the first 100 ms are as fast as the rest. apparently this is such a simple task that the interpreter already does all it takes to make it optimal.    update    changing  's inner loop to         (note  ) resulted in a significant speedup  as predicted by maaartinus. using   makes it impossible to eliminate the index range check. now the results are as follows:       "
          }
        ],
        "parent": "q_46009",
        "author": "Marko Topolnik"
      },
      {
        "date": "Oct 11, 2014 2:21:12 PM",
        "commentid": "a_55675",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.9040000000000001,
            "sent": " if you want a fast implementation of a true multi-dimentional array you could write a custom implementation like this. but you are right... it is not as crisp as the array notation. although  a neat implementation could be quite friendly.          why is it not the default implementation?     the designers of java probably had to decide how the default notation of the usual c array syntax would behave. they had a single array notation which could either implement arrays-of-arrays or true multi-dimentional arrays.     i think early java designers were really concerned with java being safe. lot of decisions seem to have been taken to make it difficult for the average programmer(or a good programmer on a bad day) to not mess up something . with true multi-dimensional arrays  it is easier for users to waste large chunks of memory by allocating blocks where they are not useful.    also  from java's embedded systems roots  they probably found that it was more likely to find pieces of memory to allocate rather than large chunks of memory required for true multi-dimentional objects.    of course  the flip side is that places where multi-dimensional arrays really make sense suffer. and you are forced to use a library and messy looking code to get your work done.     why is it still not included in the language?     even today  true multi-dimensional arrays are a risk from the the point of view of possible of memory wastage/misuse.   "
          }
        ],
        "parent": "q_46009",
        "author": "Teddy"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.9040000000000001,
        "sent": " the tl;dr version  for those who don't want the background  is the following specific question:    question        why doesn't java have an implementation of true multidimensional arrays? is there a solid technical reason? what am i missing here?      background    java has multidimensional arrays at the syntax level  in that one can declare         but it seems that this is really not what one might have expected. rather than having the jvm allocate a contiguous block of ram big enough to store 100  s  it comes out as an array of arrays of  s: so each layer is a contiguous block of ram  but the thing as a whole is not. accessing   is thus rather slow: the jvm has to      find the   stored at  ;   index this to find the   stored at  .      this involves querying an object to go from one layer to the next  which is rather expensive.    why java does this    at one level  it's not hard to see why this can't be optimised to a simple scale-and-add lookup even if it were all allocated in one fixed block. the problem is that   is a reference all of its own  and it can be changed. so although arrays are of fixed size  we could easily write         and now the scale-and-add is screwed because this layer has grown. you'd need to know at runtime whether everything is still the same size as it used to be. in addition  of course  this will then get allocated somewhere else in ram (it'll have to be  since it's bigger than what it's replacing)  so it's not even in the right place for scale-and-add.    what's problematic about it    it seems to me that this is not ideal  and that for two reasons.    for one  it's  slow . a test i ran with these methods for summing the contents of a single dimensional or multidimensional array took  nearly twice as long  (714 seconds vs 371 seconds) for the multidimensional case (an   and an   respectively  filled with random   values  run 1000000 times with warm cache).         secondly  because it's slow  it thereby  encourages obscure coding . if you encounter something performance-critical that would be naturally done with a multidimensional array  you have an incentive to write it as a flat array  even if that makes the unnatural and hard to read. you're left with an unpalatable choice: obscure code or slow code.    what could be done about it    it seems to me that the basic problem could easily enough be fixed. the only reason  as we saw earlier  that it can't be optimised is that the structure might change. but java already has a mechanism for making references unchangeable: declare them as  .    now  just declaring it with         isn't good enough because it's only   that is   here:   still isn't  and could be changed  so the structure might still change. but if we had a way of declaring things so that it was   throughout  except at the bottom layer where the   values are stored  then we'd have an entire immutable structure  and it could all be allocated as one block  and indexed with scale-and-add.    how it would look syntactically  i'm not sure (i'm not a language designer). maybe         although admittedly that looks a bit weird. this would mean:   at the top layer;   at the next layer; not   at the bottom layer (else the   values themselves would be immutable).    finality throughout would enable the jit compiler to optimise this to give performance to that of a single dimensional array  which would then take away the temptation to code that way just to get round the slowness of multidimensional arrays.    (i hear a rumour that c# does something like this  although i also hear another rumour that the clr implementation is so bad that it's not worth having... perhaps they're just rumours...)    question        so why doesn't java have an implementation of true multidimensional arrays? is there a solid technical reason? what am i missing here?      update    a bizarre side note: the difference in timings drops away to only a few percent if you use an   for the running total rather than a  . why would there be such a small difference with an    and such a big difference with a  ?    benchmarking code    code i used for benchmarking  in case anyone wants to try to reproduce these results:       "
      }
    ]
  },
  {
    "author": "durron597",
    "parent": "",
    "title": "\"How is ArrayOutOfBoundsException possible in String.valueOf(int)?\"",
    "commentid": "q_57718",
    "date": "Sep 19, 2014 1:35:15 PM",
    "children": [
      {
        "date": "Sep 20, 2014 3:21:54 PM",
        "commentid": "a_70121",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -1.06,
            "sent": " this is a jit compiler bug that has been introduced in jdk 8u20 as a side-effect of another fix:   jdk-8042786     the problem is related to auto-boxing elimination optimization.  the work-around is to switch the optimization off by   jvm flag    looks like the problem also exists in the most recent jdk 9 source base.  i've submitted the bug report:  https://bugs.openjdk.java.net/browse/jdk-8058847  with 100% reproducible minimal test case included.  "
          }
        ],
        "parent": "q_57718",
        "author": "apangin"
      },
      {
        "date": "Sep 19, 2014 2:16:20 PM",
        "commentid": "a_70122",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -1.06,
            "sent": " i can reliably reproduce your issue with this code:         the issue will almost certainly be traced to a jit compiler bug. your observation that  once it happens the first time  it happens reliably on every subsequent call  points cleanly to a jit compilation event which introduces the buggy code into the codepath.    if that's available to you  you could activate diagnostic jvm options which will print all compilation events ( ). then you may be able to correlate such an event with the moment when the exception starts appearing.  "
          }
        ],
        "parent": "q_57718",
        "author": "Marko Topolnik"
      },
      {
        "date": "Sep 19, 2014 2:28:54 PM",
        "commentid": "a_70123",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -1.06,
            "sent": " i am leaving the code snippet here  as it still ought to be run faster than the original code - at a cost of memory - but be advised it  doesn't actually fix the problem.        "
          }
        ],
        "parent": "q_57718",
        "author": "durron597"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -1.06,
        "sent": " why does this code sometimes produce arrayoutofboundsexception? how is that even possible for  ?          updates     i don't know the value of the byte when this occurs  but it doesn't seem like it should be possible for any possible value of byte.    once it happens once  every invocation then errors out with the same exception.    environment:           "
      }
    ]
  },
  {
    "author": "dkatzel",
    "parent": "",
    "title": "\"Should I return a Collection or a Stream?\"",
    "commentid": "q_96424",
    "date": "Jul 10, 2014 5:42:48 AM",
    "children": [
      {
        "date": "Jul 10, 2014 7:51:49 AM",
        "commentid": "a_118302",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.2959999999999998,
            "sent": " the answer is  as always  \\\"it depends\\\".  it depends on how big the returned collection will be.  it depends on whether the result changes over time  and how important consistency of the returned result is.  and it depends very much on how the user is likely to use the answer.      first  note that you can always get a collection from a stream  and vice versa:         so the question is  which is more useful to your callers.      if your result might be infinite  there's only one choice: stream.    if your result might be very large  you probably prefer stream  since there may not be any value in materializing it all at once  and doing so could create significant heap pressure.    if all the caller is going to do is iterate through it (search  filter  aggregate)  you should prefer stream  since stream has these built-in already and there's no need to materialize a collection (especially if the user might not process the whole result.)  this is a very common case.      even if you know that the user will iterate it multiple times or otherwise keep it around  you still may want to return a stream instead  for the simple fact that whatever collection you choose to put it in (e.g.  arraylist) may not be the form they want  and then the caller has to copy it anyway.  if you return a stream  they can do   and get it in exactly the form they want.    the above \\\"prefer stream\\\" cases mostly derive from the fact that stream is more flexible; you can late-bind to how you use it without incurring the costs and constraints of materializing it to a collection.      the one case where you must return a collection is when there are strong consistency requirements  and you have to produce a consistent snapshot of a moving target.  then  you will want put the elements into a collection that will not change.    so i would say that most of the time  stream is the right answer -- it is more flexible  it doesn't impose usually-unnecessary materialization costs  and can be easily turned into the collection of your choice if needed.  but sometimes  you may have to return a collection (say  due to strong consistency requirements)  or you may want to return collection because you know how the user will be using it and know this is the most convenient thing for them.    "
          }
        ],
        "parent": "q_96424",
        "author": "Brian Goetz"
      },
      {
        "date": "Jul 10, 2014 9:37:36 AM",
        "commentid": "a_118303",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.2959999999999998,
            "sent": " i have a few points to add to  brian goetz' excellent answer .    it's quite common to return a stream from a \\\"getter\\\" style method call. see the  stream usage page  in the java 8 javadoc and look for \\\"methods... that return stream\\\" for the packages other than  . these methods are usually on classes that represent or can contain multiple values or aggregations of something. in such cases  apis typically have returned collections or arrays of them. for all the reasons that brian noted in his answer  it's very flexible to add stream-returning methods here. many of these classes have collections- or array-returning methods already  because the classes predate the streams api. if you're designing a new api  and it makes sense to provide stream-returning methods  it might not be necessary to add collection-returning methods as well.    brian mentioned the cost of \\\"materializing\\\" the values into a collection. to amplify this point  there are actually two costs here: the cost of storing values in the collection (memory allocation and copying) and also the cost of creating the values in the first place. the latter cost can often be reduced or avoided by taking advantage of a stream's laziness-seeking behavior. a good example of this are the apis in  :         not only does   have to hold the entire file contents in memory in order to store it into the result list  it also has to read the file to the very end before it returns the list. the   method can return almost immediately after it has performed some setup  leaving file reading and line breaking until later when it's necessary -- or not at all. this is a huge benefit  if for example  the caller is interested only in the first ten lines:         of course considerable memory space can be saved if the caller filters the stream to return only lines matching a pattern  etc.    an idiom that seems to be emerging is to name stream-returning methods after the plural of the name of the things that it represents or contains  without a   prefix. also  while   is a reasonable name for a stream-returning method when there is only one possible set of values to be returned  sometimes there are classes that have aggregations of multiple types of values. for example  suppose you have some object that contains both attributes and elements. you might provide two stream-returning apis:       "
          }
        ],
        "parent": "q_96424",
        "author": "Stuart Marks"
      },
      {
        "date": "Jul 10, 2014 6:13:28 AM",
        "commentid": "a_118304",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.2959999999999998,
            "sent": "     were streams designed to always be \\\"terminated\\\" inside the same expression they were created in?      that is how they are used in most examples.      note: returning a stream is not that different to returning a iterator (admitted with much more expressive power)    imho the best solution is to encapsulate why you are doing this  and not return the collection.    e.g.         or if you intend to count them       "
          }
        ],
        "parent": "q_96424",
        "author": "Peter Lawrey"
      },
      {
        "date": "Jul 10, 2014 6:38:34 AM",
        "commentid": "a_118305",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.2959999999999998,
            "sent": " i think it depends on your scenario. may be  if you make your   implement    it is sufficient.          or in the a functional style:         but if you want a more complete and fluent api  a stream could be a good solution.   "
          }
        ],
        "parent": "q_96424",
        "author": "gontard"
      },
      {
        "date": "Jul 10, 2014 7:39:51 AM",
        "commentid": "a_118306",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.2959999999999998,
            "sent": " i would probably have 2 methods  one to return a   and one to return the collection as a  .         this is the best of both worlds.  the client can choose if they want the list or the stream and they don't have to do the extra object creation of making an immutable copy of the list just to get a stream.    this also only adds 1 more method to your api so you don't have too many methods  "
          }
        ],
        "parent": "q_96424",
        "author": "dkatzel"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -1.2959999999999998,
        "sent": " suppose i have a method that returns a read-only view into a member list:         further suppose that all the client does is iterate over the list once  immediately. maybe to put the players into a jlist or something. the client does  not  store a reference to the list for later inspection!    given this common scenario  should i return a stream instead?         or is returning a stream non-idiomatic in java? were streams designed to always be \\\"terminated\\\" inside the same expression they were created in?  "
      }
    ]
  },
  {
    "author": "Mikhail",
    "parent": "",
    "title": "\"Why do fields seem to be initialized before constructor?\"",
    "commentid": "q_9872",
    "date": "Dec 9, 2014 11:55:45 PM",
    "children": [
      {
        "date": "Dec 10, 2014 12:08:59 AM",
        "commentid": "a_12011",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.6680000000000001,
            "sent": " if you move your dog instance in the end  you may find the output becomes -2          the final fields(whose values are compile-time constant expressions) will beinitialized first   and then the rest will be executed at in textual order.     so   in your case when dog instance is initialized   (0) is not initialized yet  while  (-5) does since it is final.     http://docs.oracle.com/javase/specs/jls/se5.0/html/execution.html#12.4.2  states that:        execute either the class variable initializers and static   initializers of the class  or the field initializers of the interface     in textual order   as though they were a single block   except that   final class variables and fields of interfaces whose values are   compile-time constants are initialized first          updated a newer doc    here is the jdk7 version from  http://docs.oracle.com/javase/specs/jls/se7/html/jls-12.html#jls-12.4.2     final field is at step6:        then  initialize the final class variables and fields of interfaces   whose values are compile-time constant expressions      while static field is at step 9:        next  execute either the class variable initializers and static   initializers of the class  or the field initializers of the interface    in textual order  as though they were a single block.    "
          }
        ],
        "parent": "q_9872",
        "author": "Jaskey"
      },
      {
        "date": "Dec 10, 2014 12:01:57 AM",
        "commentid": "a_12012",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.6680000000000001,
            "sent": " variable declaration sequence. the   is initialized first because it is a constant. however   is still   at the time   is instantiated.  "
          }
        ],
        "parent": "q_9872",
        "author": "gknicker"
      },
      {
        "date": "Dec 10, 2014 12:08:51 AM",
        "commentid": "a_12013",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.6680000000000001,
            "sent": " what is happening..    the first line that is executing is this  . ow  there are 2 things that must be kept in mind.         makes it a  compile  time constant. so    is already  hard-coded  into your code.     the call to new   is done and the constructor is called which sets the value to   +   =  .       change   to   then you will see the difference (you will get   as answer.)    note : static fields are initialized as and how they are encountered.   "
          }
        ],
        "parent": "q_9872",
        "author": "TheLostMind"
      },
      {
        "date": "Dec 10, 2014 12:13:13 AM",
        "commentid": "a_12014",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.6680000000000001,
            "sent": " initialization sequence in your test;                  //constant as static final        //then 'dog' is initialize but its member val2 has not been initialized                     //at last 'val2' is initialized      this code change will output  ;       "
          }
        ],
        "parent": "q_9872",
        "author": "Wundwin Born"
      },
      {
        "date": "Dec 10, 2014 1:30:27 AM",
        "commentid": "a_12015",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.6680000000000001,
            "sent": " all static variables are initialized in a separate static constructor  which is executed on class loading. in the same order as they apear in the code. your example is compiled into something like this:         that's why order of class/instance variables matters. class constructor execution happens at the end of initialization. constants are resolved before. for more information refer to  creation of new class instances .  "
          }
        ],
        "parent": "q_9872",
        "author": "Mikhail"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.6680000000000001,
        "sent": "      the output is      from this result  it seems that the initialization of   is before the completion of   member and its instantiating.       why is this order like this?  "
      }
    ]
  },
  {
    "author": "Avi",
    "parent": "",
    "title": "\"Why NullPointerException?\"",
    "commentid": "q_29154",
    "date": "Nov 9, 2014 3:35:50 AM",
    "children": [
      {
        "date": "Nov 9, 2014 3:46:13 AM",
        "commentid": "a_35046",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.956,
            "sent": " as in  jls ?12.5 (creation of new class instances)  the following procedure is used when an instance is created:       assign the arguments for the constructor to newly created parameter variables for this constructor invocation.      if this constructor begins with an explicit constructor invocation (?8.8.7.1) of another constructor in the same class (using this)  then evaluate the arguments and process that constructor invocation recursively using these same five steps. if that constructor invocation completes abruptly  then this procedure completes abruptly for the same reason; otherwise  continue with step 5.      this constructor does not begin with an explicit constructor invocation of another constructor in the same class (using this). if this constructor is for a class other than object  then this constructor will begin with an explicit or implicit invocation of a superclass constructor (using super). evaluate the arguments and process that superclass constructor invocation recursively using these same five steps. if that constructor invocation completes abruptly  then this procedure completes abruptly for the same reason. otherwise  continue with step 4.      execute the instance initializers and instance variable initializers for this class  assigning the values of instance variable initializers to the corresponding instance variables  in the left-to-right order in which they appear textually in the source code for the class. if execution of any of these initializers results in an exception  then no further initializers are processed and this procedure completes abruptly with that same exception. otherwise  continue with step 5.      execute the rest of the body of this constructor. if that execution completes abruptly  then this procedure completes abruptly for the same reason. otherwise  this procedure completes normally.        that means your call to   and the subsequent call to the overridden   method are both done  before  the instance variable is initialized with   which discards the outcome of the   method and overwrites any value being assigned to   with the   value.    this leads to the following  golden rule : never call non-final  non-private methods in a constructor!  "
          }
        ],
        "parent": "q_29154",
        "author": "Seelenvirtuose"
      },
      {
        "date": "Nov 9, 2014 3:47:00 AM",
        "commentid": "a_35047",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.956,
            "sent": " according to  section 12.5 of the jls   the superclass constructor will run before the initialiser for    which means that it will be set back to   after it is set to  .  "
          }
        ],
        "parent": "q_29154",
        "author": "David Wallace"
      },
      {
        "date": "Nov 9, 2014 11:08:10 AM",
        "commentid": "a_35048",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.956,
            "sent": " according to   section 12.5  of the jls  the superclass constructor will run before the constructor of derived class.    the initalization of global variable is call at the time of calling of constuctor. so when super class constructor call the abstract method and set the mtitle value in abstarct method implementation it will set the first it's value to =\\\"it's a test\\\". after finishing super class constructor it will call derived class constructor and  when derived class constructor call it will first initalize its global variable which set mtitle value to the null. as it is mention in code         but in second case mtitle have not assign any value in code as mention         so it will take its default value which is we assigned in implemented of abstarct method init \\\"it a test \\\" so it will not produce any exception.  "
          }
        ],
        "parent": "q_29154",
        "author": "Avi"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.956,
        "sent": " i have a abstract class and a derived class. look at provided code:-         when i will execute the above code it will throw nullpointerexception on printing the value of mtitle. if you check the code in constructor of parent i have called the the abstract method which will called the init method of derived class  in abstract method i have initialize the value of mtitle value as =\\\"it's a test\\\";    after calling parent constructor derived class have to call the system.out.println.    if it is doing in that way then why it is throwing nullpointerexception.    but  if i just leave the assignment of mtitle it will not throw exception like:-         if initialization of variable occur on calling of the contruct of class and we know by default global object have initialize to null. but in this case it will not throw exception.  "
      }
    ]
  },
  {
    "author": "Stuart Marks",
    "parent": "",
    "title": "\"Is there a type inference regression in JDK 8 update 20?\"",
    "commentid": "q_71826",
    "date": "Aug 25, 2014 9:45:18 AM",
    "children": [
      {
        "date": "Aug 26, 2014 3:00:43 PM",
        "commentid": "a_87584",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.17999999999999994,
            "sent": " this is a new bug that exists in the jdk 8u20 update release and in the current jdk 9 development branch. this code worked before  so yes  this would be considered a regression. the jdk langtools team has filed the following bug report:         jdk-8055963  inference failure with nested invocation      judging from the comments  it appears that the current behavior actually conforms to the specification (the jls)  but the behavior is clearly wrong  so it might be the case that a clarification to the specification is necessary.    note that this is a different type inference bug from the one reported in this other stackoverflow question  java 1.8.20 compiler error   bug  jdk-8051402 . that bug has been fixed already  although but the fix isn't in jdk 8u20.  "
          }
        ],
        "parent": "q_71826",
        "author": "Stuart Marks"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.17999999999999994,
        "sent": " i have a problem with the following code:         with jdk 8 until update 11  this code compile. with jdk 8 update 20  it does not compile anymore. in the last statement  i have to explicitly specify the   type argument for the last   instantiation.    i was wondering if i am wrong with this code or if it is a regression in the last jdk update.  "
      }
    ]
  },
  {
    "author": "Sibi",
    "parent": "",
    "title": "\"What kinds of type errors can Haskell catch at compile time that Java cannot?\"",
    "commentid": "q_75230",
    "date": "Aug 18, 2014 6:22:12 PM",
    "children": [
      {
        "date": "Aug 18, 2014 10:21:49 PM",
        "commentid": "a_91817",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.2519999999999998,
            "sent": " saying that haskell's type system can catch more errors than java's is a little bit misleading. let's unpack this a little bit.    statically typed    java and haskell are both  statically  typed languages. by this i mean that they  type  of a given expression in the language is known at compile time. this has a number of advantages  for both java and haskell  namely it allows the compiler to check that the expressions are \\\"sane\\\"  for some reasonable definition of sane.     yes  java allows certain \\\"mixed type\\\" expressions  like    which some may argue is unsafe or bad  but that is a subjective choice. in the end it is just a feature that the java language offers  for better or worse.    immutability    to see how haskell code could be argued to be less error prone than java (or c  c++  etc.) code  you must consider the type system with respect to the immutability of the language. in pure (normal) haskell code  there are no side effects. that is to say  no value in the program  once created  may ever change. when we compute something we are creating a new result from the old result  but we don't modify the old value. this  as it turns out  has some  really  convenient consequences from a safety perspective. when we write code  we can be sure nothing else anywhere in the program is going to effect our function. side effects  as it turns out  are the cause of  many  programming errors. an example would be a shared pointer in c that is freed in one function and then accessed in another  causing a crash. or a variable that is set to null in java          this could not happen in normal haskell code  because   once defined   can not change . which means it can not be set to  .    enter the type system    now  you are probably wondering how the type system plays into all of this  that is what you asked about after all. well  as nice as immutability is  it turns out there is very little interesting work that you can do without any mutation. reading from a file? mutation. writing to disk? mutation. talking to a web server? mutation. so what do we do? in order to solve this issue  haskell uses its type system to encapsulate mutation in a type  called the  io monad . for instance to read from a file  this function may be used          the io monad    notice that the type of the result  is not a     it is an  . what this means  in laymans terms  is that the result introduces io (side effects) to the program. in a well formed program io will only take place inside the io monad  thus allowing us to see very clearly  where side effects can occur. this property is enforced by the type system. further   types can only produce their results  which are side effects  inside the   function of the program. so now we have very neatly and nicely isolated off the dangerous side effects to a controlled part of the program. when you get the result of the    anything  could  happen  but at least this can't happen  anywhere   only in the   function and only as the result of   types.    now to be clear  you can create   values anywhere in your code. you can even manipulate them outside the   function  but none of that manipulation will actually take place until the result is demanded in the body of the   function. for instance          this function reads input from a file  duplicates that input and appends the duplicated input onto the end of the original input. so if the file had the characters   this would create a   with the contents  . you can call this function anywhere in your code  but haskell will only actually try to read the file when expression is found in the   function  because it is an instance of the    . like so          this will almost surely fail  as the file you requested probably doesn't exist  but it will only fail here. you only have to worry about side effects  not everywhere in your code  as you do in many other languages.    there is a lot more to both io and monads in general than i have covered here  but that is probably beyond the scope of your question.    type inference    now there is one more aspect to this.  type inference     haskell uses a  very  advanced  type inference system   that allows for you to write code  that is statically typed  without having to write the type annotation  such as   in java. ghc can infer the type of  almost  any expression  even  very  complex ones.    what this means for our safety discussion is that everywhere an instance of   is used in the program  the type system will make sure that it can't be used to produce an unexpected side effect. you can't cast it to a    and just get the result out where/when ever you want.  you must explicitly introduce the side effect in the   function .    the safety of static typing with the ease of dynamic typing    the type inference system has some other nice properties as well. often people enjoy scripting languages because they don't have to write all that boilerplate for the types like they would have to do in java or c. this is because scripting languages are  dynamically typed  or the type of the expression is only computed as the expression is being run by the interpreter. this makes these languages arguably more prone to errors  because you won't know if you have a bad expression until you run the code. for example  you might say something like this in python.         the problem with this is that   and    can be anything . so this would be fine          but this would cause an error          and we have now way of checking that this is invalid  until it is run.     it is very important to understand that  all  statically type languages do not have this problem   java  included . haskell is  not  safer than java in this sense. haskell and java both keep you  safe  from this type of error  but in haskell you don't have to write all the types in order to be safe  they type system can infer the types. in general  it is considered good practice to annotate the types for your functions in haskell  even though you don't have to. in the body of the function however  you rarely have to specify types (there are some strange edge cases where you will).    conclusion    hopefully that helps illuminate how haskell keeps you safe. and in regard to java  you might say that in java you have to work against the type system to write code  but in haskell the type system  works for you .  "
          }
        ],
        "parent": "q_75230",
        "author": "isomarcte"
      },
      {
        "date": "Aug 19, 2014 4:22:55 AM",
        "commentid": "a_91818",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.2519999999999998,
            "sent": " type casts    one difference is that java allows dynamic type casts such as (silly example follows):         type casts can lead to runtime type errors  which can be regarded as a cause of type unsafety. of course  any good java programmer would regard casts as a last resort  and rely on the type system to ensure type safety instead.    in haskell  there is (roughly) no subtyping  hence no type casts. the closest feature to casts is the (unfrequently used)   library  as shown below         which roughly corresponds to         the main difference here between haskell and java is that in java we have  separate  runtime type checking ( ) and cast ( ). this might lead to runtime errors if checks do not ensure that casts will succeed.    i recall that casts were a big concern in java before generics were introduced  since e.g. using collections forced you to perform a lot of casts. with generics the java type system greatly improved  and casts should be far less common now in java  since they are less frequently needed.     casts and generics    recall that generic types are erased at run time in java  hence code such as         does not work. the check can not be fully performed since we can not check the parameter of  . also because of this erasure  if i remember correctly  the cast can succeed even if   is a different    only to cause runtime type errors later  even if casts do not appear in the code.    the   haskell machinery does not erase types at runtime.    more powerful types    haskell gadts and (coq  agda  ...) dependent types extend conventional static type checking to enforce even stronger properties on the code at compile time.    consider e.g. the   haskell function. here's an example:         this applies   in a \\\"pointwise\\\" fashion on the two lists. its definition is:         what happens  however  if we pass lists of different lengths?         the longer one gets silently truncated. this may be an unexpected behaviour. one could redefine zip as:         but raising a runtime error is only marginally better. what we need is to enforce  at compile time  that the lists are of the same lengths.         note that the compiler is able to infer that   and   are of the same length  so the recursive call is statically well-typed.    in java you could encode the list lengths in the type using the same trick:         but  as far as i can see  the   code can not access the tails of the two lists and have them available as two variables of the same type    where   is intuitively  . intuitively  accessing the two tails loses type information  in that we do no longer know they are of even length. to perform a recursive call  a cast would be needed.    of course  one can rework the code differently and use a more conventional approach  e.g. using an iterator over  . admittedly  above i am just trying to convert a haskell function in java in a direct way  which  is  the wrong approach to coding java (as much as would be coding haskell by directly translating java code). still  i used this silly example to show how haskell gadts can express  without unsafe casts  some code which would require casts in java.  "
          }
        ],
        "parent": "q_75230",
        "author": "chi"
      },
      {
        "date": "Aug 19, 2014 5:16:12 AM",
        "commentid": "a_91819",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.2519999999999998,
            "sent": " there are several things about haskell that make it \\\"safer\\\" than java. the type system is one of the obvious ones.     no type-casts . java and similar oo languages let you cast one type of object to another. if you can't convince the type system to let you do whatever it is you're trying to do  you can always just cast everything to   (although most programmers would immediately recognise this as pure evil). the trouble is  now you're in the realm of run-time type-checking  just like in a dynamically-typed language. haskell doesn't let you do such things. (unless you explicitly go out of your way to get it; and almost nobody does.)     usable generics . generics are available in java  c#  eiffel and a few other oo languages. but in haskell they actually  work . in java and c#  trying to write generic code almost always leads to obscure compiler messages about \\\"oh  you can't use it that way\\\". in haskell  generic code is  easy . you can write it  by accident!  and it works exactly the way you'd expect.     convenience . you can do things in haskell that would be way too much effort in java. for example  set up different types for raw user input verses sanitised user input. you can  totally  do that in java. but you won't. it's too much boilerplate. you will only bother doing this if it's absolutely critical for your application. but in haskell  it's only a handful of lines of code. it's  easy . people do it  for fun!      magic . [i don't have a more technical term for this.] sometimes  the type signature of a function lets you know  what the function does . i don't mean you can figure out what the function does  i mean there is only one possible thing a function with that type could be doing or it wouldn't compile. that's an extremely powerful property. haskell programmers sometimes say \\\"when it compiles  it's usually bug-free\\\"  and that's probably a direct result of this.    while not strictly properties of the  type system   i might also mention:        explicit i/o . the type signature of a function tells you whether it performs any i/o or not. functions that perform no i/o are thread-safe and extremely easy to test.      explicit null . data cannot be null unless the type signature says so. you must explicitly check for null when you come to use the data. if you \\\"forget\\\"  the type signatures won't match.      results rather than exceptions . haskell programmers tend to write functions that return a \\\"result\\\" object which contains either the result data or an explanation of why no result could be produced. as opposed to throwing an exception and hoping somebody remembers to catch it. like a nullable value  a result object is different from the actual result data  and the type system will remind you if you forget to check for failure.       having said all of that  java programs typically die with null pointer or array index exceptions; haskell programs tend to die with exceptions like the infamous \\\"head []\\\".  "
          }
        ],
        "parent": "q_75230",
        "author": "MathematicalOrchid"
      },
      {
        "date": "Aug 18, 2014 6:42:19 PM",
        "commentid": "a_91820",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.2519999999999998,
            "sent": " for a very basic example  while this is allowable in java:         the same thing will produce a compile error in haskell:            there is no implicit type casting in haskell which helps in preventing silly errors like  . you have to tell it explicitly  that you want to convert it to  :       "
          }
        ],
        "parent": "q_75230",
        "author": "Sibi"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.2519999999999998,
        "sent": " i'm just starting to learn haskell and keep seeing references to its powerful type system. i see many instances in which the inference is much more powerful than javas  but also the implication that it can catch more errors at compile time because of its superior type system. so  i'm wondering if it would be possible to explain what types of errors haskell can catch at compile time that java cannot.  "
      }
    ]
  },
  {
    "author": "Matthias Braun",
    "parent": "",
    "title": "\"After upgrading to Gradle 2.0: Could not find property &#39;Compile&#39; on root project\"",
    "commentid": "q_96699",
    "date": "Jul 9, 2014 9:54:47 PM",
    "children": [
      {
        "date": "Jul 9, 2014 9:54:47 PM",
        "commentid": "a_118662",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": 0.28400000000000025,
            "sent": " changing the line to         fixed the issue.  "
          }
        ],
        "parent": "q_96699",
        "author": "Matthias Braun"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": 0.28400000000000025,
        "sent": " to avoid warnings regarding special characters when building my java source code  i put this line in my   which worked fine before upgrading to gradle 2.0:         after upgrading  this fails with the following error:         how can i fix that?  "
      }
    ]
  },
  {
    "author": "Stephen C",
    "parent": "",
    "title": "\"What is an operand stack?\"",
    "commentid": "q_104060",
    "date": "Jun 26, 2014 2:35:07 AM",
    "children": [
      {
        "date": "Jun 26, 2014 2:42:14 AM",
        "commentid": "a_127868",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.544,
            "sent": " it's how the various individual bytecode operations get their input  and how they provide their output.    for instance  consider the   operation  which adds two  s together. to use it  you push two values on the stack and then use it:         now the top value on the stack is the sum of those two local variables. the next operation might take that top stack value and store it somewhere  or we might push another value on the stack to do something else.    suppose you want to add three values together. the stack makes that easy:         now the top value on the stack is the result of adding together those three local variables.    let's look at that second example in more detail:    we'll assume:      the stack is empty to start with  (which is almost never actually true  but we don't care what's on it before we start)    local variable 0 contains     local variable 1 contains     local variable 2 contains        so initially:    +-------+ | stack | +-------+ +-------+    then we do         now we have    +-------+ | stack | +-------+ |   27  | +-------+    next         +-------+ | stack | +-------+ |   10  | |   27  | +-------+    now we do the addition:         it \\\"pops\\\" the   and   off the stack  adds them together  and pushes the result ( ). now we have:    +-------+ | stack | +-------+ |   37  | +-------+    time for our third  :         +-------+ | stack | +-------+ |    5  | |   37  | +-------+    we do our second  :         that gives us:    +-------+ | stack | +-------+ |   42  | +-------+     (which is  of course  the  answer to the ultimate question of life the universe and everything .)   "
          }
        ],
        "parent": "q_104060",
        "author": "T.J. Crowder"
      },
      {
        "date": "Jun 26, 2014 2:38:53 AM",
        "commentid": "a_127869",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.544,
            "sent": " operand stack holds the operand used by operators to perform operations. each entry on the operand stack can hold a value of any java virtual machine type.    from  jvm specifications          java virtual machine instructions take operands from the operand   stack  operate on them  and push the result back onto the operand   stack. the operand stack is also used to prepare parameters to be   passed to methods and to receive method results.      for example      instruction will add two integer values  so it will pop top two integer values from operand stack and will push result into operand stack after adding them.    for more detailed reference  you can check  jvms#2.5 : run-time data areas     summarizing it in context of operand stack             jvm supports multithreaded execution environment. each thread of execution has its private java virtual machine stack (jvm stack) created at the same time of thread creation.   this java virtual machine stack stores frames. frame holds data  partial results  method return values and performs dynamic linking.   each frame contains stack  known as operand stack  which holds the operand values of jvm types. a depth of operand stack is determined at compile time and updated with operators.    "
          }
        ],
        "parent": "q_104060",
        "author": "KisHan SarsecHa Gajjar"
      },
      {
        "date": "Jun 26, 2014 3:16:06 AM",
        "commentid": "a_127870",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.544,
            "sent": "     but could's not understand exactly that what it is and how it works in jvm?      the jvm defines virtual computer  and the instruction set of that computer is stack based.  what this means is that instructions in the jvm instruction set will typically push and pop operands from the stack.  so for example        a load instruction might fetch a value from a local variable  instance variable or class variable and push it onto the operand stack    an arithmetical instruction will pop values from the operand stack  perform the computation and push the result back onto the stack  and   a store instruction will pop a value from the stack and store it ...      @t.j.crowder's answer gives a more concrete example in lots of detail.    how the operand stack is implemented is platform specific  and it depends on whether code is being interpreted or whether it has been jit compiled.       in the interpreted case  the operand stack is probably an array that is managed by the interpreter code.  the push and pop micro-operations would be implemented something like:         and          when the code is jit compiled  the bytecode instruction sequences are transformed into native instruction sequences that achieve the same thing as the bytecodes did.  the operand stack locations get mapped to either native registers or memory locations; e.g. in the current native stack frame.  the mapping involves various optimizations that are aimed at using registers (fast) in preference to memory (slower).    thus in the jit compiled case  the operand stack no longer has a clear physical existence  but the overall behaviour of the compiled program is  the same as if  the operand stack did exist 1 .           1 - actually  it may not be exactly the same when you take the java memory model into account.  however  the memory model places clear boundary on what the differences may be.  and in the case of a single threaded computation that doesn't interact with the outside (e.g. i/o  clocks  etc)  there can be no observable differences.   "
          }
        ],
        "parent": "q_104060",
        "author": "Stephen C"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.544,
        "sent": " i am reading about jvm architecture. today i read about the concept of the operand stack. according to an article:        the operand stack is used during the execution of byte code instructions   in a similar way that general-purpose registers are used in a native cpu.      i can't understand: what exactly is an operand stack  and how does it work in jvm?  "
      }
    ]
  },
  {
    "author": "MikeFHay",
    "parent": "",
    "title": "\"Alternative to successive String.replace\"",
    "commentid": "q_32217",
    "date": "Nov 4, 2014 5:34:08 AM",
    "children": [
      {
        "date": "Nov 4, 2014 6:59:53 AM",
        "commentid": "a_38711",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " although     is a huge improvement compared to      it is still  very far  from being optimal.    the problem with   is that if the replacement has different length than the replaceable part (applies to our case)  a bigger internal   array might have to be allocated  and the content has to be copied  and then the replace will occur (which also involves copying).    imagine this: you have a text with 10.000 characters. if you want to replace the   substring found at position   (2nd character) to    the implementation has to reallocate a   buffer which is at least larger by 1  has to copy the old content to the new array  and it has to copy 9.997 characters (starting at position  ) to the right by 1 to fit   into the place of    and finally characters of   are copied to the starter position  . this has to be done for every replace! this is slow.    faster solution: building output on-the-fly    we can build the output  on-the-fly : parts that don't contain replaceable texts can simply be appended to the output  and if we find a replaceable fragment  we append the replacement instead of it. theoretically it's enough to loop over the input only  once  to generate the output. sounds simple  and it's not that hard to implement it.     implementation:     we will use a   preloaded with mappings of the replaceable-replacement strings:          and using this  here is the replacer code:  (more explanation after the code)          testing it:           output:  (wrapped to avoid scroll bar)         this solution is faster than using regular expressions as that involves much overhead  like compiling a    creating a   etc. and regexp is also much more general. it also creates many temporary objects under the hood which are thrown away after the replace. here i only use a   (plus   array under its hood) and the code iterates over the input   only once. also this solution is much faster that using   as detailed at the top of this answer.    notes and explanation    i initialized the   in the   method like this:         so basically i created it with an initial capacity of 150% of the length of the original  . this is because our replacements are longer than the replaceable texts  so if replacing occurs  the output will obviously be longer than the input. giving a larger initial capacity to   will result in no internal   reallocation at all (of course the required initial capacity depends on the replaceable-replacement pairs and their frequency/occurrence in the input  but this +50% is a good upper estimation).    i also utilized the fact that all replaceable strings start with a   character  so finding the next potential replaceable position becomes blazing-fast:         it's just a simple loop and   comparisons inside    and since it always starts searching from   (and not from the start of the input)  overall the code iterates over the input   only once.    and finally to tell if a replaceable   does occur at the potential position  we use the     method to check the replaceable stings which is also blazing-fast as all it does is just compares   values in a loop and returns at the very first mismatching character.     and a plus:     the question doesn't mention it  but our input is an html document. html tags are case-insensitive which means the input might contain   instead of  .  to this algorithm this is not a problem. the   in the   class has an overload which  supports case-insensitive comparison :         so if we want to modify our algorithm to also find and replace input tags which are the same but are written using different letter case  all we have to modify is this one line:         using this modified code  replaceable tags become case-insensitive:       "
          }
        ],
        "parent": "q_32217",
        "author": "icza"
      },
      {
        "date": "Nov 4, 2014 5:42:25 AM",
        "commentid": "a_38712",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " for performance - use  . for convenience you can use   to store values and replacements.         ... to replace all occurences in stringbuilder you can check here:  replace all occurences of a string using stringbuilder?        "
          }
        ],
        "parent": "q_32217",
        "author": "Heisenberg"
      },
      {
        "date": "Nov 4, 2014 5:45:12 AM",
        "commentid": "a_38713",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " unfortunately   doesn't provide a   method  so you might want to consider using   and   in conjunction with  :         you could do something similar with   but in that case you'd have to implement   etc. yourself.    as for the expression you could also just try and match  any  html tag (although that might cause problems since regex and arbitrary html don't fit very well) and when the lookup doesn't have any result you just replace the match with itself.  "
          }
        ],
        "parent": "q_32217",
        "author": "Thomas"
      },
      {
        "date": "Nov 5, 2014 1:04:02 AM",
        "commentid": "a_38714",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " the particular example you provide seems to be html or xhtml. trying to edit html or xml using regular expressions is frought with problems. for the kind of editing you seem to be interested in doing you should look at using xslt. another possibility is to use sax  the streaming xml parser  and have your back-end write the edited output on the fly. if the text is actually html  you might be better using a tolerant html parser  such as jsoup  to build a parsed representation of the document (like the dom)  and manipulate that before outputting it.  "
          }
        ],
        "parent": "q_32217",
        "author": "Raedwald"
      },
      {
        "date": "Nov 4, 2014 5:40:50 AM",
        "commentid": "a_38715",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": "  stringbuilder  is  backed  by a char array. so  unlike  string  instances  it is  mutable . thus  you can call   and   on the  .  "
          }
        ],
        "parent": "q_32217",
        "author": "TheLostMind"
      },
      {
        "date": "Nov 4, 2014 5:52:44 AM",
        "commentid": "a_38716",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " i would do something like this         tagequals is a func which checks a tag name  "
          }
        ],
        "parent": "q_32217",
        "author": "Evgeniy Dorofeev"
      },
      {
        "date": "Nov 4, 2014 9:36:16 AM",
        "commentid": "a_38717",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " use  apache commons stringutils.replaceeach .       "
          }
        ],
        "parent": "q_32217",
        "author": "MikeFHay"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": 0.6680000000000001,
        "sent": " i want to replace some strings in a string input :         as you can see this approach is not the best  because each time i have to search for the portion to replace etc  and strings are immutable... also the input is large  which means that some performance issues are to be considered.    is there any better approach to reduce the complexity of this code ?   "
      }
    ]
  },
  {
    "author": "Holger",
    "parent": "",
    "title": "\"Synchronizing on local variable\"",
    "commentid": "q_40674",
    "date": "Oct 21, 2014 1:28:43 AM",
    "children": [
      {
        "date": "Oct 21, 2014 2:07:55 AM",
        "commentid": "a_49105",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -1.0,
            "sent": " right after the code has acquired the object?s monitor  the reference to the object is stored into the   which is the globally visible array of nodes which make up the contents of the  :         right at this point  other threads executing other modification methods on the same   might encounter this incomplete node while traversing the global array  in other words  the   reference has escaped.    while at the point where the   has been constructed  there is no possibility for contention on a newly created object  in the other methods  which are synchronizing on  s found in the array  there might be contention for exactly that  .    it?s like a ?priority-synchronization?. the creator is synchronizing at a point where the reference has not been escaped yet therefore it is guaranteed to succeed while at the point where the reference escapes  all other threads will have to wait  in the unlikely (but still possible) event that they access exactly that  .  "
          }
        ],
        "parent": "q_40674",
        "author": "Holger"
      }
    ],
    "sent": [
      {
        "topicid": 10,
        "systemtopicid": 10,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Network",
        "linePolarity": -1.0,
        "sent": " i noticed a weird construct in   's   and   methods :         what is the point of synchronizing on a local object considering that the jit will most likely treat it as a no-op?  "
      }
    ]
  },
  {
    "author": "Claas Wilke",
    "parent": "",
    "title": "\"Should java 8 getters return optional type?\"",
    "commentid": "q_45648",
    "date": "Oct 12, 2014 10:30:18 AM",
    "children": [
      {
        "date": "Oct 12, 2014 11:36:14 AM",
        "commentid": "a_55247",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8559999999999999,
            "sent": " of course  people will do what they want.  but we did have a clear intention when adding this feature  and it was  not  to be a general purpose maybe or some type  as much as many people would have liked us to do so.  our intention was to provide a limited mechanism for library method return types where there needed to be a clear way to represent \\\"no result\\\"  and using   for such was overwhelmingly likely to cause errors.      for example  you probably should never use it for something that returns an array of results  or a list of results; instead return an empty array or list.  you should almost never use it as a field of something or a method parameter.    i think routinely using it as a return value for getters would definitely be over-use.      there's nothing  wrong  with optional that it should be avoided  it's just not what many people wish it were  and accordingly we were fairly concerned about the risk of zealous over-use.    (public service announcement:  never  call   unless you can prove it will never be null; instead use one of the safe methods like   or  .  in retrospect  we should have called   something like   or something that made it far clearer that this was a highly dangerous method that undermined the whole purpose of   in the first place.  lesson learned.)  "
          }
        ],
        "parent": "q_45648",
        "author": "Brian Goetz"
      },
      {
        "date": "Oct 12, 2014 11:07:48 AM",
        "commentid": "a_55248",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8559999999999999,
            "sent": " after doing a bit of research of my own  i've come across a number of things that might suggest when this is appropriate. the most authoritative being the following quote from an oracle article:         \\\"it is important to note that the intention of the optional class is  not to replace every single null reference . instead  its purpose is to help design  more-comprehensible apis  so that by just reading the signature of a method  you can tell whether you can expect an optional value. this forces you to actively unwrap an optional to deal with the absence of a value.\\\"  -  tired of null pointer exceptions? consider using java se 8's optional!       i also found this excerpt from  java 8 optional: how to use it          \\\"optional is not meant to be used in these contexts  as it won't buy us anything:              in the domain model layer (not serializable)       in dtos (same reason)       in input parameters of methods       in constructor parameters\\\"           which also seems to raise some valid points.    i wasn't able to find any negative connotations or red flags to suggest that optional should be avoided. i think the general idea is  if it's helpful or improves the usability of your api  use it.  "
          }
        ],
        "parent": "q_45648",
        "author": "Justin"
      },
      {
        "date": "Oct 12, 2014 10:43:36 AM",
        "commentid": "a_55249",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8559999999999999,
            "sent": " i'd say in general its a good idea to use the optional type for return values that can be nullable. however  w.r.t. to frameworks i assume that replacing classical getters with optional types will cause a lot of trouble when working with frameworks (e.g.  hibernate) that rely on coding conventions for getters and setters.  "
          }
        ],
        "parent": "q_45648",
        "author": "Claas Wilke"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.8559999999999999,
        "sent": " optional type introduced in java 8 is a new thing for many developers.    is a getter method returning   type in place of the classic   a good practice? assume that the value can be  .  "
      }
    ]
  },
  {
    "author": "Damian Leszczy\u0144ski - Vash",
    "parent": "",
    "title": "\"Java ternary operator influence on generics type inference\"",
    "commentid": "q_108253",
    "date": "Jun 18, 2014 8:19:41 AM",
    "children": [
      {
        "date": "Jun 18, 2014 8:23:50 AM",
        "commentid": "a_132845",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.33199999999999985,
            "sent": " compiles for me fine in java 8.    earlier versions of java might need more help         should work.     edit  this is due to improvements in java 8 type inference as explained here     http://openjdk.java.net/jeps/101     and here's a blog with the highlights:  http://blog.jooq.org/2013/11/25/a-lesser-known-java-8-feature-generalized-target-type-inference/   "
          }
        ],
        "parent": "q_108253",
        "author": "dkatzel"
      },
      {
        "date": "Jun 18, 2014 8:23:43 AM",
        "commentid": "a_132846",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.33199999999999985,
            "sent": " this is related with type  inference from a generic method .     in case of code before ver. 8. it must be declared the type of result for this case.          since ver. 8 notion of what is a target type has been expanded to include method arguments. so this is no longer required.   "
          }
        ],
        "parent": "q_108253",
        "author": "Damian Leszczy\u0144ski - Vash"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.33199999999999985,
        "sent": "      why does   compiles fine whereas   has an error? (to be more precise  \\\"type mismatch: cannot convert from list&lt;capture#1-of ? extends object&gt; to list&lt;string&gt;\\\" )    i would have thought that both functions would compile to the same bytecode  so a clever compiler should infer the correct type for  ...  "
      }
    ]
  },
  {
    "author": "dasblinkenlight",
    "parent": "",
    "title": "\"Declaring variable final and static\"",
    "commentid": "q_48420",
    "date": "Oct 7, 2014 10:31:26 AM",
    "children": [
      {
        "date": "Oct 7, 2014 10:51:06 AM",
        "commentid": "a_58707",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.1519999999999999,
            "sent": " the line of code is perfectly fine  and there is no real problem because the variable is   and  .    maybe the person who made that comment was confused by the following.    in java  when you create a   variable of type   (for example; it also works with some other types)  then the compiler might  in places where you use that variable  substitute the actual constant value instead of a reference to the variable. for example  suppose you have the following:         when you compile and run this  it will obviously print 3.    now suppose that you change class   and set  . you would expect that if you recompile class   and then run class   (without recompiling class  )  you would see  . but what happens is that you will still see  . that is because the   in class   was replaced by the actual constant value   when you compiled class  .    this is an optimization that the java compiler does for constants.    as you can see this can cause problems  if you have such constants in the public api of your classes. users of your code will have to recompile their code if you change the value of such constants.    but in the code you posted in your question this is not a problem  because the variable is  .    more details:     java language specification 13.4.9   "
          }
        ],
        "parent": "q_48420",
        "author": "Jesper"
      },
      {
        "date": "Oct 7, 2014 10:53:02 AM",
        "commentid": "a_58708",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.1519999999999999,
            "sent": " the comment is most likely related to a problem of  classloader leaking  ( here is a good article ).    in a nutshell  this problem happens in environments where classloader needs to be reloaded. if you load a class dynamically through a classloader and then try reloading the classloader  keeping static final fields with objects of classes created through this classloader will prevent unloading the classloader itself. once this happens  you get an  .     the article linked above lists logging libraries among the top culprits that could produce this behavior  along with measures you can take to work around the leaks (such as releasing classloaders explicitly).  "
          }
        ],
        "parent": "q_48420",
        "author": "dasblinkenlight"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.1519999999999999,
        "sent": " this comment was made in a code review and the person who made it is no longer on our team.          any type that must be resolved by the classloader at runtime should never have instances which are held by references declared to be both final and static.      here's the line of code:         i'm familiar with the debate of declaring loggers static or non-static  but this comment seems to be more general.  i can't find any explanations of why static and final are bad.  can somebody elaborate?  "
      }
    ]
  },
  {
    "author": "mkrakhin",
    "parent": "",
    "title": "\"How to compile and run my Maven unit tests for Java 8  while having my code compiled for Java 7\"",
    "commentid": "q_107221",
    "date": "Jun 20, 2014 1:38:33 AM",
    "children": [
      {
        "date": "Jun 20, 2014 1:46:29 AM",
        "commentid": "a_131615",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": -0.8240000000000001,
            "sent": " in maven compile and testcompile goals are different. and maven even has parameters for testcompile: testtarget and testsource. so:       "
          }
        ],
        "parent": "q_107221",
        "author": "mkrakhin"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": -0.8240000000000001,
        "sent": " i want to use java 8 syntax in my unit tests. but my 'main' code must be compiled for java 7 since my production environment only has jdk 7 installed.    is there a way of doing this with the maven-compiler-plugin ? my jenkins server has java 8 installed.    i will accept the risk that i can accidental use java 8 specific functionality in my production code.  "
      }
    ]
  },
  {
    "author": "Mikhail",
    "parent": "",
    "title": "\"Why aren&#39;t method references singleton?\"",
    "commentid": "q_2978",
    "date": "Dec 23, 2014 1:17:37 AM",
    "children": [
      {
        "date": "Dec 23, 2014 1:26:08 AM",
        "commentid": "a_3584",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": -0.544,
            "sent": " for  instance  methods  i don't think it would make sense for them to be cached. you'd have to cache one method per instance... which would either mean an extra field within the class associated with the method - one per public method  presumably  because the methods could be referenced from outside the class - or cached within the  user  of the method reference  in which case you'd need some sort of per-instance cache.    i think it makes more sense for method references to  static  methods to be cached  because they'll be the same forever. however  to cache the actual    you'd need a cache per type that it was targeting. for example:         should   and   be equal here? how would the compiler know what type to create  if so? it could create two instances here and cache them separately - and always cache at the point of use rather than the point of method declaration. (your example has the same class declaring the method and referencing it  which is a very special case. you should consider the more general case where they're different>)    the jls  allows  for method references to be cached. from  section 15.13.3 :        next  either a new instance of a class with the properties below is allocated and initialized  or an existing instance of a class with the properties below is referenced.      ... but even for static methods  it seems   doesn't do any caching at the moment.  "
          }
        ],
        "parent": "q_2978",
        "author": "Jon Skeet"
      },
      {
        "date": "Dec 23, 2014 1:50:13 AM",
        "commentid": "a_3585",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": -0.544,
            "sent": " in this simple case  you could do as you suggest by creating extra static or instance fields as appropriate  more complex if the lambda refers to objects  however the intent is to inline these instances out of existence e.g.         should be as efficient (even creating no more objects than)         it can eliminate the objects by inlining the stream code and using escape analysis to remove the need to create objects in the first place.    for this reason there has been little focus on implementing equals()  hashcode() or tostring()  accessing them via reflection for closures.  afaik  this deliberate to avoid the objects being using in ways not intended.  "
          }
        ],
        "parent": "q_2978",
        "author": "Peter Lawrey"
      },
      {
        "date": "Dec 23, 2014 2:49:46 AM",
        "commentid": "a_3586",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": -0.544,
            "sent": " making methods singletons will require to synchronize acquiring a reference to them. this gives a big overhead for such a simple operation with unpredictable result. the other solution is to create object for each method on class loading  but this results in many redundant objects  because only few method require referencing. i think synchronization is the main issue.  "
          }
        ],
        "parent": "q_2978",
        "author": "Mikhail"
      }
    ],
    "sent": [
      {
        "topicid": 6,
        "systemtopicid": 6,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "JSON",
        "linePolarity": -0.544,
        "sent": " in java  the following code returns false on both queries. why? wouldn't it be simpler for method references to be singleton? it would certainly make attaching and detaching listeners a lot simpler. as it is you need to keep a constant for any method reference that will need to be equivalence checked  you can't just use the method reference operator at every necessary location.       "
      }
    ]
  },
  {
    "author": "Alexey Malev",
    "parent": "",
    "title": "\"Behavior of memory barrier in Java\"",
    "commentid": "q_102792",
    "date": "Jun 28, 2014 9:58:31 AM",
    "children": [
      {
        "date": "Jun 28, 2014 11:33:12 AM",
        "commentid": "a_126278",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.6680000000000001,
            "sent": " doug lea is right. you can find the relevant part in section  ?17.4.4  of the  java language specification :          ?17.4.4 synchronization order          [..] a write to a volatile variable  v  (?8.3.1.4)  synchronizes-with  all subsequent reads of  v  by any thread (where \\\"subsequent\\\" is defined according to the synchronization order). [..]      the memory model of the  concrete machine  doesn't matter  because the semantics of the java programming language are defined in terms of an  abstract machine  -- independent of the  concrete machine . it's the responsibility of the java runtime environment to execute the code in such a way  that it complies with the guarantees given by the  java language specification .        regarding the actual question:       if there is no further synchronization  the method   can print    because   can be executed before  .   if there is an additional synchronization with a   to make sure that   is executed  after     then method   will never print    because the synchronization with   removes the  data race  on  .          independent volatile variables:     does it make sense  that a write to a volatile variable does not synchronize-with a read of any other volatile variable?    yes  it makes sense. if two threads need to interact with each other  they usually have to use the same   variable in order to exchange information. on the other hand  if a thread uses a volatile variable without a need for interacting with all other threads  we don't want to pay the cost for a memory barrier.    it is actually important in practice. let's make an example. the following class uses a volatile member variable:         imagine this class is used only locally within a method. the jit compiler can easily detect  that the object is only used within this method ( escape analysis ).         with the above rule  the jit compiler can remove all effects of the   reads and writes  because the   variable can not be accesses from any other thread.    this optimization actually exists in the java jit compiler:       src/share/vm/opto/memnode.cpp     "
          }
        ],
        "parent": "q_102792",
        "author": "nosid"
      },
      {
        "date": "Jun 28, 2014 12:02:37 PM",
        "commentid": "a_126279",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.6680000000000001,
            "sent": " as far as i understood the question is actually about volatile read/writes and its happens-before guarantees. speaking of that part  i have only one thing to add to nosid's answer:    volatile writes cannot be moved before normal writes  volatile reads cannot be moved after normal reads. that's why   and   results will be as nosid wrote.    speaking about barriers - the defininition sounds fine for me  but the one thing probably confused you is that these are things/tools/way to/mechanism call it whatever you like to implement behavior described in jmm in hotspot. when using java  you should rely on jmm guarantees  not implementation details.  "
          }
        ],
        "parent": "q_102792",
        "author": "Alexey Malev"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.6680000000000001,
        "sent": " after reading more blogs/articles etc  i am now really confused about the behavior of load/store before/after memory barrier.    following are 2 quotes from doug lea in one of his clarification article about jmm  which are both very straighforward:       anything that was visible to thread a when it writes to volatile field f becomes visible to thread b when it reads f.     note that it is important for both threads to access the same volatile variable in order to properly set up the happens-before relationship. it is not the case that everything visible to thread a when it writes volatile field f becomes visible to thread b after it reads volatile field g.       but then when i looked into another  blog  about memory barrier  i got these:       a store barrier  ?sfence? instruction on x86  forces all store instructions prior to the barrier to happen before the barrier and have the store buffers flushed to cache for the cpu on which it is issued.     a load barrier  ?lfence? instruction on x86  forces all load instructions after the barrier to happen after the barrier and then wait on the load buffer to drain for that cpu.       to me  doug lea's clarification is more strict than the other one: basically  it means if the load barrier and store barrier are on different monitors  the data consistency will not be guaranteed. but the later one means even if the barriers are on different monitors  the data consistency will be guaranteed. i am not sure if i understanding these 2 correctly and also i am not sure which of them is correct.    considering the following codes:         let's say we have 1 write thread tw1 first call the memorybarrier's write() method  then we have 2 reader threads tr1 and tr2 call memorybarrier's read1() and read2() method.consider this program run on cpu which does not preserve ordering (x86 do preserve ordering for such cases which is not the case)  according to memory model  there will be a storestore barrier (let's say sb1) between w01/w02  as well as 2 loadload barrier between r11/r12 and r21/r22 (let's say rb1 and rb2).      since sb1 and rb1 are on same monitor  i   so thread tr1 which calls  read1  should always see 14 on x  also \\\"foo\\\" is always printed.   sb1 and rb2 are on different monitors  if doug lea is correct  thread tr2 will not be guaranteed to see 14 on x  which means \\\"bar\\\" may be printed occasionally. but if memory barrier runs like martin thompson described in the  blog   the store barrier will push all data to main memory and load barrier will pull all data from main memory to cache/buffer  then tr2 will also be guaranteed to see 14 on x.      i am not sure which one is correct  or both of them are but what martin thompson described is just for x86 architecture. jmm does not guarantee change to x is visible to tr2 but x86 implementation does.    thanks~  "
      }
    ]
  },
  {
    "author": "meriton",
    "parent": "",
    "title": "\"How does the JVM internally handle race conditions?\"",
    "commentid": "q_16884",
    "date": "Nov 28, 2014 9:27:05 AM",
    "children": [
      {
        "date": "Nov 28, 2014 9:37:49 AM",
        "commentid": "a_20445",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.0,
            "sent": " the precise term is a  data race   which is a specialization of the general concept of a  race condition . the term  data race  is an official  precisely specified concept  which means that it arises from a  formal  analysis of the code.    the only way to get the real picture is to go and study the memory model chapter of the java language specification  but this is a simplified view: whenever you have a data race  there is almost no guarantee as to the outcome and a reading thread may see any value which has ever been written to the variable. therein also lies the only guarantee: the thread will  not  observe an \\\"out-of-thin-air\\\" value  such which was never written. well  unless you're dealing with  s or  s  then you may see torn writes.  "
          }
        ],
        "parent": "q_16884",
        "author": "Marko Topolnik"
      },
      {
        "date": "Nov 28, 2014 9:34:17 AM",
        "commentid": "a_20446",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.0,
            "sent": " maybe i'm missing something but what is there to handle? there is still a thread that will get there first. depending on which thread that is  that thread will just update/read some variable and proceed to the next instruction. it can't magically construct a sync block  it doesn't really know what you want to do. so in other words what happens will depend on the outcome of the 'race'.    note i'm not heavily into the lower level stuff so perhaps i don't fully understand the depth of your question.  "
          }
        ],
        "parent": "q_16884",
        "author": "Sebastiaan van den Broek"
      },
      {
        "date": "Nov 28, 2014 9:48:37 AM",
        "commentid": "a_20447",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.0,
            "sent": " java provides   and   to deal with these situations.  using them properly can be frustratingly difficult  but keep in mind that java is only exposing the complexity of modern cpu and memory architectures.  the alternatives would be to always err on the side of caution  effectively synchronizing everything which would kill performance; or ignore the problem and offer no thread safety whatsoever.  and fortunately  java provides excellent high-level constructs in the   package  so you can often avoid dealing with the low-level stuff.  "
          }
        ],
        "parent": "q_16884",
        "author": "Kevin Krumwiede"
      },
      {
        "date": "Nov 28, 2014 10:31:41 AM",
        "commentid": "a_20448",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.0,
            "sent": " the jvm will handle the situation just fine (ie it will not hang or complain)  but you may not get a result that you like!     when multiple threads are involved  java becomes fiendishly complicated and even code that looks obviously correct can turn out to be horribly broken. as an example:         is flawed in many ways.    first  let's say that i is currently 0 and thread a and thread b both call   at about the same time. there is a danger that they will both see that i is 0  then both increment it 1 and then save the result. so at the end of the two calls  i is only 1  not 2!    that's the race condition problem with the code  but there are other problems concerning memory visibility. when thread a changes a shared variable  there is no guarantee (without synchronization) that thread b will ever see the changes!    so thread a could increment i 100 times  and an hour later  thread b  calling getint()  might see i as 0  or 100 or anywhere in between!    the only sane thing to do if you are delving into java concurrency is to read java concurrency in practice by brian goetz et al. (ok there's probably other good ways to learn about it  but this is a great book co written by joshua bloch  doug lea and others)  "
          }
        ],
        "parent": "q_16884",
        "author": "user384842"
      },
      {
        "date": "Nov 30, 2014 11:04:59 AM",
        "commentid": "a_20449",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.0,
            "sent": " in short  the jvm  assumes  that code is free of data races when translating it into machine code. that is  if code is not correctly synchronized  the java language specification provides only  limited  guarantees about the behavior of that code.    most modern hardware likewise assumes that code is free of data races when executing it. that is  if code is not correctly synchronized  the hardware makes only limited guarantees about the result of its execution.     in particular  the java language specification  guarantees  the following only in the absence of a data race:       visibility : reading a field yields the value last assigned to it (it is unclear which write was  last   and writes of long or double variables  need not be atomic )     ordering : if a write is visible  so are any writes preceding it. for instance  if one thread executes:         another thread can read   only after the constructor of   has executed completely.       in the presence of a data race  these guarantees are null and void. it is possible for a reading thread to never see a write. it is also possible to see the write of    without seeing the effect of the constructor that logically preceded the write of  . it is very unlikely that the program is correct if such basic assumptions can not be made.    a data race will however not compromise the integrity of the java virtual machine. in particular  the jvm will not crash or halt  and still guarantee memory safety (i.e. prevent memory corruption) and  certain semantics of final fields .  "
          }
        ],
        "parent": "q_16884",
        "author": "meriton"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -1.0,
        "sent": " if multiple threads try to update the same member variable  it is called a race condition. but i was more interested in knowing how the jvm handles it internally if we don't handle it in our code by making it synchronised or something else? will it hang my program? how will the jvm react to it? i thought the jvm would temporarily create a sync block for this situation  but i'm not sure what exactly would be happening.     if any of you have some insight  it would be good to know.  "
      }
    ]
  },
  {
    "author": "bcsb1001",
    "parent": "",
    "title": "\"Sorting a List in parallel without creating a temporary array in Java 8\"",
    "commentid": "q_57141",
    "date": "Sep 21, 2014 8:55:57 AM",
    "children": [
      {
        "date": "Sep 21, 2014 11:49:03 AM",
        "commentid": "a_69454",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -0.964,
            "sent": " there doesn't appear to be any straightforward way to sort a   in parallel in java 8. i don't think this is fundamentally difficult; it looks more like an oversight to me.    the difficulty with a hypothetical   is that the   implementation knows nothing about the list's implementation or its internal organization. this can be seen by examining the java 7 implementation of  . as you observed  it has to copy the list elements out to an array  sort them  and then copy them back into the list.    this is the big advantage of the   extension method over  . it might seem that this is merely a small syntactic advantage being able to write   instead of  . the difference is that    being an interface extension method   can be overridden  by the specific   implementation. for example    sorts the list in-place using   whereas the default implementation implements the old copyout-sort-copyback technique.    it should be possible to add a   extension method to the   interface that has similar semantics to   but does the sorting in parallel. this would allow   to do a straightforward in-place sort using  . (it's not entirely clear to me what the default implementation should do. it might still be worth it to do copyout-parallelsort-copyback.) since this would be an api change  it can't happen until the next major release of java se.    as for a java 8 solution  there are a couple workarounds  none very pretty (as is typical of workarounds). you could create your own array-based   implementation and override   to sort in parallel. or you could subclass    override    grab the   array via reflection and call   on it. of course you could just write your own   implementation and provide a   method  but the advantage of overriding   is that this works on the plain   interface and you don't have to modify all the code in your code base to use a different   subclass.  "
          }
        ],
        "parent": "q_57141",
        "author": "Stuart Marks"
      },
      {
        "date": "Sep 21, 2014 10:35:40 AM",
        "commentid": "a_69455",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -0.964,
            "sent": " i think you are doomed to use a custom   implementation augmented with your own   or else change all your other code to store the big data in   types.    this is the inherent problem with layers of abstract data types. they're meant to isolate the programmer from details of implementation.  but when the details of implementation matter - as in the case of underlying storage model for sort - the otherwise splendid isolation leaves the programmer helpless.    the standard   sort documents provide an example.  after the explanation that  mergesort  is used  they say        the default implementation obtains an array containing all elements in this list  sorts the array  and iterates over this list resetting each element from the corresponding position in the array. (this avoids the n2 log(n) performance that would result from attempting to sort a linked list in place.)      in other words  \\\"since we don't know the underlying storage model for a   and couldn't touch it if we did  we make a copy organized in a known way.\\\" the parenthesized expression is based on the fact that the   \\\"i'th element accessor\\\" on a linked list is omega(n)  so the normal array mergesort implemented with it would be a disaster. in fact it's easy to implement mergesort efficiently on linked lists. the   implementer is just prevented from doing it.    a parallel sort on   has the same problem.  the standard sequential sort fixes it with custom  s in the concrete   implementations. the java folks just haven't chosen to go there yet.  maybe in java 9.  "
          }
        ],
        "parent": "q_57141",
        "author": "Gene"
      },
      {
        "date": "Sep 21, 2014 10:09:50 AM",
        "commentid": "a_69456",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -0.964,
            "sent": " just speculating here  but i see several good reasons for generic sort algorithms preferring to work on arrays instead of   instances:      element access is performed via method calls. despite all the optimizations jit can apply  even for a list that implements    this probably means a lot of overhead compared to plain array accesses which can be optimized very well.   many algorithms require copying some fragments of the array to temporary structures. there are efficient methods for copying arrays or their fragments. an arbitrary   instance on the other hand  can't be easily copied. new lists would have to be allocated which poses two problems. first  this means allocating some new objects which is likely more costly than allocating arrays. second  the algorithm would have to choose what implementation of   should be allocated for this temporary structure. there are two obvious solutions  both bad: either just choose some hard-coded implementation  e.g.    but then it could just allocate simple arrays as well (and if we're generating arrays then it's much easier if the soiurce is also an array). or  let the user provide some list factory object  which makes the code much more complicated.   related to the previous issue: there is no obvious way of copying a list into another due to how the api is designed. the best the   interface offers is   method  but this is probably not efficient for most cases (think of pre-allocating the new list to its target size vs adding elements one by one which many implementations do).   most lists that need to be sorted will be small enough for another copy to not be an issue.      so probably the designers thought of cpu efficiency and code simplicity most of all  and this is easily achieved when the api accepts arrays. some languages  e.g. scala  have sort methods that work directly on lists  but this comes at a cost and probably is less efficient than sorting arrays in many cases (or sometimes there will probably just be a conversion to and from array performed behind the scenes).  "
          }
        ],
        "parent": "q_57141",
        "author": "Micha\u0142 Kosmulski"
      },
      {
        "date": "Sep 21, 2014 10:43:55 AM",
        "commentid": "a_69457",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": -0.964,
            "sent": " use the following:         this will be parallel when sorting  because of  . i believe this is what you mean by parallel sort?  "
          }
        ],
        "parent": "q_57141",
        "author": "bcsb1001"
      }
    ],
    "sent": [
      {
        "topicid": 8,
        "systemtopicid": 8,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Database",
        "linePolarity": -0.964,
        "sent": " java 8 provides      which sorts arrays in parallel using the fork-join framework.  but there's no corresponding   for sorting lists.    i can use    sort that array  and store the result back in my list  but that will temporarily increase memory usage  which if i'm using parallel sorting is already high because parallel sorting only pays off for huge lists.  instead of twice the memory (the list plus parallelsort's working memory)  i'm using thrice (the list  the temporary array and parallelsort's working memory).  (arrays.parallelsort documentation says \\\"the algorithm requires a working space no greater than the size of the original array\\\".)    memory usage aside  collections.parallelsort would also be more convenient for what seems like a reasonably common operation.  (i tend not to use arrays directly  so i'd certainly use it more often than arrays.parallelsort.)    the library can test for  randomaccess  to avoid trying to e.g. quicksort a linked list  so that can't a reason for a deliberate omission.     how can i sort a list in parallel without creating a temporary array?   "
      }
    ]
  },
  {
    "author": "Maroun Maroun",
    "parent": "",
    "title": "\"Java overloading with variable length arguments\"",
    "commentid": "q_6992",
    "date": "Dec 15, 2014 4:50:41 AM",
    "children": [
      {
        "date": "Dec 15, 2014 5:20:43 AM",
        "commentid": "a_8565",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.0,
            "sent": " the rules for selecting the correct overloaded method are as follows:      primitive widening uses the smallest method argument possible   wrapper type cannot be widened to another wrapper type   you can box from int to integer and widen to object but no to long   widening beats boxing  boxing beats var-args.   you can box and then widen (an int can become object via integer)   you cannot widen and then box (an int cannot become long)   you cannot combine var-args  with either widening or boxing      have a look at that last rule. you can  not  combine widening or boxing with variable length arguments. that means that the types can not be manipulated in any way and you have to perform the comparison as is.   and   can be compared  no problem and the compiler can deduce that   is the smaller of the two. as per the first rule  it will go for the smallest method argument possible  hence it has worked out the correct (and only) route to a method.    however  when you get to   and    there exists no comparison method between the two because of java's  strong typing . with no knowledge of which type is smallest  the compiler has absolutely no clue which method you mean.     more visual example     let's take it step by step from the perspective of the compiler. first  with   and  .     int and long      step 1 - checking if the parameters match any arguments and if so  which one it matches exactly     well    means that you can pass   to many arguments. in this case  you've elected to pass   arguments  hence your call matches both the   type and the   type.     step 2 - attempt to autobox or widen. this should help it work out which one to go for     you're using varargs  so the compiler knows it can't do this  as per the final rule.     step 3 - attempt to work out which type is smallest     the compiler is able to compare the type   with the type  . from this  it works out that the   is the smallest type.     step 4 - make the call     with the knowledge that   is the smallest type  it then passes the value to the method for execution.    okay  and now let's do the same thing with   and  .     boolean and int      step 1 - checking if the parameters match any arguments and if so  which one it matches exactly     same story. you've passed nothing so match both arguments.     step 2 - attempt to autobox or widen. this should help it work out which one to go for     as above  you're not permitted to do this because you used varargs.     step 3 - attempt to work out which type is smallest     this is the crucial difference. here  the types are  not  comparable. this means that the compiler doesn't know which method you want to call by your parameters  or  by the smallest type. ergo  it has been unable to work out the correct route.     step 4 - make the call     without the knowledge of which method to call  it can not continue execution and throws the appropriate exception.  "
          }
        ],
        "parent": "q_6992",
        "author": "christopher"
      },
      {
        "date": "Dec 15, 2014 5:31:37 AM",
        "commentid": "a_8566",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.0,
            "sent": " in your second example  the compiler is unable to determine the  most specific method  to invoke.     the gory details are explained in   the language spec    but essentially if two variable-arity (var-arg) methods are being compared  then if method a could accept the arguments passed to method b  but not the other way around  then method b is most specific.    in your first example  the   rules of primitive sub-typing   are applied  which are:        double > 1  float        float > 1  long        long > 1  int        int > 1  char        int > 1  short        short > 1  byte         ( where > 1  means 'direct supertype of' )       here we can see that an   is more specific than a    so your   method is chosen.    in the second example  the compiler is choosing between an   and  . there is no sub-type relationship between those primiritve types  therefore there is no most specific method and \\\" the method invocation is ambiguous  and a compile-time error occurs. \\\" (last line in 15.12.2.5).  "
          }
        ],
        "parent": "q_6992",
        "author": "Duncan"
      },
      {
        "date": "Dec 15, 2014 5:04:18 AM",
        "commentid": "a_8567",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.0,
            "sent": " when you have   and   (comparable types)  the smallest will be used by default  which is   (since you're not passing an argument) - i think it's because    can  be widen to   but   cannot (unless you  explicitly  cast it)  the compiler will select the lowest precision type.    but when you have   and    the comparison cannot be done and you'll get        "
          }
        ],
        "parent": "q_6992",
        "author": "Maroun Maroun"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.0,
        "sent": " why is there no compile error in this code:         yet this code gives compile error?         i believe there should be compile error in both the cases  but this is not so.  "
      }
    ]
  },
  {
    "author": "Peter",
    "parent": "",
    "title": "\"Difference between calling a class constructor and using Class.forName().newInstance\"",
    "commentid": "q_56936",
    "date": "Sep 21, 2014 10:52:55 PM",
    "children": [
      {
        "date": "Sep 21, 2014 10:59:57 PM",
        "commentid": "a_69194",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.6680000000000001,
            "sent": "      calls the   operator and the constructor of   directly  where   is mentioned in the source text and has therefore already been loaded and initialized.           look to see if   has already been loaded   load it if necessary   initialize it if necessary   locate the no-args constructor via reflection   call the   operator and the no-args constructor via reflection   typecast the result to      "
          }
        ],
        "parent": "q_56936",
        "author": "EJP"
      },
      {
        "date": "Sep 21, 2014 11:00:19 PM",
        "commentid": "a_69195",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.6680000000000001,
            "sent": " using reflection   is not only a costly operation (because the correct class-loader needs to be delegated to and load the class during  runtime )  but also makes your code more difficult to debug and you lose all the advantage of type safety (which takes place during compilation).     conclusion:     avoid reflection unless you  must  use it (for example if you're writing aspect-oriented plugin/library)  "
          }
        ],
        "parent": "q_56936",
        "author": "alfasin"
      },
      {
        "date": "Sep 21, 2014 11:15:34 PM",
        "commentid": "a_69196",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.6680000000000001,
            "sent": " the primary difference between the conventional   and   is that newinstance allows the flexibility to instantiate a class that you don't know until runtime  and makes your code more dynamic. when the class is not known until runtime  then it is a valid case where you should use reflection.    from the  javadoc   the invocation   returns the class object associated with the class or interface with the given string name i.e. it returns the class a     so   breaks down to:          returns the class a of type class.       creates a new instance of the class represented by this class object  so you get an instance of type a. the class is instantiated as if by a new expression with an empty argument list. the class is initialized if it has not already been initialized. this is here actually equivalent to a new a() and which returns a new instance of a.    important:  use of this method effectively bypasses the compile-time exception checking that would otherwise be performed by the compiler.        reference:       javadoc for class     "
          }
        ],
        "parent": "q_56936",
        "author": "Infinite Recursion"
      },
      {
        "date": "Sep 21, 2014 11:24:56 PM",
        "commentid": "a_69197",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.6680000000000001,
            "sent": " the difference between   operator and   method is listed below:       new operator can be used with any constructor of the class by passing any number of arguments as accepted by the constructor. ewinstance method requires the presence of no-arg constructor in the class for which it has been invoked. if you want to use a constructor with newinstance  then you need to get an instance of constructor class for any constructor and then invoke newinstance like:          using new operator doesn?t require explicit class loading as it is internally handled by the jvm. for newinstance() method an instance of that class?s class object is required (class.forname(?a?).newinstance(); or as shown in above point). the class object referring to the underlying class is obtained by invoking the forname method.     the use of new operator is recommended when the name of class is known at compile time. since newinstance uses reflection to create an object of class  it is recommended to be used when the class is not known at compile time but is determined at run time.     since there is no extra processing related to method invocation like forname in new operator  it is faster than newinstance. the use of newinstance results in extra processing on part of jvm (type checks  security checks) and hence is not recommended to be used for performance degradation reasons.(at least when thousands of instances are being created using newinstance)     all java developers are supposed to know the new operator as it is basic concept which is taught at beginner level  so there is nothing special to learn about it. ot all developers working on a application be aware of reflection and hence there is a learning curve for beginners working on the code with newinstance method.     you can see new operator being used in any normal java program. ewinstance is being used at multiple places inside java especially in server side like loading and instantiating servlets  applets  jndi stub/skeletons  jdbc database drivers.     with new operator  the class loading and object creation is done by the default class loader of jdk.but with newinstance method  one can explicitly specify the class loader to be used for loading class and object instantiation.     there are very less chances for runtime exception with new operator. only rare case is when the class was present during compile time but was not available on classpath during runtime. the use of newinstance method with class.forname(string ?) can result in runtime exception even if the class name passed as argument to forname method is invalid.     the use of new operator results in generation of corresponding byte code in the .class file. when newinstance is used  there is no extra byte code generated for object creation inside the class as object creation is handled dynamically.     with new operator there is inherent type checking and compiler error is shown if the class doesn?t exist. since the class name is passed as argument to class.forname method as string  there is no compile type checking and usually results in run time exception as described in one of the earlier points.       ref:  http://www.javaexperience.com/difference-between-new-operator-and-class-forname-newinstance/   "
          }
        ],
        "parent": "q_56936",
        "author": "Vipul Paralikar"
      },
      {
        "date": "Sep 21, 2014 11:00:19 PM",
        "commentid": "a_69198",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.6680000000000001,
            "sent": " the test you use to measure speed is not a valid test. java performance is very complex and it involves hotspot vm  smart compilers and garbage collectors to start with.    in method 1 java wil be usually smart enough to only make 1 instance of a in memory and reuse it for each iteration    in method 2 you are forcing the vm to use reflection and classloaders to make the object. that in itself is already slower  "
          }
        ],
        "parent": "q_56936",
        "author": "Peter"
      }
    ],
    "sent": [
      {
        "topicid": 2,
        "systemtopicid": 2,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Loops",
        "linePolarity": -0.6680000000000001,
        "sent": " i have been trying to understand the difference between using   to instantiate an object vs using  .    i have run the following code for a simple class   which shows using   is 70-100 times slower than using just  .    i am curious to know why there is such a difference in time  but couldn't figure out. please someone help me to understand the reason.       "
      }
    ]
  },
  {
    "author": "OO7",
    "parent": "",
    "title": "\"Differences between Java 8 Date Time API (java.time) and Joda-Time\"",
    "commentid": "q_97835",
    "date": "Jul 8, 2014 5:28:57 AM",
    "children": [
      {
        "date": "Jul 8, 2014 8:20:43 AM",
        "commentid": "a_120149",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.28400000000000003,
            "sent": "  common features     a) both libraries use immutable types. joda-time also offers additional mutable types like  .     b) furthermore: both libraries are inspired by the design study  \\\"timeandmoney\\\" from eric evans  or ideas from  martin fowler about domain driven style  so they strive more or less for a  fluent programming style  (although not always perfect ;-)).    c) with both libraries we get a real calendar date type (called  )  a real wall time type (called  ) and the composition (called  ). that is a very big win compared with old   and  .    d) both libraries use a method-centric approach meaning they encourage the user to use   instead of  . this causes a lot of extra methods compared with   (although latter is not type-safe at all due to excessive use of ints).     performance     see the other answer by @oo7 pointing to the analysis of mikhail vorontsov although point 3 (exception catching) is probably obsolete - see  this jdk-bug . the different performance (which is in general favour of  jsr-310 ) is mainly due to the fact that the internal implementation of  joda-time  always use a machine-time-like long-primitive (in milliseconds).     null     joda-time often use null as default for system timezone  default locale  current timestamp etc. while jsr-310 almost always rejects null values.     precision     jsr-310 handles  nanonsecond  precision while joda-time is limited to  millisecond  precision.      supported fields:     an overview about supported fields in java-8 (jsr-310) is given by some classes in the temporal-package (for example  chronofield  and  weekfields ) while joda-time is rather weak on this area - see  datetimefieldtype . the biggest lack of joda-time is here the absence of localized week-related fields. a common feature of both field implementation design is that both are based on values of type long (no other types  not even enums).     enum     jsr-310 offers  enums  like   or   while joda-time does not offer this because it was mainly developed in years 2002-2004 before  java 5 .     zone api     a) jsr-310 offers more timezone features than joda-time. latter is not able to yield a programmatical access to the history of timezone offset transitions while jsr-310 is capable to do this.    b) for your information: jsr-310 has moved its internal timezone repository to a new location and a different format. the old library folder lib/zi does not exist any more.     adjuster vs. property     jsr-310 has introduced the  -interface as a formalized way to externalize temporal calculations and manipulations  especially for library or framework-writers this is a nice and relative easy way to embed new extensions of jsr-310 (a kind of equivalent to static helper classes for former  ).    for most users however  this feature has very limited value because the burden to write code is still with the user. built-in solutions based on the new  -concept are not so many  there is currently only the helper class   with a limited set of manipulations (and the enums   or other temporal types).     joda-time offers a field-package but practice has shown evidence that new field implementations are very hard to code. on the other side joda-time offers so-called properties which make some manipulations much easier and more elegant than in jsr-310  for example  property.withmaximumvalue() .     calendar systems     jsr-310 offers 4 extra calendar systems. the most interesting one is  umalqura  (used in saudi arabia). the other 3 are:  minguo  (taiwan)  japanese (only the modern calendar since 1871!) and  thaibuddhist  (only correct after 1940).    joda-time offers an  islamic calendar  based on calculatory base - not a sighting-based calendar like umalqura. thai-buddhist is also offered by joda-time in a similar form  minguo and the japanese one not. otherwise joda-time offers coptic and ethiopic calendar  too (i cannot say anything about the quality and correctness here).    more interesting for europeans: joda-time also offers a  gregorian    julian  and mixed-gregorian-julian calendar. however  the practical value for real historical calculations is limited because important features like different year starts in date history are not supported at all (the same criticism is valid for old  ).    other calendars like  hebrew  or  persian  or  hindu  are completely missing in both libraries.     epoch days     jsr-310 has the class  julianfields  while joda-time (version 2.0) offers some helper methods in the class  datetimeutils .     clocks     jsr-310 has no interface (a design mistake) but an abstract class   which can be used for any clock dependency injection. joda-time offers the interface  millisprovider  and some helper methods in  datetimeutils  instead. so this way joda-time is also capable of supporting test-driven models with different clocks (mocking etc.).     duration arithmetic     both libraries support the calculation of time distances in one or more temporal units. however  when handling single-unit-durations the jsr-310-style is obviously nicer (and long-based instead of using int):    jsr-310 =&gt;      joda-time =&gt;      handling of multiple-unit-durations are also different. even the calculation results can differ - see this closed  joda-time issue . while jsr-310 use a very simple and limited approach to use just the classes   (duration based on years  months and days) and   (based on seconds and nanoseconds)  joda-time uses a more sophisticated way using the class   in order to control in which units a duration (joda-time call it \\\"period\\\") shall be expressed. while the  -api is somehow awkward to use a similar way is not offered by jsr-310 at all. especially it is not yet possible in jsr-310 to define mixed date and time durations (based on days and hours for example). so be warned if it comes to migration from one library to another. the libraries in discussion are incompatible - despite of partially same class names.     intervals     jsr-310 does not support this feature while joda-time has limited support. see also this  so-answer .     formatting and parsing     best way to compare both libraries is to view the equal-named classes  datetimeformatterbuilder  (jsr-310) and  datetimeformatterbuilder  (joda-time). the jsr-310-variant is a little bit more powerful (can also handle any kind of   provided the field implementor has managed to code some extension points like  resolve() ). most important difference is however - in my opinion: jsr-310 can much better parse timezone names (format pattern symbol z) while joda-time could not do this at all in its earlier versions and now only in a very limited way.    otherwise it should be noted that joda-time can format durations using  periodformatter . jsr-310 cannot do this.       hope this overview helps. all the gathered information is mainly there due to my efforts and investigations how to design and implement a better date-and-time library (nothing is perfect).  "
          }
        ],
        "parent": "q_97835",
        "author": "Meno Hochschild"
      },
      {
        "date": "Jul 8, 2014 6:35:36 AM",
        "commentid": "a_120150",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": -0.28400000000000003,
            "sent": "  java 8 date/time :       java 8 classes are built around the human time. it makes them fast for human datetime arithmetics/conversion.   date/time component getters like getdayofmonth have o(1) complexity in java 8 implementation.   parsing of offsetdatetime/offsettime/zoneddatetime is very slow in java 8 ea b121 due to exceptions thrown and caught internally in the jdk.    a set of packages: java.time.   java.time.chrono.   java.time.format.  java.time.temporal.* java.time.zone.*    instants (timestamps) date and time partial date and time parser and formatter time zones different chronologies (calendars).    existing classes have issues like date has no support for i18n or l10n. they are mutable!.   simpler &amp; more robust.   clocks can be injected.    clocks can be created with various properties - static clocks  mocked clocks  low-precision clocks (whole seconds  whole minutes  etc).    clocks can be created with specific time zones. clock.system(zone.of(?america/los_angeles?)).    makes code handling date and time testable.    makes tests independent of timezone.       joda-time :       joda-time is using machine time inside. a manual implementation based on int/long values would be much faster.   joda-time getters require the computer-to-human time calculation on every getter call  which makes joda-time a bottleneck in such scenarios.    it is composed of immutable classes it handles instants  date &amp; time  partials  and durations it is flexible it is well designed.    represents dates as instants. but a date&amp;time may correspond to more than one instant. overlap hour when daylight savings end. as well as not have any instant that corresponds to it at all. gap hour when daylight starts. has to perform complex computations for simple operations.    accepts nulls as valid values on most of its methods. leads to subtle bugs.        for more detailed comparision see :-      java 8 date/time library performance (as well as joda-time 2.3 and j.u.calendar) . &amp;  new date &amp; time api in java 8   "
          }
        ],
        "parent": "q_97835",
        "author": "OO7"
      }
    ],
    "sent": [
      {
        "topicid": 3,
        "systemtopicid": 3,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Servlets",
        "linePolarity": -0.28400000000000003,
        "sent": " i know there are questions relating to  java.util.date  and joda-time. but after some digging  i couldn't find a thread about the differences between the  java.time api  (new in  java 8   defined by  jsr 310 ) and  joda-time .    i have heard that java 8?s java.time api is much cleaner and can do much more than joda-time. but i cannot find examples comparing the two.       what can java.time do that joda-time cannot?    what can java.time do better than joda-time?    is the performance better with java.time?    "
      }
    ]
  },
  {
    "author": "Jeffrey Bosboom",
    "parent": "",
    "title": "\"Why does Collection.parallelStream() exist when .stream().parallel() does the same thing?\"",
    "commentid": "q_98732",
    "date": "Jul 6, 2014 9:44:20 PM",
    "children": [
      {
        "date": "Jul 6, 2014 9:44:20 PM",
        "commentid": "a_121217",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8559999999999999,
            "sent": " the javadocs for   and   itself don't answer the question  so it's off to the mailing lists for the rationale.  i went through the lambda-libs-spec-observers archives and found  one thread specifically about collection.parallelstream()  and another thread that touched on whether  java.util.arrays should provide parallelstream()  to match (or actually  whether it should be removed).  there was no once-and-for-all conclusion  so perhaps i've missed something from another list or the matter was settled in private discussion.  (perhaps  brian goetz   one of the principals of this discussion  can fill in anything missing.)    the participants made their points well  so this answer is mostly just an organization of the relevant quotes  with a few clarifications in  [brackets]   presented in order of importance (as i interpret it).    parallelstream() covers a very common case     brian goetz  in the first thread  explaining why   is valuable enough to keep even after other parallel stream factory methods have been removed:        we do  not  have explicit parallel versions of each of these  [stream factories] ; we did    originally  and to prune down the api surface area  we cut them on the    theory that dropping 20+ methods from the api was worth the tradeoff of    the surface yuckiness and performance cost of  .      but we did not make that choice with collection.        we could either remove the    or we could add    the parallel versions of all the generators  or we could do nothing and    leave it as is.  i think all are justifiable on api design grounds.        i kind of like the status quo  despite its inconsistency.  instead of    having 2n stream construction methods  we have n+1 -- but that extra 1    covers a huge number of cases  because it is inherited by every    collection.  so i can justify to myself why having that extra 1 method    is worth it  and why accepting the inconsistency of going no further is    acceptable.        do others disagree?  is n+1  [collections.parallelstream() only]  the practical choice here?  or should we go    for the purity of n  [rely on stream.parallel()] ?  or the convenience and consistency of 2n  [parallel versions of all factories] ?  or is    there some even better n+3  [collections.parallelstream() plus other special cases]   for some other specially chosen cases we    want to give special support to?       brian goetz  stands by this position in the later discussion about  :        i still really like collection.parallelstream; it has huge    discoverability advantages  and offers a pretty big return on api    surface area -- one more method  but provides value in a lot of places     since collection will be a really common case of a stream source.      parallelstream() is more performant     brian goetz :        direct version  [parallelstream()]  is more performant  in that it requires less wrapping (to    turn a stream into a parallel stream  you have to first create the    sequential stream  then transfer ownership of its state into a new    stream.)      in response to kevin bourrillion's skepticism about whether the effect is significant   brian again :        depends how seriously you are counting.  doug counts individual object    creations and virtual invocations on the way to a parallel operation     because until you start forking  you're on the wrong side of amdahl's    law -- this is all \\\"serial fraction\\\" that happens before you can fork    any work  which pushes your breakeven threshold further out.  so getting    the setup path for parallel ops fast is valuable.       doug lea follows up   but hedges his position:        people dealing with parallel library support need some attitude   adjustment about such things. on a soon-to-be-typical machine    every cycle you waste setting up parallelism costs you say 64 cycles.   you would probably have had a different reaction if it required 64   object creations to start a parallel computation.        that said  i'm always completely supportive of forcing implementors   to work harder for the sake of better apis  so long as the   apis do not rule out efficient implementation. so if killing     is really important  we'll find some way to   turn   into a bit-flip or somesuch.      indeed  the later discussion about    takes notice of lower stream.parallel() cost .    stream().parallel() statefulness complicates the future    at the time of the discussion  switching a stream from sequential to parallel and back could be interleaved with other stream operations.   brian goetz  on behalf of doug lea   explains why sequential/parallel mode switching may complicate future development of the java platform:        i'll take my best stab at explaining why: because it (like the stateful    methods (sort  distinct  limit)) which you also don't like  move us    incrementally farther from being able to express stream pipelines in    terms of traditional data-parallel constructs  which further constrains    our ability to to map them directly to tomorrow's computing substrate     whether that be vector processors  fpgas  gpus  or whatever we cook up.        filter-map-reduce map[s] very cleanly to all sorts of parallel computing    substrates; filter-parallel-map-sequential-sorted-limit-parallel-map-uniq-reduce    does not.        so the whole api design here embodies many tensions between making it    easy to express things the user is likely to want to express  and doing    is in a manner that we can predictably make fast with transparent cost    models.      this mode switching was  removed after further discussion .  in the current version of the library  a stream pipeline is either sequential or parallel; last call to  /  wins.  besides side-stepping the statefulness problem  this change also improved the performance of using   to set up a parallel pipeline from a sequential stream factory.    exposing parallelstream() as a first-class citizen improves programmer perception of the library  leading them to write better code     brian goetz again   in response to  tim peierls's argument  that   allows programmers to understand streams sequentially before going parallel:        i have a slightly different viewpoint about the value of this sequential    intuition -- i view the pervasive \\\"sequential expectation\\\" as one if the    biggest challenges of this entire effort; people are  constantly     bringing their incorrect sequential bias  which leads them to do stupid    things like using a one-element array as a way to \\\"trick\\\" the \\\"stupid\\\"    compiler into letting them capture a mutable local  or using lambdas as    arguments to map that mutate state that will be used during the    computation (in a non-thread-safe way)  and then  when its pointed out    that what they're doing  shrug it off and say \\\"yeah  but i'm not doing    it in parallel.\\\"        we've made a lot of design tradeoffs to merge sequential and parallel    streams.  the result  i believe  is a clean one and will add to the    library's chances of still being useful in 10+ years  but i don't    particularly like the idea of encouraging people to think this is a    sequential library with some parallel bags nailed on the side.    "
          }
        ],
        "parent": "q_98732",
        "author": "Jeffrey Bosboom"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.8559999999999999,
        "sent": " in java 8  the collection interface was extended with two methods that return  :    which returns a sequential stream  and    which returns a possibly-parallel stream.  stream itself also has a   method that returns an equivalent parallel stream (either mutating the current stream to be parallel or creating a new stream).    the duplication has obvious disadvantages:       it's confusing.  a question asks  whether calling both parallelstream().parallel() is necessary to be sure the stream is parallel   given that parallelstream() may return a sequential stream.  why does parallelstream() exist if it can't make a guarantee?  the other way around is also confusing -- if parallelstream() returns a sequential stream  there's probably a reason (e.g.  an inherently sequential data structure for which parallel streams are a performance trap); what should stream.parallel() do for such a stream?  (unsupportedoperationexception is not allowed by parallel()'s specification.)     adding methods to an interface risks conflicts if an existing implementation has a similarly-named method with an incompatible return type.  adding parallelstream() in addition to stream() doubles the risk for little gain.  (note that parallelstream() was at one point just named parallel()  though i don't know if it was renamed to avoid name clashes or for another reason.)       why does collection.parallelstream() exist when calling collection.stream().parallel() does the same thing?  "
      }
    ]
  },
  {
    "author": "Boris the Spider",
    "parent": "",
    "title": "\"Generics  Type Parameters and Wildcards\"",
    "commentid": "q_7590",
    "date": "Dec 13, 2014 6:10:12 PM",
    "children": [
      {
        "date": "Dec 13, 2014 6:32:33 PM",
        "commentid": "a_9290",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -1.0,
            "sent": " the main thing to understand with generic types  is that they aren't covariant.    so whilst you can do this:         the following will not compile:         this is to avoid the situations where you circumvent the generic types:         so  going through your examples one by one    1    your generic method takes a    you pass in a  ; which is (essentially) a  .   can be assigned to the   type and the compiler is happy.    2    your generic method is the same  you pass in a  .   can be assigned to the   type and the compiler is again happy.    3    this is basically the same as  2  with another level of nesting.   is still the   type.    4    here is where it goes a little pear shaped  and where my point from above comes in.    your generic method takes a  . you pass in a  . now  as generic types are not covariant    cannot be assigned to a  .    the actual compiler error (java 8) is:        required:     found:       reason: cannot infer   type-variable(s)         (argument mismatch;   cannot be converted to  )      basically the compiler is telling you that it cannot find a   to assign because of having to infer the type of the   nested in the outer list.    lets look at this in a little more detail:      is a   of  some unknown type  - it could be a   or a  ; we can   from it as     but we cannot   . because otherwise we run into the covariance issue i mentioned.      is a   of   of some unknown type - it could be a   or a  . in case  1  it was possible to assign   to   and just not allow   operations on wildcard list. in case  4  this cannot be done - primarily because there is not a generics construct to prevent   to the outer  .    if the compiler were to assign   to   in the second case then something like the following would be possible:         so  due to covariance  it is not possible to assign a   to  any other  generic   safely.  "
          }
        ],
        "parent": "q_7590",
        "author": "Boris the Spider"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -1.0,
        "sent": " i am trying to understand java generics and they seem extremely difficult to understand. for example  this is fine...           ... as is this...         ... and this ...         ... but this doesn't compile:         can someone explain what is going on in simple language?   "
      }
    ]
  },
  {
    "author": "Elliott Frisch",
    "parent": "",
    "title": "\"Why is writing data to disk as fast as keeping it in-memory?\"",
    "commentid": "q_84934",
    "date": "Jul 30, 2014 11:36:29 PM",
    "children": [
      {
        "date": "Jul 30, 2014 11:41:09 PM",
        "commentid": "a_104063",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": 1.0,
            "sent": " as said in the comments  you're not measuring anything useful. the jvm caches the write operation in its memory  which it then flushes to the operating system  which caches it in its memory before finally writing it to disk at some point.  but you're only measuring the time it takes the jvm to cache it in its own memory (which is all you can measure).    anyway  you shouldn't bother with such micro optimisations.   "
          }
        ],
        "parent": "q_84934",
        "author": "jwenting"
      },
      {
        "date": "Jul 30, 2014 11:43:50 PM",
        "commentid": "a_104064",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": 1.0,
            "sent": " your hard drive and operating system employ write buffering so that your system can continue operation in the face of multiple concurrent tasks (for example  programs reading and writing the disk). this can (and sometimes does) lead to data loss in the event of power failure on desktop class machines. servers and laptops can also experience the issue (but usually employ sophisticated technology called a battery to mitigate the chances). anyway  on linux you might have to   and on windows you might   when it happens.  "
          }
        ],
        "parent": "q_84934",
        "author": "Elliott Frisch"
      }
    ],
    "sent": [
      {
        "topicid": 8,
        "systemtopicid": 8,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Database",
        "linePolarity": 1.0,
        "sent": " i have the following 10000000x2 matrix:         now i want to save this matrix to   array:         the output:         now i want to save this matrix to disk:         the output:         shouldn't be saving to memory much faster?  "
      }
    ]
  },
  {
    "author": "Alex",
    "parent": "",
    "title": "\"Why can enum implementations not access private fields in the enum class\"",
    "commentid": "q_86173",
    "date": "Jul 29, 2014 1:25:09 AM",
    "children": [
      {
        "date": "Jul 29, 2014 1:34:10 AM",
        "commentid": "a_105576",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8240000000000001,
            "sent": "   your abstract class is not equivalent to your enum  since enums are implicitly public static final. thus  you'll observe the same behavior if you use:         as explained in  http://docs.oracle.com/javase/tutorial/java/javaoo/nested.html   chapter \\\"static nested classes\\\":        a static nested class cannot refer directly to instance variables or   methods defined in its enclosing class: it can use them only through   an object reference.      thus the need of  . you could also use   if the field were   rather than  .  "
          }
        ],
        "parent": "q_86173",
        "author": "sp00m"
      },
      {
        "date": "Jul 29, 2014 1:34:44 AM",
        "commentid": "a_105577",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8240000000000001,
            "sent": " when an identifier is resolved  java prefers the lexical scope over inherited members. so when you have an inner class that extends the outer class and use a field of the outer class without using   or    the field of the outer instance is accessed which fails if the inner class is   as there is no outer instance then. in contrast  when using   you are explicitly accessing the inherited member. note that   classes are implicitly  . you can even use   to access the inherited member but you have to use   to access it if it?s declared  .  "
          }
        ],
        "parent": "q_86173",
        "author": "Holger"
      },
      {
        "date": "Jul 29, 2014 1:34:45 AM",
        "commentid": "a_105578",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.8240000000000001,
            "sent": " class   is an inner class of   and also a sub class. the reason you do not see an error when accessing   in it is because you are accessing the   of the outer class  not the super class.         prints  .    in the other case your enum constants behave as static nested sub classes  not inner classes  so they do not have a reference to the outer class  only their super class.  "
          }
        ],
        "parent": "q_86173",
        "author": "Alex"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.8240000000000001,
        "sent": " i just answered this question by saying how to solve the compilation problem:     how to use fields in java enum by overriding the method?     but what i don't understand is why the error is happening in the first place.    here is the example written as an enum:         here is the exact same thing as abstract classes         in the case of   within the   implementation it cannot access  . however in the abstract class case it can.    additionally adding   fixes the problem  as does removing the   modifier on the field.    does anyone know why this slight quirk in the behaviour is happening?  "
      }
    ]
  },
  {
    "author": "gontard",
    "parent": "",
    "title": "\"Do default constructors for private inner classes have a formal parameter?\"",
    "commentid": "q_89647",
    "date": "Jul 22, 2014 1:04:22 PM",
    "children": [
      {
        "date": "Jul 23, 2014 5:37:21 AM",
        "commentid": "a_109944",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.3999999999999999,
            "sent": " there is a difference between the implementation and the specification.    in my opinion the \\\"except\\\" jls statement         ... except  in a non-private inner member class...      is poorly worded.     it means  the compiler  is not required  to implicitly declares one formal parameter representing the immediately enclosing instance of the class... but  it could .       why implicitly formal parameter  is required  in non-private inner member class ?    from  jls 8.8.1 :        the member class may have been emitted by a compiler which is different than the compiler of the class instance creation expression. therefore  there must be a standard way for the compiler of the creation expression to pass a reference (representing the immediately enclosing instance) to the member class's constructor      for example if i compile this inner class with a first compiler:         if i want to compile this sub class with another compiler:         the second compiler must be able to use the default constructor with the formal parameter.  in this case the instance of the enclosing class with a   instance.       why implicitly formal parameter  is not required  in non-private inner member class ?    because a non-private inner member class is always accessed by the same compiler that compiled it. as you shown  javac generates the same constructor regardless to the class visibility but it is not require to. another compiler implementation is free to choose another way.    there is also another point in  jls 8.8.1  which is very much along the same line        in a class instance creation expression for a  local class (not in a static context)  or anonymous class  ?15.9.2 specifies the immediately enclosing instance of the local/anonymous class. the local/anonymous class is necessarily emitted by the same compiler as the class instance creation expression. that compiler can represent the immediately enclosing instance how ever it wishes.  there is no need for the java programming language to implicitly declare a parameter in the local/anonymous class's constructor.     "
          }
        ],
        "parent": "q_89647",
        "author": "gontard"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.3999999999999999,
        "sent": " caveat #1: this is actually a potential two-parter: first  does the constructor for a private inner class have a formal parameter? if yes  why does the jls say it doesn't? and if no  how/why not?    caveat #2:  this question is not for speculation.  i'm looking for  authoritative  answers only.    default constructors are defined in  jls 8.8.9   which states (in part):        the default constructor has no formal parameters  except in a  non-private  inner member class  where the default constructor implicitly declares one formal parameter representing the immediately enclosing instance of the class (?8.8.1  ?15.9.2  ?15.9.3).       (emphasis added)     the \\\"non-private\\\" bit seems odd to me: in order for an inner class to access fields defined in its enclosing class  it needs a reference to that instance. this should be the same regardless of whether the inner class is private.    in fact  javac seems to agree with me  in contradiction to the spec. if i compile this:         ...and run    then we see a constructor with a single formal parameter  for the instance of the enclosing class:         for reference  this is on oracle jdk 1.8.0_05.    so the jls says that the default constructor for private inner member classes has no formal parameters  but javac/javap say it has one. (my understanding of the most natural way for things to work would also say it should have one  for the little that's worth.) which is right  and why does the jls specifically exclude private inner classes?  "
      }
    ]
  },
  {
    "author": "Jon Skeet",
    "parent": "",
    "title": "\"Why is this Float constant null when executing the static block?\"",
    "commentid": "q_110879",
    "date": "Jun 13, 2014 3:14:17 AM",
    "children": [
      {
        "date": "Jun 13, 2014 3:19:17 AM",
        "commentid": "a_135948",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": 0.6680000000000001,
            "sent": "   is a compile-time constant -   is not  because it's of type  . if you change it to   then you'll see the value in the static initialization block.    so currently  in the static initializer block  the value of   is actually being inlined - your code is equivalent to:         from  jls section 15.28 (constant expressions) :        a constant expression is an expression denoting a value of primitive type or a string that does not complete abruptly and is composed using only the following: [...]        is not a primitive type.    additionally  even without the inlining  the constant variable   is initialized before any of the static initializer blocks are executed. from  section 12.4.2 of the jls  (class initialization details):            ...     then  initialize the static fields of c which are constant variables (?4.12.4  ?8.3.2  ?9.3.1).     ...     next  execute either the class variable initializers and static initializers of the class  or the field initializers of the interface  in textual order  as though they were a single block.        "
          }
        ],
        "parent": "q_110879",
        "author": "Jon Skeet"
      }
    ],
    "sent": [
      {
        "topicid": 3,
        "systemtopicid": 3,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Servlets",
        "linePolarity": 0.6680000000000001,
        "sent": " the following code  when executed  prints   instead of the expected  . why isn't   initialized before executing the static block?       "
      }
    ]
  },
  {
    "author": "assylias",
    "parent": "",
    "title": "\"Java8 Iterator to Stream\"",
    "commentid": "q_101507",
    "date": "Jul 1, 2014 6:05:15 AM",
    "children": [
      {
        "date": "Jul 1, 2014 6:29:33 AM",
        "commentid": "a_124655",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": " one way is to create a spliterator from the iterator and use that as a basis for your stream:         an alternative which is maybe more readable is to use an iterable - and creating an iterable from an iterator is very easy with lambdas because iterable is a functional interface:       "
          }
        ],
        "parent": "q_101507",
        "author": "assylias"
      }
    ],
    "sent": [
      {
        "topicid": 2,
        "systemtopicid": 2,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Loops",
        "linePolarity": 0.0,
        "sent": " i am looking for a concise way to convert an     to a     or more specifically to \\\"view\\\" the   as a  .    for performance reason  i would like to avoid a copy of the iterator in a new list:            based on the some suggestions in the comments  i have also tried to use    :         but i get a   (since there is no invocation of  )         i have looked at     and     but i didn't find anything.  "
      }
    ]
  },
  {
    "author": "Serge Ballesta",
    "parent": "",
    "title": "\"Why can&#39;t we set the value of static final variable in static block through class name\"",
    "commentid": "q_6370",
    "date": "Dec 16, 2014 6:43:55 AM",
    "children": [
      {
        "date": "Dec 16, 2014 7:09:54 AM",
        "commentid": "a_7735",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.0,
            "sent": " a static final variable must be initialized before use. it may be initialized either directly at declaration time  or in a static block.    but when you use   it is not seen as an initialization but as an assignation.    with a jdk 7  the error is  cannot assign a value to final variable .    that explains why it works if you remove the   keyword         edit     in the jls the correct word for initialization is  definite assignement     extract from the jls :      for every access of a local variable or blank final field x  x must be definitely assigned before the access  or a compile-time error occurs.      similarly  every blank final variable must be assigned at most once; it must be definitely unassigned when an assignment to it occurs.      such an assignment is defined to occur if and only if either  the simple name of the variable (or  for a field  its simple name qualified by this)  occurs on the left hand side of an assignment operator.      for every assignment to a blank final variable  the variable must be definitely unassigned before the assignment  or a compile-time error occurs.     emphasize mine  but i think this is the real reason for the error.  "
          }
        ],
        "parent": "q_6370",
        "author": "Serge Ballesta"
      }
    ],
    "sent": [
      {
        "topicid": 10,
        "systemtopicid": 10,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Network",
        "linePolarity": 0.0,
        "sent": " for example  consider code snap below:         why can't we use   inside a static block of the   class itself? without the class name it's working fine.    is there any reason behind this?  "
      }
    ]
  },
  {
    "author": "Sotirios Delimanolis",
    "parent": "",
    "title": "\"Why can&#39;t I use from the static method of the implemented interface?\"",
    "commentid": "q_7376",
    "date": "Dec 14, 2014 9:22:12 AM",
    "children": [
      {
        "date": "Dec 14, 2014 9:37:19 AM",
        "commentid": "a_9038",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.28400000000000025,
            "sent": "  from the java language specification          a class c inherits from its direct superclass all concrete methods m   (both static and instance) of the superclass for which all of the   following are true:            [...]            a class c inherits from its direct superclass and direct   superinterfaces all abstract and default (?9.4) methods m for which   all of the following are true:            [...]             a class does not inherit static methods from its superinterfaces.        so that method is not inherited.     you can statically import the member         or use it with the fully qualified type name         or import the type to which   belongs and invoke it with its short name       "
          }
        ],
        "parent": "q_7376",
        "author": "Sotirios Delimanolis"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.28400000000000025,
        "sent": " as you  specialists  know in java 8  interfaces can have static methods which have implementations inside themselves.    as i have read in a related tutorial  the classes which implement such interface can use its static methods. but  i have a problem which  here  i show it in a simpler example than what i have         when i implement such interface         i encounter compile error.         what's the problem?  "
      }
    ]
  },
  {
    "author": "eckig",
    "parent": "",
    "title": "\"ObservableList: how to reliably detect a setAll?\"",
    "commentid": "q_23509",
    "date": "Nov 18, 2014 5:55:16 AM",
    "children": [
      {
        "date": "Dec 9, 2014 2:53:54 AM",
        "commentid": "a_28465",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.3159999999999998,
            "sent": " unfortunately there is no reliable way of detecting this on the listener side.    the struggle starts with the default implemention  which mostly looks like this:          if you pass an empty collection to   the result and the event that is fired are both exactly the same as when you would have called  .    so your method   returns   when   has been called  too (as would the core implementation).    so in the end there is no generic detection of    it all depends on your use-case. if you can narrow down what you are trying to detect you may write a filter for that.  "
          }
        ],
        "parent": "q_23509",
        "author": "eckig"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -1.3159999999999998,
        "sent": " in some contexts it's necessary to detect - in a listchangelistener  without control about the list itself - a \\\"all data swapped out\\\"  f.i. when we need to clear some state like selection - on completely new data the old state is meaningless.     completely new data can be reached by      list.setall(...)   list.set(otherobservablelist)  if list is a listproperty      thinking about which type of changes could be fired on setall (c is the change  items is the observed list  \\\"subchangecount\\\" pseudo-code for counting the subchanges):         this seems to allow a utility check like:         in contrast  internal fx code  f.i. in listening to combobox' items:            stores the old itemcount and compare that against the current removedsize (which i'm uncomfortable with  old state gets stale far too often for my taste)  nevertheless there's a good probability that i'm missing something with my approach.    question is:    in which context would my utility method fail (and core approach would detect the setall correctly)?  "
      }
    ]
  },
  {
    "author": "Jukka K. Korpela",
    "parent": "",
    "title": "\"Which subset of Unicode symbols should I use to mark special substrings in text?\"",
    "commentid": "q_76162",
    "date": "Aug 16, 2014 12:41:30 PM",
    "children": [
      {
        "date": "Aug 16, 2014 8:17:28 PM",
        "commentid": "a_92957",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.33199999999999985,
            "sent": " no  code points in the  specials block  have their own uses. using them for other purposes may result in unexpected effects. even if you code all the processing yourself  the incoming data might contain those code points. it is of course possible to detect them and filter them out  but it is better to use code points that cannot clash with any assigned code points.    use code points in the range u+fdd0..u+fdef. they are designated as ?noncharacters? and intended for use inside an application. see the unicode faq section  private-use characters  noncharacters &amp; sentinels faq .  "
          }
        ],
        "parent": "q_76162",
        "author": "Jukka K. Korpela"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.33199999999999985,
        "sent": " our application sends   which then shall be localized on  . sometimes those are    sometimes only    so we have to mark them. it would be the best if it only used unicode as it wouldn't require any protocol changes.     example:          where 10 is length in cm but it should be converted so it is displayed as inches or mm.  are unicode special characters   right choice for marking such special substrings in text?  "
      }
    ]
  },
  {
    "author": "Aniket Thakur",
    "parent": "",
    "title": "\"Why does java.util.concurrent.RunnableFuture have a run() method?\"",
    "commentid": "q_83768",
    "date": "Aug 2, 2014 12:32:00 AM",
    "children": [
      {
        "date": "Aug 2, 2014 12:43:27 AM",
        "commentid": "a_102529",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": 0.6680000000000001,
            "sent": " it's defined in the interface so that they can attach  -specific javadoc to it. there's no  technical  significance.  "
          }
        ],
        "parent": "q_83768",
        "author": "T.J. Crowder"
      },
      {
        "date": "Aug 2, 2014 12:45:21 AM",
        "commentid": "a_102530",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": 0.6680000000000001,
            "sent": " there are no docs that provide such explanation. so i am going to provide my opinion.    i dont think it has any major significance. imagine how the interface world look         though it is perfectly valid it does not clearly indicate its purpose. so in my opinion it is just been provided for easy understanding for   method specific to   interface. so that you know to put your runnable logic by overriding   method.    another point that i can think of is   is one of the early interfaces and if you see the run() method it is         and    and   keywords are redundant as methods in an interface are by default    and  . to improvise this might be one of the reasons.  "
          }
        ],
        "parent": "q_83768",
        "author": "Aniket Thakur"
      }
    ],
    "sent": [
      {
        "topicid": 8,
        "systemtopicid": 8,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Database",
        "linePolarity": 0.6680000000000001,
        "sent": " as i was going through jdk 7  i found that   has a run method. i wonder what the significance of duplicating the same run method signature in the interface is when it already extends  .       "
      }
    ]
  },
  {
    "author": "niculare",
    "parent": "",
    "title": "\"Comparing two maps\"",
    "commentid": "q_92136",
    "date": "Jul 17, 2014 3:29:32 PM",
    "children": [
      {
        "date": "Jul 17, 2014 3:34:25 PM",
        "commentid": "a_112954",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.28400000000000003,
            "sent": " quick answer    you should use the   method since this is implemented to perform the comparison you want.   itself uses an iterator just like   but is a more inefficient approach. additionally   as @teepeemm pointed out   is affected by order of elements (basically iterator return order) and hence is not gauranteed to provide same output for 2 different maps (especially if we compare two different maps).      note/warning : your question and my answer assumes that classes implementing the map interface respect expected   and   behavior. the default java classes do so  but a custom map class needs to be examined to verify expected behavior.     see:  http://docs.oracle.com/javase/7/docs/api/java/util/map.html              compares the specified object with this map for equality. returns true   if the given object is also a map and the two maps  represent the same   mappings . more formally   two maps m1 and m2 represent the same   mappings if m1.entryset().equals(m2.entryset()) . this ensures that the   equals method works properly across different implementations of the   map interface.      implementation in java source (java.util.abstractmap)    additionally  java itself takes care of iterating through all elements and making the comparison so you don't have to. have a look at the implementation of   which is used by classes such as  :         comparing two different types of maps      fails miserably when comparing a   and   though   does compare contents correctly.      code:           output:        "
          }
        ],
        "parent": "q_92136",
        "author": "maythesource.com"
      },
      {
        "date": "Jul 17, 2014 3:34:01 PM",
        "commentid": "a_112955",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.28400000000000003,
            "sent": " as long as you override   on each key and value contained in the map  then   should be reliable to check for maps equality.    the same result can be obtained also by comparing   of each map as you suggested  but using   is a more intuitive approach.    may not be your specific situation  but if you store arrays in the map  may be a little tricky  because they must be compared value by value  or using  . more details about this see  here .  "
          }
        ],
        "parent": "q_92136",
        "author": "niculare"
      }
    ],
    "sent": [
      {
        "topicid": 2,
        "systemtopicid": 2,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Loops",
        "linePolarity": -0.28400000000000003,
        "sent": " this might be a dumb question but i couldn't find if what i was thinking is right or wrong. i have two maps   and the   here could basically be another   which goes on deeper. now i want to check if two maps are exactly the same without knowing the depth of them. so  instead of using recursion (which i hate) can i just to   on both and check if the strings are the same? or is there a better/simpler way of comparing the maps?  "
      }
    ]
  },
  {
    "author": "Stuart Marks",
    "parent": "",
    "title": "\"How to debug stream().map(...) with lambda expressions?\"",
    "commentid": "q_100582",
    "date": "Jul 2, 2014 3:02:59 PM",
    "children": [
      {
        "date": "Jul 2, 2014 3:33:13 PM",
        "commentid": "a_123483",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.748,
            "sent": " i usually have no problem debugging lambda expressions while using eclipse kepler or intellij idea (using jdk8u5). just set a breakpoint and be sure not to inspect the whole lambda expression (inspect only the lambda body).          another approach is to use   to inspect the elements of the stream:          update:     i think you're getting confused because   is an   - in other words: it is a lazy operation which will be executed only after a   was executed. so when you call   the lambda body isn't being executed at the moment. you need to set a breakpoint and inspect it after a terminal operation was called (   in this case).    check  stream operations  for further explanations.     update 2:     quoting  holger's  comment:         what makes it tricky here is that the call to map and the lambda   expression are in one line so a line breakpoint will stop on two   completely unrelated actions.         inserting a line break right after     would allow you to set a break point for the lambda expression only.   and it?s not unusual that debuggers don?t show intermediate values of   a   statement. changing the lambda to     would allow you to inspect result. again  insert line   breaks appropriately when stepping line by line?    "
          }
        ],
        "parent": "q_100582",
        "author": "Marlon Bernardes"
      },
      {
        "date": "Jul 2, 2014 8:55:38 PM",
        "commentid": "a_123484",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.748,
            "sent": " debugging lambdas also works well with netbeans. i'm using netbeans 8 and jdk 8u5.    if you set a breakpoint on a line where there's a lambda  you actually will hit once when the pipeline is set up  and then once for each stream element. using your example  the first time you hit the breakpoint will be the   call that's setting up the stream pipeline:         you can see the call stack and the local variables and parameter values for   as you'd expect. if you continue stepping  the \\\"same\\\" breakpoint is hit again  except this time it's within the call to the lambda:         note that this time the call stack is deep within the streams machinery  and the local variables are the locals of the lambda itself  not the enclosing   method. (i've changed the values in the   list to make this clear.)    as  marlon bernardes  pointed out (+1)  you can use   to inspect values as they go by in the pipeline. be careful though if you're using this from a parallel stream. the values can be printed in an unpredictable order across different threads. if you're storing values in a debugging data structure from    that data structure will of course have to be thread-safe.    finally  if you're doing a lot of debugging of lambdas (especially multi-line statement lambdas)  it might be preferable to extract the lambda into a named method and then refer to it using a method reference. for example          this might make it easier to see what's going on while you're debugging. in addition  extracting methods this way makes it easier to unit test. if your lambda is so complicated that you need to be single-stepping through it  you probably want to have a bunch of unit tests for it anyway.  "
          }
        ],
        "parent": "q_100582",
        "author": "Stuart Marks"
      }
    ],
    "sent": [
      {
        "topicid": 2,
        "systemtopicid": 2,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Loops",
        "linePolarity": -0.748,
        "sent": " in our project we are migrating to java 8 and we are testing the new features of it.    on my project i'm using guava predicates and functions to filter and transform some collections using   and  .     on this migration i need to change for example guava code to java 8 changes. so  the changes i'm doing are the kind of:         to...         using guava i was very confortable debugging the code since i could debug each transformation process but my concern is how to debug for example  .    using the debugger i can see some code like:         but it isn't as straighforward as guava to debug the code  actually i couldn't find the   transformation.    is there a way to see this transformation or a way to easy debug this code?     edit: i've added answer from different comments and posted answers     thanks to   comment that answered my question  the approach of having lambda block allowed me to see the transformation process and debug what happened inside lambda body:         thanks to   the approach of having method references also allowed me to debug the transformation process:         thanks to   answer i noticed that my eclipse doesn't show what it should and the usage of peek() helped to display results.   "
      }
    ]
  },
  {
    "author": "Holger",
    "parent": "",
    "title": "\"Lambdas and functional interfaces with generic throw clauses\"",
    "commentid": "q_110995",
    "date": "Jun 12, 2014 11:39:34 PM",
    "children": [
      {
        "date": "Jun 13, 2014 10:31:36 AM",
        "commentid": "a_136110",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.33199999999999985,
            "sent": " i don?t think that there is a rule forbidding this pattern. it?s rather likely you found a compiler bug.    it?s easy to show that this pattern does not result in unsound code by just writing down the equivalent inner class code of  :         this compiles and runs without any problems  even under java?6 (i assume it would even run on java?5 but i had no jdk to test it) and there is no reason why it shouldn?t work when doing the same with a lambda. writing down this code in netbeans results even in the recommendation to convert it to a lambda.    there is also no runtime restriction which would forbid such a construct. besides the fact that under the hood there are no exception rules enforced and everything relies on the compile-time checks  we can even prove that it would work if the compiler accepted our code by creating the code manually that the compiler would create:         this code runs and produces   as expected  regardless of which   we use to  . only the exceptions we have to catch differ. but as said  that?s a compile-time artifact.  "
          }
        ],
        "parent": "q_110995",
        "author": "Holger"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.33199999999999985,
        "sent": " consider this snippet of java 8 code:         the lambda at the call to   compiles fine  whereas the lambda at the call to   does not compile  but rather gives this compile error:         why is this?    it seems to me that both the   and   methods are equivalent: by the rules of type erasure    becomes   as it is unbounded  and   becomes    as that's the upper type bound. so why does the compiler think the overridden method does not throw java.lang.exception?    even disregarding type erasure  which is likely not relevant here because this is all happening at compile time  it still does not make sense to me: i don't see a reason why this pattern  if allowed  would result in  say  unsound java code.    so can someone enlighten me as to why this isn't allowed?     update:     so i found something that's maybe even more interesting. take the above file  change each occurrence of   to   and add throws clause to  . compile works! change back to  : compile breaks!    this compiles fine:         at this point it's starting to look more and more like a java bug...  "
      }
    ]
  },
  {
    "author": "Saher Ahwal",
    "parent": "",
    "title": "Related to String interning",
    "commentid": "q_399",
    "date": "Dec 30, 2014 12:38:18 AM",
    "children": [
      {
        "date": "Dec 30, 2014 12:43:38 AM",
        "commentid": "a_477",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": 2.0,
            "sent": " the expression         is not a compile time constant. actually  it compiles to:         (or similar) which creates a new string object at runtime.    however  because   is final  the compiler  can  determine that the concatenation of   and  's value is a constant value  and so interns it.  "
          }
        ],
        "parent": "q_399",
        "author": "Bohemian"
      },
      {
        "date": "Dec 30, 2014 12:44:11 AM",
        "commentid": "a_478",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": 2.0,
            "sent": " all these strings are calculated in runtime  this is why they are different         this one calculated during compile time  because e is final:         if you change code to this:         all of them will produce true  "
          }
        ],
        "parent": "q_399",
        "author": "Lashane"
      },
      {
        "date": "Dec 30, 2014 12:47:19 AM",
        "commentid": "a_479",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": 2.0,
            "sent": "\"  java compiler  ( ) translates your  code in java  to  byte code  which is executed by the  jvm . it also does some optimizations for you. you can check the generated byte code using   utility with   parameter     concatenation with final string       is true because   is   here is the byte code for this snippet (last comparison only):         as you see the java compiler has merged the \\\"\"hel\\\"\" with \\\"\"lo\\\"\" and just comparing two string leterals \\\"\"hello\\\"\". java interns string literals by default - that's why it returns true     concatenation with non-final string     if you are concatenating the string literal with  non-final  string variable  the byte code will be different:         here we are comparing the result of   method which obviously returns another object - it is equal to \\\"\"hello\\\"\" by value but not by reference    you can find more details on comparing strings in this  stackoverflow question   \""
          }
        ],
        "parent": "q_399",
        "author": "bedrin"
      },
      {
        "date": "Dec 30, 2014 12:50:44 AM",
        "commentid": "a_480",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": 2.0,
            "sent": " even though you are using   method  you have to still remember that   compares by reference and not value.    so in the cases of             or     or   will have a new reference in memory that is not equal to that of  .     in the final case since the string value is final  the evaluation happens at compile time instead of runtime as optimization since it will never change. also if you are thinking when you define string literal  java internally interns them.  "
          }
        ],
        "parent": "q_399",
        "author": "Saher Ahwal"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": 2.0,
        "sent": "      this results in          the expression   is true implies same reference. so why the last expression is true but the 4th to last ie   is false?  "
      }
    ]
  },
  {
    "author": "assylias",
    "parent": "",
    "title": "\"Why can&#39;t I use generics in an inner interface?\"",
    "commentid": "q_4964",
    "date": "Dec 18, 2014 11:03:55 AM",
    "children": [
      {
        "date": "Dec 18, 2014 11:06:27 AM",
        "commentid": "a_5973",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.6680000000000001,
            "sent": " a \\\"nested\\\" interface (bar in your example) is implicitly static. so it can't access instance specific information related to foo  such as its generic type.    see for example  jls #8.5.1 :        a member interface is implicitly static    "
          }
        ],
        "parent": "q_4964",
        "author": "assylias"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.6680000000000001,
        "sent": " i tried to compile the following code:         but i get this error: \\\"foo.this cannot be referenced from a static context.\\\"    specifically  i get it on the \\\"t\\\" in bar(t t). however foo(t t) does not produce the same error. i don't understand why that's a static context and what the error really means.   "
      }
    ]
  },
  {
    "author": "Maroun Maroun",
    "parent": "",
    "title": "\"try{} finally{} construct with return values\"",
    "commentid": "q_5727",
    "date": "Dec 17, 2014 6:59:45 AM",
    "children": [
      {
        "date": "Dec 17, 2014 7:04:47 AM",
        "commentid": "a_6904",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.6680000000000001,
            "sent": " everything works exactly as expected  no bugs here. when you have doubts  the jls is your savior:      jls - 14.20.2. execution of try-finally and try-catch-finally  :        if execution of the   block completes abruptly for any other   reason r  then the   block is executed  and then there is a   choice:             if the finally block completes normally  then the try statement    completes  abruptly for reason r.       if the finally block completes abruptly for reason s  then the try    statement  completes abruptly for reason s (and reason r is    discarded).           it  overrides  the value in the   block.      inside   discards all exceptions that can be thrown in   clause.  "
          }
        ],
        "parent": "q_5727",
        "author": "Maroun Maroun"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.6680000000000001,
        "sent": " i was wondering why the following code would be accepted by the java compiler:         this can and should not work. the java specification states the   block will  always  be executed  but at the same time the return value has already been specified. so either you cannot execute the   statement  because you have exited at    which would be incorrect.    however  the other option is that you execute the   statement  and thereby totally disregarding the   statement...    i would say that both are wrong  and i would expect that this does not compile. however it compiles and runs fine. i will leave the answer as a nice exercise to the reader ;).     basically my question is:  besides being bad practice  would this be considered a java bug  or does this have other wonderful uses other than obfuscation?     edit:    the question is not so much if it is a bug  that has been answered  but does it have nice use cases?  "
      }
    ]
  },
  {
    "author": "aurelius",
    "parent": "",
    "title": "\"Synch and Asynchronous interface of MqttClient object are not working\"",
    "commentid": "q_17091",
    "date": "Nov 28, 2014 2:31:26 AM",
    "children": [
      {
        "date": "Dec 11, 2014 8:55:29 AM",
        "commentid": "a_20667",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.6240000000000001,
            "sent": " your machine on which resides the client  which process your callback  may have the outgoing port blocked by the machine's firewall.  "
          }
        ],
        "parent": "q_17091",
        "author": "aurelius"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.6240000000000001,
        "sent": " i have created a   of type   and as shown below in the code  i create a client and se its  . the problem is      1-when i run the programm  the   appears  but i receive no response fr0m the   or from o   why? wha i am doing wrong in the code.    2-i implemented the   interface  but since i have a client of type    i can not use this   interface. i tried to use   but because i program for java and not for   i can not use it. how to use   interface.?     update_1     in the below code \\\"updated_code_1\\\"  i slightly modified the code  but i expect every time i connect successfully to the   the message in   synchronous callback to be printed  and the message in   synchronous callbck to be printed in case of the onnection terminated such as when i intentionally disconnect the network. but at ru time when i connect to the    neither   nor   dispays any thing. so  what are they designed for?     *update_2_17_dec_2014     i have an inquiry that might lead us to a solution  which is  does it matter if i am connecting to the broker through wired/wire-less network? would that change the behaviour of he synchronous and asynchronous listener?     updated_1_code :          newclient :          asynch callback :       "
      }
    ]
  },
  {
    "author": "Yazan",
    "parent": "",
    "title": "\"Prevent Android phone from connecting to WiFi network unless my app approves it?\"",
    "commentid": "q_33826",
    "date": "Nov 1, 2014 12:44:38 AM",
    "children": [
      {
        "date": "Nov 6, 2014 7:45:35 AM",
        "commentid": "a_40613",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.124,
            "sent": " you can't implement a very robust system without rooting the device. here's the closest you can get  i think:      use  getconfigurednetworks()  to fetch a list of networks currently configured on the user's device   for each  wificonfiguration  in the list  set the public field  bssid  to the desired \\\"safe\\\" mac address   call  saveconfiguration()  to persist the changes      alternatively for step (2.)  you could call  disablenetwork()  for each configured network  and selectively enabled them based on the bssid. note that mac addresses can still be spoofed fairly easily.  "
          }
        ],
        "parent": "q_33826",
        "author": "Justin Powell"
      },
      {
        "date": "Nov 10, 2014 9:16:21 PM",
        "commentid": "a_40614",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.124,
            "sent": " you can listen to connectivity change of wifi and act on that events to enable disable wifi       "
          }
        ],
        "parent": "q_33826",
        "author": "Kirtan"
      },
      {
        "date": "Nov 12, 2014 12:51:09 AM",
        "commentid": "a_40615",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -1.124,
            "sent": " as you know when connecting to the wifi the sifi manager app displays a hint message under the wifi name that is connecting     like connecting  authenticating  obtaining ip ... etc    so i tried to search how can detect those stages of connecting to a wifi network i came to an answer showing how is this done   it was done using the a receiver to      and i tried to implement it adding the code to just disconnect ... and that was success as the wifi never got connected  the icon did not appear on the notification bar and the logs keep repeating the steps  though some how it say's connected (at logs) but nothing actually appears on the device itself  so maybe it got connected for like (10 ms)    anyhow   below is the code i used:         where ever you find a   thats how i interrupted the connection. what remains here  is to get the network name or mac address to allow or disallow the process to complete    permissions:         adding the broadcast receiver:         thanks  "
          }
        ],
        "parent": "q_33826",
        "author": "Yazan"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -1.124,
        "sent": " i want to develop an app that can prevent connection to a wifi network unless i approve it. i want to be able to query the mac address of the access point and compare that to a list of known addresses corresponding to ssids. the goal of the app is to protect users from accidentally connecting to malicious access points  such as the types that can be produced with  pineapple devices .    i'm not clear from my research how i would achieve this goal. questions such as  how to be notified on wifi network status change?  explain how to detect the connection  has happened   but for my use case that's already too late.    neither    nor   seem to offer methods for adding listeners that could interrupt a connection in progress.    some thoughts i've had for a solution:       install myself as a proxy and make the decision as to whether to allow data through. however  this doesn't seem to be an option based on  do android proxy settings apply to all apps on the device?  (hint: the answer is \\\"no\\\").     replace the existing wifi manager with something of my own creation. however  i've really struggled to find any information in the android developer guides regarding replacing system components. consequently  i'm not sure this is possible on non-rooted phones.      store the network passwords within my app and set the passwords in the wifi manager to nonsense values. then capture a broadcast message that warns of a failed connection (presumably something like  ) and selectively decide to reconnect back to that network. might be a possible (if ugly) solution  but can i set the password back to a nonsense value while the network is still connected  to ensure we don't quietly connect to another ssid of the same name? i'm not sure . it occurs to me that pineapple devices would probably accept any password  thus rendering this approach void.     find some way to prevent android automatically connecting to known networks (i.e. networks that have been used before or have a password stored with them). then i could manage all connections/disconnections from my app. i can't see how to do this manually on my phone  however  so i'm doubtful this is possible programmatically.       can anyone suggest an approach that would work on a non-rooted phone?  "
      }
    ]
  },
  {
    "author": "Mureinik",
    "parent": "",
    "title": "\"Java syntax - extra plus sign after cast is valid?\"",
    "commentid": "q_47296",
    "date": "Oct 9, 2014 4:34:00 AM",
    "children": [
      {
        "date": "Oct 9, 2014 4:38:05 AM",
        "commentid": "a_57265",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": "     why exactly is this happening?      if you put a unary   operator before a number or expression it negates it.    similarly  if you put a unary   operator before a number or expression it does nothing.    a safer way to convert a byte to a char is         this will work for characters between 0 and 255  rather than 0 to 127.    btw you can use unary operators to write some confusing code like       "
          }
        ],
        "parent": "q_47296",
        "author": "Peter Lawrey"
      },
      {
        "date": "Oct 9, 2014 4:37:47 AM",
        "commentid": "a_57266",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": " b is a byte  and that be expressed as + b as well. for example  3 can be written as +3 as well. so  ((char) + b) is same as ((char) b)  "
          }
        ],
        "parent": "q_47296",
        "author": "Debasish Jana"
      },
      {
        "date": "Oct 9, 2014 4:38:49 AM",
        "commentid": "a_57267",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": 0.0,
            "sent": " the   is the unary plus operator - like you can say that   is equivalent to      is equivalent to  . the space between   and   is inconsequential. this operator has a higher precedence than the cast  so after it's applied (doing nothing  as noted)  the resulting   is then cast to a   and produces the same result as before.  "
          }
        ],
        "parent": "q_47296",
        "author": "Mureinik"
      }
    ],
    "sent": [
      {
        "topicid": 2,
        "systemtopicid": 2,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Loops",
        "linePolarity": 0.0,
        "sent": " so i came across something that confused me when casting a   to    usually i would do this:         which will print out         i accidentally left a   between the   and   and got the  same  result!?    like so:          why exactly is this happening?     am i essentially doing this:    because changing the   to a    like so:   yields a different result.    note: using java version 1.8.0_20   "
      }
    ]
  },
  {
    "author": "NoDataFound",
    "parent": "",
    "title": "\"Java 8 functional interface ambiguous reference (is this a bug?)\"",
    "commentid": "q_66889",
    "date": "Sep 3, 2014 1:37:39 PM",
    "children": [
      {
        "date": "Sep 3, 2014 1:43:24 PM",
        "commentid": "a_81509",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.9319999999999999,
            "sent": "   should not fail. indeed  it does  not  fail for me. running:         from what i can tell  this is the most recent. what platform are you working on?    as it turns out  this is a bug on osx (and not on windows). if someone has a *nix box  they should also test op's code.  "
          }
        ],
        "parent": "q_66889",
        "author": "David Titarenco"
      },
      {
        "date": "Sep 4, 2014 1:37:16 AM",
        "commentid": "a_81510",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.9319999999999999,
            "sent": " i have the same problem  but i tried this:         while this fails in javac (debian wheezy + jdk1.8.0_20x64)  it works fine in eclipse.    on a side note  i guess it's one of those type inference problems. you should probably fill a bug report.  "
          }
        ],
        "parent": "q_66889",
        "author": "NoDataFound"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.9319999999999999,
        "sent": " i have two functions similar to these ones:         to me it is obvious that they cannot(!) clash and that calls cannot be ambiguous. however  with the most recent version of java 8  the following call fails:         with      while         and         work.    is this a bug in the compiler (or should this fail?)    more information about my setup below    java version:         error:         test program:            since the issue is confirmed by others  i've reported it as a bug to oracle.  "
      }
    ]
  },
  {
    "author": "DSquare",
    "parent": "",
    "title": "\"SwingWorker  done() is executed before process() calls are finished\"",
    "commentid": "q_87755",
    "date": "Jul 25, 2014 8:04:39 AM",
    "children": [
      {
        "date": "Jul 25, 2014 12:39:51 PM",
        "commentid": "a_107545",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.6680000000000001,
            "sent": "  short answer:     this happens because publish() doesn't directly schedule    it sets a timer which will fire the scheduling of a process() block in the edt after  . so when the the worker is cancelled there is still a timer waiting to schedule a process() with the data of the last publish. the reason for using a timer is to implement the optimization where a single process may be executed with the combined data of several publishes.     long answer:     let's see how publish() and cancel interact with each other  for that  let us dive into some source code.    first the easy part   :         this cancel ends up calling the following code:         the swingworker state is set to    the thread is interrupted and   is called  however this is not swingworker's done  but the   done()  which is specified when the variable is instantiated in the swingworker constructor:         and the   code is:         which calls the swingworkers's   directly if we are in the edt which is our case. at this point the swingworker should stop  no more   should be called  this is easy enough to demonstrate with the following modification:         however we still get a \\\"writing...\\\" message from process(). so let us see how is process() called. the source code for   is         we see that the   of the runnable   is who ends up calling    but this code just calls   not   and there's a   around too. let's see  .         so what   actually does is adding the chunks into some internal arraylist   and calling  . we just saw that submit just calls    which is this very same   method  since both   and   extend    however this time around   is   instead of   as in  . so a chunk is the runnable that calls  . however the   call is a completely different method defined in the class of  :         it creates a timer that fires the   code once after   miliseconds. once the event is fired the code will be enqueued in the edt which will call an internal   which ends up calling   of   and thus executing    where chunk is the flushed data of the   arraylist. i skipped some details  the chain of \\\"run\\\" calls is like this:      dosubmit.run()    dosubmit.run(flush()) //actually a loop of runnables but will only have one (*)   doprocess.run()   doprocess.run(flush())    process(chunk)      (*)the boolean   and   (which resets this boolean) make it so additional calls to publish don't add doprocess runnables to be called in  dosubmit.run(flush()) however their data is not ignored. thus executing a single process for any amount of publishes called during the life of a timer.    all in all  what   does is scheduling the call to   in the edt  after  a delay. this explains why even after we cancelled the thread and no more publishes are done  still one process execution appears  because the moment we cancel the worker there's (with high probability) a timer that will schedule a   after   is already scheduled.    why is this timer used instead of just scheduling process() in the edt with an  ? to implement the performance optimization explained in the  docs :        because the process method is invoked asynchronously on the event   dispatch thread multiple invocations to the publish method might occur   before the process method is executed. for performance purposes all   these invocations are coalesced into one invocation with concatenated   arguments.       for example:           we now know that this works because all the publishes that occur within a delay interval are adding their   into that internal variable we saw   and the   will execute with all that data in one go.     is this a bug? workaround?     it's hard to tell if this is a bug or not  it might make sense to process the data that the background thread has published  since the work is actually done and you might be interested in getting the gui updated with as much info as you can (if that's what   is doing  for example). and then it might not make sense if   requires to have all the data processed and/or a call to process() after done() creates data/gui inconsistencies.    there's an obvious workaround if you don't want any new process() to be executed after done()  simply check if the worker is cancelled in the   method too!         it's more tricky to make done() be executed after that last process()  for example done could just use also a timer that will schedule the actual done() work after >delay. although i can't think this is would be a common case since if you cancelled it shouldn't be important to miss one more process() when we know that we are in fact cancelling the execution of all the future ones.  "
          }
        ],
        "parent": "q_87755",
        "author": "DSquare"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.6680000000000001,
        "sent": " i have been working with  swingworker s for a while and have ended up with a strange behavior  at least for me. i clearly understand that due to performance reasons several invocations to  publish()  method are coallesced in one invocation. it makes perfectly sense to me and i suspect swingworker keeps some kind of queue to process all that calls.    according to  tutorial  and api  when swingworker ends its execution  either  doinbackground()  finishes normally or worker thread is cancelled from the outside  then  done()  method is invoked. so far so good.    but i have an example (similar to shown in tutorials) where there are   method  calls done   after     method is executed. since both methods execute in the  event dispatch thread  i would expect   be executed after all   invocations are finished. in other words:    expected:         result:         sample code       "
      }
    ]
  },
  {
    "author": "Christian",
    "parent": "",
    "title": "\"Is there a formatting flag that converts to a lowercase String in Java?\"",
    "commentid": "q_88263",
    "date": "Jul 24, 2014 11:31:33 AM",
    "children": [
      {
        "date": "Jul 24, 2014 11:40:27 AM",
        "commentid": "a_108240",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.6680000000000001,
            "sent": " no  there is not. but  according to  java docs :        conversions denoted by an upper-case character (i.e. 'b'  'h'  's'  'c'  'x'  'e'  'g'  'a'  and 't') are the same as those for the corresponding lower-case conversion characters except that the result is converted to upper case according to the rules of the prevailing  .  the result is equivalent to the following invocation of         in other words  the following         is equivalent to         so  if you want to get a lower case string  you can just do:         there won't be an optimization by doing the conversion using a flag.  "
          }
        ],
        "parent": "q_88263",
        "author": "Christian"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.6680000000000001,
        "sent": " there are two placeholders in java that convert to a string:        -- converts to a string as-is     -- converts to an uppercase string.      so  given:           if template =    the result will be     if template =    the result will be         question:    generally  is there a way to convert an argument to a lowercase string using only java's format conversion syntax?  (in other words  without using  .) specifically  is there any possible value for   such that the result will be  ?  "
      }
    ]
  },
  {
    "author": "La-comadreja",
    "parent": "",
    "title": "\"Static char and int array difference\"",
    "commentid": "q_91567",
    "date": "Jul 18, 2014 1:48:16 PM",
    "children": [
      {
        "date": "Jul 18, 2014 1:54:06 PM",
        "commentid": "a_112245",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.0,
            "sent": " the answer exists in the   source code (of which   is an instance).    start with the fact that the uninitialized arrays  as reference variables  are given the default of  .    the   (eventually) attempts to call   on the passed in array.  it's null  resulting in the  .    (eventually) calls  :         there is no overload of   matching    but there is a  .  there it (eventually) attempts    passing the   reference  so   takes the   and returns the    .   calls  :       "
          }
        ],
        "parent": "q_91567",
        "author": "rgettman"
      },
      {
        "date": "Jul 18, 2014 1:55:34 PM",
        "commentid": "a_112246",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.0,
            "sent": "   is an instance of   and this class has a few overloaded   methods. in your case:        is using        is using   (there is no   method so the closest available type for   which   can use is  ).      the second method is using          to get a string representation of the object we want to print and the code of the   method looks like         so it is null-safe (will return the string   if the reference holds  ).    the first method is at some level using          and because   is     will throw nullpointerexception because   doesn't have   (or any other) field.  "
          }
        ],
        "parent": "q_91567",
        "author": "Pshemo"
      },
      {
        "date": "Jul 18, 2014 2:00:30 PM",
        "commentid": "a_112247",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.0,
            "sent": " these are two different methods  and their apis describe the behavior fully.      calls string.valueof at first  which returns \\\"null\\\" if x is null.    see:       http://docs.oracle.com/javase/8/docs/api/java/lang/string.html#valueof-java.lang.object-     http://docs.oracle.com/javase/8/docs/api/java/io/printstream.html#println-java.lang.object-         calls the print(char[] c) method which throws nullpointerexception if c is null.    see:       http://docs.oracle.com/javase/8/docs/api/java/io/printstream.html#println-char:a-     http://docs.oracle.com/javase/8/docs/api/java/io/printstream.html#print-char:a-     "
          }
        ],
        "parent": "q_91567",
        "author": "NESPowerGlove"
      },
      {
        "date": "Jul 18, 2014 2:07:21 PM",
        "commentid": "a_112248",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.0,
            "sent": " you are actually calling two separate  overloaded   methods.    is overloaded as per the documentation.  no specific overloading of   exists for the   arrays.    so  when   is called on an integer array    is called.  according to the javadocs:        this method calls at first string.valueof(x) to get the printed   object's string value  then behaves as though it invokes print(string)   and then println().      since the value of the string is    the method prints null.    the   method works somewhat differently when it takes a character array as a parameter.  superficially  the method itself seems similar:        prints an array of characters and then terminate the line. this method   behaves as though it invokes print(char[]) and then println().      however  the   method behaves quite differently in key ways:        prints an array of characters. the characters are converted into bytes   according to the platform's default character encoding  and these   bytes are written in exactly the manner of the write(int) method.      thus  it calls a different method.  the documentation for   explicitly states that in your situation  your exception is thrown:         throws: nullpointerexception - if [input parameter] s is null       hence  the cause of the difference in behavior.    see javadocs for more information about the   and   methods:  http://docs.oracle.com/javase/7/docs/api/java/io/printstream.html#println()   "
          }
        ],
        "parent": "q_91567",
        "author": "La-comadreja"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": 0.0,
        "sent": " when i try to print the uninitialized  static char array  it gives run time error (null pointer exception) whereas the uninitialized  static int array  gives null value. why?       "
      }
    ]
  },
  {
    "author": "amalloy",
    "parent": "",
    "title": "\"Very confused by Java 8 Comparator type inference\"",
    "commentid": "q_103711",
    "date": "Jun 26, 2014 10:36:57 AM",
    "children": [
      {
        "date": "Jun 26, 2014 7:03:49 PM",
        "commentid": "a_127446",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.508,
            "sent": " first  all the examples you say cause errors compile fine with the reference implementation (javac from jdk 8.)  they also work fine in intellij  so its quite possible the errors you're seeing are eclipse-specific.      your underlying question seems to be: \\\"why does it stop working when i start chaining.\\\"  the reason is  while lambda expressions and generic method invocations are  poly expressions  (their type is context-sensitive) when they appear as method parameters  when they appear instead as method receiver expressions  they are not.      when you say         there is enough type information to solve for both the type argument of   and the argument type  .  the   call gets its target type from the signature of    so it is known   must return a    and therefore   must be  .      but when you start chaining:         now we've got a problem.  we know that the compound expression   has a target type of    but because the receiver expression for the chain ( ) is a generic method call  and we can't infer its type parameters from its other arguments  we're kind of out of luck.  since we don't know the type of this expression  we don't know that it has a   method  etc.      there are several ways to fix this  all of which involve injecting more type information so that the initial object in the chain can be properly typed.  in rough order of decreasing desirability and increasing intrusiveness:      use an exact method reference (one with no overloads)  like  .  this then gives enough type information to infer the type variables for the   call  and therefore give it a type  and therefore continue down the chain.   use an explicit lambda (as you did in your example)   provide a type witness for the   call:  .     provide an explicit target type with a cast  casting the receiver expression to  .      "
          }
        ],
        "parent": "q_103711",
        "author": "Brian Goetz"
      },
      {
        "date": "Jun 26, 2014 10:59:35 AM",
        "commentid": "a_127447",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.508,
            "sent": " the problem is type inferencing.  without adding a   to the first comparison    doesn't know the type of the input so it defaults to object.    you can fix this problem 1 of 3 ways:       use the new java 8 method reference syntax          pull out each comparison step into a local reference          edit      forcing the type returned by the comparator (note you need both the input type and the comparison key type)            i think the \\\"last\\\"   syntax error is misleading you.  it's actually a type problem with the whole chain  it's just the compiler only marking the end of the chain as a syntax error because that's when the final return type doesn't match i guess.    i'm not sure why   is doing a better inferencing job than   since it should do the same capture type but apparently not.  "
          }
        ],
        "parent": "q_103711",
        "author": "dkatzel"
      },
      {
        "date": "Jun 26, 2014 11:17:03 AM",
        "commentid": "a_127448",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.508,
            "sent": "   creates a bound of song for the type variable e  from the declaration of playlist1  which \\\"ripples\\\" to the comparator.     in    there is no such bound  and the inference from the type of the first comparator is not enough for the compiler to infer the rest.     i think you would get \\\"correct\\\" behavior from    but don't have a java 8 install to test it out for you.  "
          }
        ],
        "parent": "q_103711",
        "author": "amalloy"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.508,
        "sent": " i've been looking at the difference between   and    specifically regarding using the   static methods and whether param types are required in the lambda expressions. before we start  i know i could use method references  e.g.   to overcome my problems  but my query here is not so much something i want to fix but something i want an answer to  i.e. why is the java compiler handling it in this way.     these are my finding. suppose we have an   of type    with some songs added  there are 3 standard get methods:         here is a call to both types of sort method that works  no problem:         as soon as i start to chain    the following happens:         i.e. syntax errors because it does not know the type of   anymore. so to fix this i add the type   to the first parameter (of comparing):         now here comes the confusing part. for p   i.e. the list  this solve all compilation errors  for both the following   calls. however  for    it solves it for the first one  but not the last one. i tested added several extra calls to   and it always shows an error for the last one  unless i put   for the parameter.    now i went on to test this further with creating a   and with using  :         the same thing happens as in  for the    there are no compilation errors but for   the last call to   shows an error.    can anyone please explain why this is happening and also why there is no need to use   at all when simply calling the comparing method (without further   calls).    one other query on the same topic is when i do this to the  :         i.e. remove the type   from the first lambda parameter for the comparing method call  it shows syntax errors under the call to comparing and the first call to   but not to the final call to   - almost the opposite of what was happening above! whereas  for all the other 3 examples i.e. with      and   when i remove that first   param type it shows syntax errors for all the calls.    many thanks in advance.    edited to include screenshot of errors i was receiving in eclipse kepler sr2  which i have now since found are eclipse specific because when compiled using the jdk8 java compiler on the command-line it compiles ok.        "
      }
    ]
  },
  {
    "author": "Konstantinos Chalkias",
    "parent": "",
    "title": "\"Does Java reordering affect System.currentTimeMillis()?\"",
    "commentid": "q_2148",
    "date": "Dec 24, 2014 8:42:15 PM",
    "children": [
      {
        "date": "Dec 24, 2014 9:01:35 PM",
        "commentid": "a_2559",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": 0.0,
            "sent": " please see this question  instruction reordering &amp; happens-before relationship in java .    i believe that unless you are in a different thread  the outcome of any execution will always be consistent with the order in your code. in this situation  since it is impossible to process it out of order  it should be good even if your fields are visible to another thread.  "
          }
        ],
        "parent": "q_2148",
        "author": "Joseph K. Strauss"
      },
      {
        "date": "Dec 25, 2014 1:50:41 AM",
        "commentid": "a_2560",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": 0.0,
            "sent": " due to being a user system call  compilers shouldn't reorder them in the same thread. if this was not true  we could even experience reordering effects in system.out.println(independent values); i guess that access to the system's/os's clock creates a sort of relationship between these operations (always for the current thread)  so theoretically there is some kind of dependency between them. probably  jvm considers this issue and never reorders user system calls.   "
          }
        ],
        "parent": "q_2148",
        "author": "Konstantinos Chalkias"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": 0.0,
        "sent": " according to java memory model  instructions can be reordered as long as the execution is  well-formed .    so i wonder  is it possible that the following codes produces the following output?     [codes][in a same thread]           [output]          if not possible  then what does jvm / implementations do to prevent this from happening?  "
      }
    ]
  },
  {
    "author": "m3th0dman",
    "parent": "",
    "title": "\"Most efficient way to get the last element of a stream\"",
    "commentid": "q_5149",
    "date": "Dec 18, 2014 6:17:25 AM",
    "children": [
      {
        "date": "Dec 18, 2014 6:18:02 AM",
        "commentid": "a_6203",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": 0.6680000000000001,
            "sent": " use the   method and keep only the current value:       "
          }
        ],
        "parent": "q_5149",
        "author": "Bohemian"
      },
      {
        "date": "Dec 18, 2014 6:57:53 AM",
        "commentid": "a_6204",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": 0.6680000000000001,
            "sent": " this heavily depends on the nature of the  . keep in mind that ?simple? doesn?t necessarily mean ?efficient?. if you suspect the stream to be very large  carrying heavy operations or having a source which knows the size in advance  the following might be substantially more efficient than the simple solution:         you may illustrate the difference with the following example:         it will print:         in other words  it did not perform the operation on the first 9999999 elements but only on the last one.  "
          }
        ],
        "parent": "q_5149",
        "author": "Holger"
      },
      {
        "date": "Dec 22, 2014 10:28:13 PM",
        "commentid": "a_6205",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": 0.6680000000000001,
            "sent": " this is just a refactoring of  holger 's answer because the code  while fantastic  is a bit hard to read/understand  especially for people who were not c programmers before java. hopefully my refactored example class is a little easier to follow for those who are not familiar with spliterators  what they do  or how they work.       "
          }
        ],
        "parent": "q_5149",
        "author": "Steve K"
      },
      {
        "date": "Dec 18, 2014 8:54:08 AM",
        "commentid": "a_6206",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": 0.6680000000000001,
            "sent": " here is another solution (not that efficient):       "
          }
        ],
        "parent": "q_5149",
        "author": "demostene"
      },
      {
        "date": "Dec 18, 2014 1:47:02 PM",
        "commentid": "a_6207",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": 0.6680000000000001,
            "sent": " not as elegant  but probably best performing:       "
          }
        ],
        "parent": "q_5149",
        "author": "Bohemian"
      },
      {
        "date": "Dec 20, 2014 4:54:20 PM",
        "commentid": "a_6208",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": 0.6680000000000001,
            "sent": "   don't necessarily have an order  meaning that the last element might not even be defined; the order is given by their source. for example if the stream is created from a    then there's no order. moreover  even if a   is ordered  it doesn't assure that all the operations acted upon it will be applied on the stream's element in that order.    actually  parallel streams perform better if they are not ordered and   provides  a method  returning the same stream but unordered.     http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html#ordering     since   does not provide a method   the only way last element makes sense is to   the stream into an ordered  :         this is also the most efficient in terms of cpu consumption  because it's o(1) instead of o(n). on the other hand it takes more memory. i obtained the following results:         edit: if you look at how   is implemented you see it instantiates   which besides other parameters it takes a   that takes 2   and combines them adding all the elements from one to another  . in order to improve memory consumption and not allocate the whole   one might provide a different   meaning the collected stream would return a list with just one element  the last one.         there are two problems with this though:   is not   and i have not tested to see if it's functionally correct.  "
          }
        ],
        "parent": "q_5149",
        "author": "m3th0dman"
      }
    ],
    "sent": [
      {
        "topicid": 6,
        "systemtopicid": 6,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "JSON",
        "linePolarity": 0.6680000000000001,
        "sent": " stream doesn't have a   method:         so what's the most elegant and or efficient way to get the last element or null for an empty stream?  "
      }
    ]
  },
  {
    "author": "Marcin Szymczak",
    "parent": "",
    "title": "\"static method with static default code?\"",
    "commentid": "q_7156",
    "date": "Dec 14, 2014 10:48:08 PM",
    "children": [
      {
        "date": "Dec 14, 2014 10:50:07 PM",
        "commentid": "a_8786",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -1.0,
            "sent": " this bit declares a static field:         so it's accessible directly on the class  e.g.   (or just  )  but only from code within the class  because it's private.    this bit is a static initializer:         that runs when the class is loaded  before any instances are created  and does indeed add some entries to  . if there are multiple static initializers  they're run in source code order. see  static initializer blocks  in  this tutorial .       fwiw  there are also per-instance versions of both of those. an instance field:         ...and an instance initializer; they look a bit weird  because they're just blocks with nothing in front of the block:         in context  and with a second instance member:         so you can do the same sort of thing for instances.  "
          }
        ],
        "parent": "q_7156",
        "author": "T.J. Crowder"
      },
      {
        "date": "Dec 14, 2014 10:49:48 PM",
        "commentid": "a_8787",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -1.0,
            "sent": " basically yes  it's formally called a static initializer.  and per  jls-8.7 static initializers          a static initializer declared in a class is executed when the class is initialized ( ?12.4.2 ). together with any field initializers for class variables ( ?8.3.2 )  static initializers may be used to initialize the class variables of the class.    "
          }
        ],
        "parent": "q_7156",
        "author": "Elliott Frisch"
      },
      {
        "date": "Dec 14, 2014 10:51:16 PM",
        "commentid": "a_8788",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -1.0,
            "sent": " those are called  static initialization blocks .        a static initialization block is a normal block of code enclosed in braces  { }  and preceded by the static keyword. here is an example:             a class can have any number of static initialization blocks  and they   can appear anywhere in the class body. the runtime system guarantees   that static initialization blocks are called in the order that they   appear in the source code.        there is an alternative to static blocks ? you can write a private   static method:             the advantage of private static methods is that they can be reused   later if you need to reinitialize the class variable.      they are called whenever the class is first initialized  and can be used to conveniently initialize fields.  "
          }
        ],
        "parent": "q_7156",
        "author": "Pokechu22"
      },
      {
        "date": "Dec 14, 2014 10:56:31 PM",
        "commentid": "a_8789",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -1.0,
            "sent": " the static variable   which is declared as a   is populated using a static initializer. there are two entries that are being added to the  .             one effect of the         static initializer      is that it runs when the class is loaded and that the block is thread safe and therefore it is used in  design patterns such as   etc. to alleviate the need for thread synchronization.         please see this thread for details on that:      are java static initializers thread safe?   "
          }
        ],
        "parent": "q_7156",
        "author": "Khanna111"
      },
      {
        "date": "Dec 14, 2014 11:00:38 PM",
        "commentid": "a_8790",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -1.0,
            "sent": " think of it the same way you might think of an object's member fields.  you might have something like this:         this would initialize the member field each time a   is declared to some \\\"initial\\\" state.  the following:         ...essentially does the same thing  except does it for  static  fields.  now when the class is loaded  this static block is called and initializes the static fields before they can be used so that they have some \\\"initial\\\" state.  this is often used when you have something that will not change (for instance you may want to set up a map that makes lookup of certain things easy) or when you need to set something up that might throw an exception yet the thing you are trying to set up really should be static  such as:         in the above case  i only need a single   for  all  of my classes (assuming i'm always going to use rsa encryption with a key size of 2048).  with this single   i can generate  multiple  public private key pairs.  however  creating the   throws a   exception (which cannot happen since java is required to support rsa with a key length of 2048).  so i can still get my static   by using this static initializer.  "
          }
        ],
        "parent": "q_7156",
        "author": "Jared"
      },
      {
        "date": "Dec 19, 2014 3:52:52 AM",
        "commentid": "a_8791",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -1.0,
            "sent": " it is used to create variable and initialize it during class loading (that's when static{} block executes).    as java language specification states:        static initializers (?8.7) are blocks of executable code that may be   used to help initialize a class.      and it is indeed commonly used in small tutorial programs to initialize values.    on the other hand they also have usage in \\\"bigger\\\" programs. you can also use it register classes in factory object. in this case each concrete product registers itself in the factory. by doing this you don't have to modify factory class when new concrete product is added.       "
          }
        ],
        "parent": "q_7156",
        "author": "Marcin Szymczak"
      }
    ],
    "sent": [
      {
        "topicid": 10,
        "systemtopicid": 10,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Network",
        "linePolarity": -1.0,
        "sent": " sorry for the bad title  but i found the code listed below on  mkyoung.com  and was wondering what this code does. is this a way in java to set some default value into a variable?       "
      }
    ]
  },
  {
    "author": "David Conrad",
    "parent": "",
    "title": "\"How to convert big hex number (with decimal point) to BigDecimal\"",
    "commentid": "q_15365",
    "date": "Dec 1, 2014 10:55:34 AM",
    "children": [
      {
        "date": "Dec 1, 2014 12:27:46 PM",
        "commentid": "a_18679",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " i'm surprised that   doesn't already support this. i wrote the following  which i think should work. it passes my initial tests (with             and your    and also some negative exponents  from   down to  )  but it should be tested more thoroughly before being used in production code.         i've  released this on github  under the mit license. there are unit tests  but only a pretty minimal set.    if you find any cases for which this returns an incorrect value  please let me know.  "
          }
        ],
        "parent": "q_15365",
        "author": "David Conrad"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": 0.6680000000000001,
        "sent": " i have a big number in hex format (with decimal point) in a   and i want to convert it to a       example value:      in   class  i don't see any function which takes string representing number in hex format as input and returns corresponding      is there a way to convert a big hex number from   to    "
      }
    ]
  },
  {
    "author": "Christian Strempfer",
    "parent": "",
    "title": "\"Cannot convert from List&lt;List&gt; to List&lt;List&lt;?&gt;&gt;\"",
    "commentid": "q_31170",
    "date": "Nov 5, 2014 1:41:49 PM",
    "children": [
      {
        "date": "Nov 5, 2014 3:30:04 PM",
        "commentid": "a_37469",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.0,
            "sent": "      first let's sort out why these are actually unrelated assignments. that is to say  they are defined by different rules.    &#35;1 is called an   unchecked conversion  :        there is an  unchecked conversion  from the raw class or interface type ( ?4.8 )   to any parameterized type of the form  .      specifically it is a special case of an   assignment context   just for this scenario:        if  after [other possible conversions] have been applied  the resulting type is a raw type ( ?4.8 )  an unchecked conversion ( ?5.1.9 ) may then be applied.       &#35;2 is a reference type conversion; however the problem with it is that it is not a   widening conversion   (which is the kind of reference conversion that would be implicitly allowed without a cast).    why is that? well  this is specifically governed by the  rules of generic subtyping  and more specifically this bullet point:        given a generic type declaration   ( n  > 0)  the  direct supertypes  of the parameterized type    where   (1 ?  i  ?  n ) is a type  are all of the following:               where   contains   (1 ?  i  ?  n ) ( ?4.5.1 ).          this refers us to something the jls calls   containment    where to be a valid assignment  the arguments of the left-hand side must 'contain' the arguments of the right-hand side. containment largely governs the subtyping of generic arguments since  as you may know   java generics are invariant .    so the question becomes  \\\"does a list&lt;?&gt; contain a raw list\\\" ? and the answer is: no  raw types simply are not included in the rules listed by the jls.      is not a subtype of   and unchecked conversion does not apply to the type arguments. therefore the assignment is invalid. similarly  you cannot perform a direct   narrowing conversion   cast because   is not a supertype of   either.       to make the assignment you can still apply a cast. there are two ways to do it that seem reasonable to me.         (substitute   for the inner   with your use-case.)    also  umm  i should admit i don't fully understand why the 'slightly safer' cast statement compiles. conceptually  it works because the bounded wildcard   allows the relationship of the type arguments to be considered for subtyping. however  i cannot find the spec to exactly support this when a raw type is involved.    your use-case for this cast should be safe because   is a more restrictive type than  .  "
          }
        ],
        "parent": "q_31170",
        "author": "Radiodef"
      },
      {
        "date": "Nov 5, 2014 1:50:20 PM",
        "commentid": "a_37470",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.0,
            "sent": " if you want only to remove warnings you can use @suppresswarnings(\\\"rawtypes\\\").    basically the problem is that the compiler treats the rawtype as a primitive object previous generics so... an \\\"old object\\\" is not a \\\"generic object\\\" so... you can't cast them.    read this from official doc:  http://docs.oracle.com/javase/tutorial/java/generics/rawtypes.html         but if you assign a raw type to a parameterized type  you get a   warning:        box rawbox = new box();           // rawbox is a raw type of box   box intbox = rawbox;     // warning: unchecked conversion you   also get a warning if you use a raw type to invoke generic methods   defined in the corresponding generic type:        box stringbox = new box&lt;>(); box rawbox = stringbox;   rawbox.set(8);  // warning: unchecked invocation to set(t) the warning   shows that raw types bypass generic type checks  deferring the catch   of unsafe code to runtime. therefore  you should avoid using raw   types.    "
          }
        ],
        "parent": "q_31170",
        "author": "Carlos Verdes"
      },
      {
        "date": "Nov 5, 2014 1:52:18 PM",
        "commentid": "a_37471",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.0,
            "sent": "  you cannot assign or cast it directly   because raw type    isn't the same  as  .    when using   type checking is ignored and you may use any generic method with any type. when using   the  compiler won't let you use methods with generic parameters .       therefore you could either ignore the warnings:         and/or cast it explicitly with a workaround:       "
          }
        ],
        "parent": "q_31170",
        "author": "Christian Strempfer"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.0,
        "sent": " a raw list converts to   just fine. why can't a list of raw lists convert to a list of  ?          (backstory  to mitigate  the xy problem :  an api i'm using returns  . i happen to know that it is always  . i plan to loop and build my own    but i was trying to fix (but not suppress) the raw type compiler warning when i write  . i tried:         but these give the type mismatch error.    interestingly  this gives no warning or error:          )   "
      }
    ]
  },
  {
    "author": "Pier-Alexandre Bouchard",
    "parent": "",
    "title": "\"How does this static code work?\"",
    "commentid": "q_33538",
    "date": "Nov 1, 2014 10:34:31 PM",
    "children": [
      {
        "date": "Nov 1, 2014 11:06:25 PM",
        "commentid": "a_40270",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": 0.0,
            "sent": " first     literals composed of the same characters resolve to the same instance.  so in         both variables are referring to the same object.    second     initializer blocks are executed when a class is first loaded (and initialized) . this occurs before any class methods are invoked  ie. before  .      third  your java version's implementation of    presumably  uses a   field to store the string of characters. this field is named  .      you're  using reflection to retrieve  this   for the   object referenced by the   literal  .         and assigning it to the   field of the   object referenced by the   literal           now  when your   method executes          the   literal   is referencing the same object for which you set the   field previously. this   contains the characters             and   since it was taken from the   object referenced by the literal  .    these characters are copied into a  new    object created here       "
          }
        ],
        "parent": "q_33538",
        "author": "Sotirios Delimanolis"
      },
      {
        "date": "Nov 1, 2014 10:58:15 PM",
        "commentid": "a_40271",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": 0.0,
            "sent": " the first thing that happens is your   block executes. by reflection  it changes the value of   to   (actually it changes the     to   but that has the same effect). however  you get         because the compiler has already replaced the value with  . i ran   and got         so that is exactly what has happened. as i noted in the comments  if you change   to         the last line changes to         for the same reason.  "
          }
        ],
        "parent": "q_33538",
        "author": "Elliott Frisch"
      },
      {
        "date": "Nov 1, 2014 10:51:23 PM",
        "commentid": "a_40272",
        "sent": [
          {
            "topicid": 3,
            "systemtopicid": 3,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Servlets",
            "linePolarity": 0.0,
            "sent": " take a look here:      http://docs.oracle.com/javase/7/docs/api/java/lang/reflect/field.html      public void set(object obj  object value)         sets the field represented by this field object on the specified   object argument to the specified new value. the new value is   automatically unwrapped if the underlying field has a primitive type.       public get(object obj)         returns the value of the field represented by this field  on the   specified object.      your static block is executed in first.    all occurrences of string containing \\\"hello\\\" are replaced by the string \\\"howdy\\\" by reflection before the   execution.    \\\"hello\\\" and \\\"howdy\\\" are referring to the same object. this is why   outputs true   would also output  .    take a look to the exact execution process:    "
          }
        ],
        "parent": "q_33538",
        "author": "Pier-Alexandre Bouchard"
      }
    ],
    "sent": [
      {
        "topicid": 3,
        "systemtopicid": 3,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Servlets",
        "linePolarity": 0.0,
        "sent": " code:         result:         how does this code change \\\"hello\\\" to \\\"howdy\\\" when printing?  "
      }
    ]
  },
  {
    "author": "Chriss",
    "parent": "",
    "title": "\"Concurrent ArrayList\"",
    "commentid": "q_42583",
    "date": "Oct 17, 2014 4:26:14 AM",
    "children": [
      {
        "date": "Oct 18, 2014 1:12:36 PM",
        "commentid": "a_51443",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.8879999999999999,
            "sent": " in your case you can use a  readwritelock  to access a backed list  this allows multiple threads to read your list. only if one thread needs write access all reader-thread must wait for the operation to complete. the javadoc make's it clear:        a readwritelock maintains a pair of associated locks  one for   read-only operations and one for writing. the read lock may be held   simultaneously by multiple reader threads  so long as there are no   writers. the write lock is exclusive.      here is a sample:       "
          }
        ],
        "parent": "q_42583",
        "author": "Chriss"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.8879999999999999,
        "sent": " i need an arraylist-like structure allowing just the following operations                         because of the iterator being used in many places  using   would be too error-prone. the list can grow to a few thousand elements and gets used a lot  so i'm pretty sure  that   will be too slow. i'll start with it to avoid premature optimizations  but i'd bet it won't work well.    most accesses will be single-threaded reads. so i'm asking what's the proper data structure for this.       i though that wrapping the   in something providing a synchronized iterator would do  but it won't because of the  . concenrning concurrent behavior  i obviously need that all changes will be visible by subsequent reads and iterators.     the iterator doesn't have to show a consistent snapshot  it may or may not see the updates via   as this operation gets used only to replace an item with its updated version (containing some added information  which is irrelevant for the user of the iterator). the items are fully immutable.       i clearly stated why   would not do.   is out of question as it lacks an indexed access. i need just a couple of operations rather than a fully fledged  . so unless  any java concurrent list-related  question is a duplicate of  this question   this one is not.  "
      }
    ]
  },
  {
    "author": "Mondkin",
    "parent": "",
    "title": "\"How can I check if an object(s) are in front of the camera?\"",
    "commentid": "q_48523",
    "date": "Oct 7, 2014 7:20:06 AM",
    "children": [
      {
        "date": "Oct 7, 2014 2:52:09 PM",
        "commentid": "a_58841",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.10000000000000009,
            "sent": " could you write it like this         as you can see there is far less calculation being performed for each tree.  "
          }
        ],
        "parent": "q_48523",
        "author": "Peter Lawrey"
      },
      {
        "date": "Nov 21, 2014 12:41:47 AM",
        "commentid": "a_58842",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.10000000000000009,
            "sent": " your camera rotations yaw around y  implying y is your up vector. however    gives   for your up vector. there is an inconsistency. i'd start by swapping   and   here  then print everything out every frame so you can see what happens as you rotate the camera. also render just one tree and print  . e.g. you might quickly notice the numbers approach 1.0 only when you look at 90 degrees left of the tree which narrows down the problem. as @dwilches notes  swapping cos/sin will change the phase of the rotation  which would produce such an effect.    you might consider limiting the dot product to the camera's field of view. there are still problems in that trees are not just points. a better way would be to test tree bounding boxes against the camera frustum  as @glampert suggests.    still  the tree geometry doesn't look that complex. optimization wise  i'd start trying to draw them faster. are you using vbos? perhaps look at methods to reduce draw calls such as instancing. perhaps even use a few models for lod or billboards. going even further  billboards with multiple trees on them. occlusion culling methods could be used to ignore trees behind mountains.    [ edit ]  since your trees are all roughly on a plane  you could limit the problem to the camera's yaw:       "
          }
        ],
        "parent": "q_48523",
        "author": "jozxyqk"
      },
      {
        "date": "Nov 20, 2014 10:01:08 PM",
        "commentid": "a_58843",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": 0.10000000000000009,
            "sent": " for what i see  here  the correct formulas are:         could you try them ?  "
          }
        ],
        "parent": "q_48523",
        "author": "Mondkin"
      }
    ],
    "sent": [
      {
        "topicid": 10,
        "systemtopicid": 10,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Network",
        "linePolarity": 0.10000000000000009,
        "sent": " i have got some trees  which are greatly lagging the game  so i would like to check if the trees are in front of the camera or not.      i have had some help from the  mathematics forum   and also had a look at  this link  to help me convert pitch/yaw to the directional vector needed.    but for some reason  whenever i move the camera to the left  the trees become visible  wheras whenever i move it to the right  they become unvisible (so if camera is pointing at +1 on the z axis  it seems to be rendering the trees  but -1 on the z axis and it seems to not render them).   (see  http://i.gyazo.com/cdd05dc3f5dbdc07577c6e41fab3a549  for a less-jumpy .mp4)    i am using the following code to check if an object is in front of the camera or not:         is anyone able to tell me what i have done wrong here? i can't work out if it's the math.. or the code.. or what?    camera translation code:         update:    it appears to be where the camera is looking. for example  if i look to -z  nothing happens  but if i look to +z  they all render. the   appears to somehow being +z rather than +thecamerarotation.   "
      }
    ]
  },
  {
    "author": "Steve K",
    "parent": "",
    "title": "\"Ambiguous overload in Java8 - is ejc or javac right?\"",
    "commentid": "q_48989",
    "date": "Oct 6, 2014 11:30:20 AM",
    "children": [
      {
        "date": "Oct 8, 2014 9:02:30 PM",
        "commentid": "a_59432",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": -0.44399999999999995,
            "sent": " in the invocation  when the constructor is called with t set explicitly  there is no ambiguity:         because t is defined at the time of the constructor call  object passes the ? extends object check at compile time just fine and there is no problem. when t is set explicitly to object  the choices for the two constructors become:         and in this case  it's very easy for the compiler to choose the first one. in the other example (using the diamond operator) t is not explicitly set  so the compiler first attempts to determine t by checking the type of the actual parameter  which the first option didn't need to do.    if the second constructor was changed to properly reflect what i imagine is the desired operation (that since overloadtest is a hashset of lists of t  then passing in a hashset of lists of t should be possible) like so:         ...then the ambiguity is resolved. but as it currently stands there will be the conflict when you ask the compiler to resolve that ambiguous invocation.    the compiler will see the diamond operator and will attempt to resolve t based on what was passed in and what the various constructors expect. but the way that the hashset constructor is written will ensure that no matter which class is passed in  both constructors will remain valid  because after erasure  t is always replaced with object. and when t is object  the hashset constructor and the overloadtest constructor have similar erasures because overloadtest is a valid instance of hashset. and because the one constructor doesn't override the other (because overloadtest&lt;t&gt; does not extend hashset&lt;t&gt;)  it can't actually be said that one is more specific than the other  so it won't know how to make a choice  and will instead throw a compile error.    this only occurs because by using t as a boundary you are enforcing the compiler to do type-checking. if you simply made it &lt;?&gt; instead of &lt;? extends t&gt; it would compile just fine. the java 8 compiler is stricter about types and erasure than java 7 was  partially because many of the new features in java 8 (like interface defender methods) required them to be a little bit more pedantic about generics. java 7 was not correctly reporting these things.  "
          }
        ],
        "parent": "q_48989",
        "author": "Steve K"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": -0.44399999999999995,
        "sent": " i have the following class:         this compiles fine under jdk 7's javac  as well as eclipse (with compliance set to 1.7 or 1.8). however  attempting to compile under jdk 8's javac  i get the following error:         note that this error applies only to the constructor invocation in the   method  not the one in the   method. the only difference is that   is relying on the diamond operator.    my question is this: is javac under jdk 8 properly flagging an ambiguous resolution  or was javac under jdk 7 failing to catch an ambiguity? depending on the answer  i need to either file a jdk bug  or an ecj bug.  "
      }
    ]
  },
  {
    "author": "ratchet freak",
    "parent": "",
    "title": "\"Passing mutable data between threads\"",
    "commentid": "q_75128",
    "date": "Aug 19, 2014 12:15:44 AM",
    "children": [
      {
        "date": "Aug 19, 2014 12:21:41 AM",
        "commentid": "a_91681",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.45999999999999996,
            "sent": " if you can guarantee that the threads don't modify the tree at the same time (i.e. by atomically passing over the only reference to the tree)  it is fine from a thread-safety point of view.    data visibility / consistency is another concern  though. unless all fields in the tree are (recursively) declared    changes made by one thread may not become visible to the other thread. to avoid  make sure a monitor (which acts as a memory barrier and ensures that all writes becomes visible) is acquired when the threads exchange ownership of the tree.  "
          }
        ],
        "parent": "q_75128",
        "author": "Alexander Gessler"
      },
      {
        "date": "Aug 19, 2014 12:20:07 AM",
        "commentid": "a_91682",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.45999999999999996,
            "sent": " yes  what you are describing would work just fine  as long as you take specific steps to avoid memory consistency errors  when you're passing the object between threads. using locking is one way to achieve this  but there are other -- less expensive -- ways.    the  tutorial  is a good starting point.    basically  you need to ensure that when thread a is passing the object to thread b  all changes by a  happen-before  b accesses the object.    there's more in the  jls   but it's rather technical.  "
          }
        ],
        "parent": "q_75128",
        "author": "NPE"
      },
      {
        "date": "Aug 19, 2014 12:20:40 AM",
        "commentid": "a_91683",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.45999999999999996,
            "sent": " this is  not  thread-safe without a further synchronization guarantee.    basically  one cannot be sure of  consistent visibility  between threads without proper synchronization - such as    or other happens-before guarantees of the jls. that is  even though there may be no \\\"concurrent modifications\\\"  there is  no  guarantee that the non-writer thread sees the modifications to said object.  "
          }
        ],
        "parent": "q_75128",
        "author": "user2864740"
      },
      {
        "date": "Aug 19, 2014 12:23:40 AM",
        "commentid": "a_91684",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.45999999999999996,
            "sent": " yes  but this still begs the question of how you will transfer ownership of the data between threads. the answer is that this is typically done with locks; holding the lock represents that the given thread is currently the one in charge that is allowed to mutate the object.    it's also imporant to note that thread safety is a concern whenever there are one or more readers and at least one writer. just because both objects are not modifying the object at the same time  it will still be an issue if one thread is reading from the structure at the same time that it is being mutated.    so  to get to the point: keep it simple and just use a lock.  "
          }
        ],
        "parent": "q_75128",
        "author": "Michael Aaron Safyan"
      },
      {
        "date": "Aug 19, 2014 12:21:20 AM",
        "commentid": "a_91685",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.45999999999999996,
            "sent": " in general it isn't  but in many specific case it could turn out to work (just don't rely on those).     locking does two things.       it prevents two threads from accessing the same data at the same time.   when a lock is taken all the caches that the program/system has are flushed. this is also why the keyword in java is called  .      if in some way you can make a mechanism that does it's own locking (that is really what you are saying) you still need to make sure that all the data is synchronized across threads. locking isn't the only way to do this  but its is an added feature of it.  "
          }
        ],
        "parent": "q_75128",
        "author": "Thirler"
      },
      {
        "date": "Aug 19, 2014 12:47:40 AM",
        "commentid": "a_91686",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.45999999999999996,
            "sent": " i assume that you have create custom data structure.    try to create your datastructure class    like this  use carefully with   block where needed as shown in example.    if you need to share same data over all thread then create   methods and   variables  so even though by mistake you create multiple instance of datastructure class it is thread safe  "
          }
        ],
        "parent": "q_75128",
        "author": "U2Answer"
      },
      {
        "date": "Aug 20, 2014 1:54:59 AM",
        "commentid": "a_91687",
        "sent": [
          {
            "topicid": 4,
            "systemtopicid": 4,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "MultiThreading",
            "linePolarity": -0.45999999999999996,
            "sent": " this can be done by a simple volatile helper variable:         the semantics of volatile reads and writes ensure all changes will be visible if this class is used correctly  "
          }
        ],
        "parent": "q_75128",
        "author": "ratchet freak"
      }
    ],
    "sent": [
      {
        "topicid": 4,
        "systemtopicid": 4,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "MultiThreading",
        "linePolarity": -0.45999999999999996,
        "sent": " here's a basic question about multi-threading in java: i have very big mutable data structure (a tree  to be exact) and i understand that if i want to modify this data structure concurrently from two different threads  i need to use locks and/or other kinds of thread safety.    however  in my case  the two threads don't need to modify the data structure  at the same time ; rather  thread a  which normally owns the data structure  should temporarily pass the latter to thread b  and thread b should pass the data structure back to thread a after having done some long-running modifications on it.    is it thread-safe to pass this mutable data structure back and forth between threads  if it's guaranteed that the threads do not modify the data at the same time?  "
      }
    ]
  },
  {
    "author": "ntoskrnl",
    "parent": "",
    "title": "\"Why is throwing a checked exception type allowed in this case?\"",
    "commentid": "q_87101",
    "date": "Jul 27, 2014 7:01:50 AM",
    "children": [
      {
        "date": "Jul 27, 2014 7:10:52 AM",
        "commentid": "a_106696",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.1319999999999999,
            "sent": " this is because of a change that was included in  project coin   introduced in java 7  to allow for general exception handling with rethrowing of the original exception. here is an example that works in java 7 but not java 6:           you can read the entire article explaining the changes  here .  "
          }
        ],
        "parent": "q_87101",
        "author": "Keppil"
      },
      {
        "date": "Jul 27, 2014 12:49:45 PM",
        "commentid": "a_106697",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.1319999999999999,
            "sent": " i think that the wording in  ?14.18 the   statement   that you refer to  is a mistake in the jls &mdash; text that should have been updated with java se 7  and was not.    the bit of jls text that describes the intended behavior is in  ?11.2.2 exception analysis of statements :        a   statement whose thrown expression is a final or effectively final exception parameter of a   clause c can throw an exception class e iff:            e is an exception class that the   block of the   statement which declares c can throw; and     e is assignment compatible with any of c's catchable exception classes; and     e is not assignment compatible with any of the catchable exception classes of the   clauses declared to the left of c in the same   statement.          the first bullet point is the relevant one; because the  -clause parameter   is effectively final (meaning that it's never assigned to or incremented or decremented; see  ?4.12.4   variables )    can only throw something that the   block could throw.    but as you say  the compile-time checking in ?14.18 does not make any allowance for this. ?11.2.2 does not decide what's allowed and what's not; rather  it's supposed to be an analysis of the consequences of the various restrictions on what can be thrown. (this analysis does feed back into more-normative parts of the spec &mdash; ?14.18 itself uses it in its second bullet point &mdash; but ?14.18 can't just say \\\"it's a compile-time error if it throws an exception it can't throw per ?11.2.2\\\"  because that would be circular.)    so i think ?14.18 needs to be adjusted to accommodate the intent of ?11.2.2.    good find!  "
          }
        ],
        "parent": "q_87101",
        "author": "ruakh"
      },
      {
        "date": "Jul 27, 2014 12:49:09 PM",
        "commentid": "a_106698",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.1319999999999999,
            "sent": " this behavior is described in detail in the jls in  11.2. compile-time checking of exceptions :         a   statement whose thrown expression is a final or effectively   final exception parameter of a   clause c can throw an exception   class e iff:               e is an exception class that the   block of the   statement which   declares c can throw ; and       e is assignment compatible with any of c's catchable exception   classes; and       e is not assignment compatible with any of the catchable exception   classes of the   clauses declared to the left of c in the same     statement.           (emphasis mine.)    your second example fails because   is not an \\\"exception parameter of a   clause\\\".  "
          }
        ],
        "parent": "q_87101",
        "author": "ntoskrnl"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.1319999999999999,
        "sent": " i noticed by accident that this   statement (extracted from some more complex code) compiles:         for a brief but happy moment i thought that checked exceptions had finally decided to just die already  but it still gets uppity at this:         the   block doesn't have to be empty; it seems it can have code so long as that code doesn't throw a checked exception. that seems reasonable  but my question is  what rule in the language specification describes this behavior? as far as i can see   ?14.18 the throw statement  explicitly forbids it  because the type of the   expression is a checked exception  and it's not caught or declared to be thrown. (?)  "
      }
    ]
  },
  {
    "author": "Didier L",
    "parent": "",
    "title": "\"Creating object using static keyword in Java\"",
    "commentid": "q_88693",
    "date": "Jul 24, 2014 12:20:38 AM",
    "children": [
      {
        "date": "Jul 24, 2014 12:38:15 AM",
        "commentid": "a_108801",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.8559999999999999,
            "sent": " jls  says :        the static initializers and class variable initializers are executed   in textual order  and may not refer to class variables declared in the   class whose declarations appear textually after the use  even though   these class variables are in scope (?8.3.2.3). this restriction is   designed to detect  at compile time  most circular or otherwise   malformed initializations.      which is exactly your case.    here is your original example:  http://ideone.com/pievbx  - static initializer of   goes  after  static instance of   is assigned - so static initializer can't be executed - it's textually after static variable initialization    let's move line 4  after  static initialization block -  http://ideone.com/em7nc1  :         now you can see the following output:         now initialization order is more like you expected - first called static initializer and then static instance of   is initialized in common manner.  "
          }
        ],
        "parent": "q_88693",
        "author": "Konstantin V. Salikhov"
      },
      {
        "date": "Jul 24, 2014 12:36:50 AM",
        "commentid": "a_108802",
        "sent": [
          {
            "topicid": 2,
            "systemtopicid": 2,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Loops",
            "linePolarity": -0.8559999999999999,
            "sent": " static fields initialization and static blocks are executed in the order they are declared. in your case  the code is equivalent to this after separating declaration and initialization:         so when you reach line 4 from your code  the static initialization is actually being executed and not finished yet. hence your constructor is called before   can be printed.  "
          }
        ],
        "parent": "q_88693",
        "author": "Didier L"
      }
    ],
    "sent": [
      {
        "topicid": 2,
        "systemtopicid": 2,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Loops",
        "linePolarity": -0.8559999999999999,
        "sent": "      when i wrote this code i am getting output in order like this:         here when i created a new object in   got loaded and   variables and blocks are executed in order they are written. when control came to line 4   instance initialization block is called. why? why is static block not called when a new object is created at line 4 and till that time static block was also not called even once  so according to convention static block should have been called. why is this unexpected output coming?  "
      }
    ]
  },
  {
    "author": "Syam S",
    "parent": "",
    "title": "\"Assigning to multilevel wildcards\"",
    "commentid": "q_94508",
    "date": "Jul 14, 2014 6:28:25 AM",
    "children": [
      {
        "date": "Jul 14, 2014 7:14:19 AM",
        "commentid": "a_115948",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.6680000000000001,
            "sent": " i will try to explain the java generics using two simple rules. these rules suffice to answer your question and are basically enough to remember for almost any case:      two generic types   and   are never assignable unless  . i.e.  generics are  invariant  by default.   wildcards allow the assignment of  :    to     to   iff   is assignable to   (apply rules recursively to   and  )   to   iff   is assignable to   (apply rules recursively to   and  )         case      in your example  you try to assign   to  . that is  in your case   and  . since these types are not equal  they are not assignable; they violate  rule 1 .     the question is  why doesn't the wildcard help? the answer is simple:  rule 2 is  not  transitive. i.e.    cannot be assinged to    there has to be a wildcard in the outermost level; otherwise rule 2 does not apply to the outermost level.    case      here  you got a wildcard in the outer type. because it is in the outer type  rule 2 kicks in:   is assignable to   (again  because of rule 2). therefore  this is legal.     general approach    here is how you can check any complex generic type: simply check each generic level by level using the two rules. start with the outermost level. once a level violates a rules  you know the assignment is illegal; if all levels adhere to the rules  then the assignment is legal. lets consider your types again:         is x assignable to y ?         is x assignable to z ?         simple rule to remember     each level of generic nesting either needs to be completely identical ( ) or   needs to contain a wildcard in this level.   "
          }
        ],
        "parent": "q_94508",
        "author": "gexicide"
      },
      {
        "date": "Jul 14, 2014 7:20:11 AM",
        "commentid": "a_115949",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.6680000000000001,
            "sent": " first of all  let's simplify the code by removing extra type parameter:         we know that   essentially means \\\"a   you can get   from\\\"  and   is the same thing as  .    so  types of the variables above can be explained as follows:      c3: \\\"a collection of  s that allow you to get  s from them   c4: \\\"a collection that allows you to get  s that allow you to get   from them      in particular  this explanation incurs the following:         now  it   were allowed  you can see that   would break type safety of  :       "
          }
        ],
        "parent": "q_94508",
        "author": "axtavt"
      },
      {
        "date": "Jul 14, 2014 9:18:59 AM",
        "commentid": "a_115950",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.6680000000000001,
            "sent": " the key thing to remember here is that  nested wildcards don't capture .    what this means is that the \\\"normal\\\" behavior you expect from top-level wildcards (i.e. wildcards stand for one  specific  something) doesn't apply to nested wildcards. instead  nested wildcards stand for  any  type.    for example  take this declaration:         this means   is a   of one  specific  type. easy.    but what about this?         this is  not  a   of  s of one specific type. this is a   of  s   each of which  is one specific type.    for example  you were expecting something like this to happen:         but consider this:         the type of   exactly matches the type parameter for    right? so shouldn't you be able to add   to    even though   isn't a  ?    also consider this:         what type should this return?   returns an    and   returns    which means...   returns a  . which is not the   you were expecting. why is that?    because  nested wildcards don't capture . and that is the key distinction here. the wildcard in   doesn't capture a single type \\\"overall\\\". it captures a single type  for each of the elements in the   .    thus  this is perfectly valid code:         keeping this in mind  let's look at your examples.            that's intuitively ok. the types exactly match  so   is assignable to  .            now  let's look back. a   isn't a   of  s of  s and a single unknown type. it's a   of  s  each of which is a pair of a   and some unknown type  which may or may not be the same type as another pair in the collection. so this is valid:         and because this is valid for    but shouldn't be valid for    assigning   to   isn't allowed  because it would allow you to put stuff into an   that isn't a  .            now  this is a bit more tricky. the top-level captures one  specific  type that extends  . because wildcards are supertypes of specific types (e.g.   is a supertype of  )    can be captured by    because the former extends the latter. thus  because   is assignment-compatible with    the assignment is valid.       as you can tell from the variety of answers here  there are multiple ways of explaining the behavior of nested wildcards. i was going for a bit more of an intuitive explanation  which i hope i achieved.  "
          }
        ],
        "parent": "q_94508",
        "author": "user3580294"
      },
      {
        "date": "Jul 14, 2014 7:10:13 AM",
        "commentid": "a_115951",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": 0.6680000000000001,
            "sent": " one of the advantages of java generics is stronger type checks at compile time. so anything declared in generics should exactly match. to make it a simple answer i would use some examples.    to initialize a list of numbers you could do like.         this technically means the list \\\"numbers\\\" can store any object that is either a number or sub-class of number. but we cannot initilaize this list with sub types. anything given in generic tags   should literally match the assignment. (java 7 provides a type inference though)          even though integer and double are subclasses of number the generics prevents from these initialization. the generic argument specified inside   should exactly match.     now if assignment is tightly bound how can the numbers list store sub-class objects? the answer is the   methods accepts   or anything that extends    where   is the generic type we gave. so in case of the list \\\"numbers\\\" e is   so the following statements are perfectly valid.         again one exception to this is the wildcards. the wildcard is used to accept any arguments. so the following statments are valid.         similarly         but now we cannot add anything to this list as the element should extend   and that is an unknown type and the only allowable element is    which is a member of every type. java does not infer this from the assignment. so the following result in error         also the wildcard inference happens only on one level. any nesting should again match exactly like usual generics. so          so this is your case          so you could do something like         case 4: when you say   its again first level. that means it checks whether the element under question extends  .          so   is similar to    "
          }
        ],
        "parent": "q_94508",
        "author": "Syam S"
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": 0.6680000000000001,
        "sent": " simple class:         and a few assignments:         why does bullet number three not compile while the fourth one is perfectly legal?    compiler error:       "
      }
    ]
  },
  {
    "author": "Edwin Buck",
    "parent": "",
    "title": "\"Can&#39;t get Maven to recognize Java 1.8\"",
    "commentid": "q_95516",
    "date": "Jul 11, 2014 1:19:24 PM",
    "children": [
      {
        "date": "Oct 29, 2014 9:33:07 AM",
        "commentid": "a_117219",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": 0.18000000000000016,
            "sent": " for me the issue was that when i installed from   the installation did not update the   directory link to the new 1.8 jdk. this may not have been the issue of the original question but it was the solution for me when i encountered the same error message.    i don't know why the installer doesn't link it and to make matters more confusing  the path is different depending on whether you installed it from oracle or apple. see  mac os x 10.6.7 java path current jdk confusing     i did the following to fix my environment       "
          }
        ],
        "parent": "q_95516",
        "author": "Kirby"
      },
      {
        "date": "Jul 11, 2014 1:56:38 PM",
        "commentid": "a_117220",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": 0.18000000000000016,
            "sent": " it turns out that i had a forgotten   file that had set the value of  .    for future readers  check the following locations for places that may override   (in ascending order of precedence):                               "
          }
        ],
        "parent": "q_95516",
        "author": "Mike"
      },
      {
        "date": "Jul 11, 2014 1:39:26 PM",
        "commentid": "a_117221",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": 0.18000000000000016,
            "sent": " your   is being overridden from somewhere else. try echoing in   right before java invocation.  "
          }
        ],
        "parent": "q_95516",
        "author": "Jigar Joshi"
      },
      {
        "date": "Nov 22, 2014 8:41:35 AM",
        "commentid": "a_117222",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": 0.18000000000000016,
            "sent": " the solution which worked for me:    in eclipse  i had a wrong run configuration in my maven build. go to         run configurations -> jre [tab]      and set the runtime to java 8.  "
          }
        ],
        "parent": "q_95516",
        "author": "Bevor"
      },
      {
        "date": "Jul 11, 2014 1:36:49 PM",
        "commentid": "a_117223",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": 0.18000000000000016,
            "sent": " you are using an early access release of 1.8.  now that is has been released  you should install the released version.    check you maven installation to see if you have a configuration associated with it that specifies a particular jdk.  occasionally there are wrapper scripts which reset java_home prior to maven launch  or the maven executable is launched in a wrapper that refers to a config file that could bind you to a particular jvm.  "
          }
        ],
        "parent": "q_95516",
        "author": "Edwin Buck"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": 0.18000000000000016,
        "sent": " i can't seem to be able to get maven to use java 1.8. using 1.8 as the target turns up the following error:         the cause of the error is obvious enough: maven isn't using the right version of java:         but the installed version of java should be 1.8:         and java_home is set:         i also tried the  command here  (which creates  ). i've tried restarting the computer several times  and have verified that the env var is correctly set (it's set in  ).    maven was installed with homebrew.    java 1.8 is working fine in eclipse (which is using m2e). i just can't get maven to work on the command line.  "
      }
    ]
  },
  {
    "author": "Petter",
    "parent": "",
    "title": "\"What are the &#39;shadow$_klass_&#39; and &#39;shadow$_monitor_&#39; variables for in java.lang.Object?\"",
    "commentid": "q_25652",
    "date": "Nov 14, 2014 8:55:33 AM",
    "children": [
      {
        "date": "Nov 21, 2014 3:14:16 AM",
        "commentid": "a_30933",
        "sent": [
          {
            "topicid": 8,
            "systemtopicid": 8,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Database",
            "linePolarity": 0.0,
            "sent": " they are indeed connected to gc. they seem to have been added in order to support brooks pointers. i found some information on brooks pointers  here :        the idea is that each object on the heap has one additional reference field. this field either points to the object itself  or  as soon as the object gets copied to a new location  to that new location. this will enable us to evacuate objects concurrently with mutator threads      see especially these two commits:     libcore: a7c69f785f7d1b07b7da22cfb9150c584ee143f4      art: 9d04a20bde1b1855cefc64aebc1a44e253b1a13b   "
          }
        ],
        "parent": "q_25652",
        "author": "Petter"
      }
    ],
    "sent": [
      {
        "topicid": 8,
        "systemtopicid": 8,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Database",
        "linePolarity": 0.0,
        "sent": " in the latest android update (sdk 21)  it appears that 2 new variables have been added to java.lang.object:         i notice that   is briefly used in  :         but otherwise there are no references to them.  are they somehow related to gc in art? or some sort of native stuff?  "
      }
    ]
  },
  {
    "author": "Nick L.",
    "parent": "",
    "title": "\"Cast to unimplemented interface compiles\"",
    "commentid": "q_1995",
    "date": "Dec 25, 2014 10:33:27 AM",
    "children": [
      {
        "date": "Dec 25, 2014 10:42:41 AM",
        "commentid": "a_2361",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3999999999999999,
            "sent": " the reason the compiler accepts this is explained in the  jls section 5.5.1  (relevant part in bold):        given a compile-time reference type s (source) and a compile-time reference type t (target)  a casting conversion exists from s to t if no compile-time errors occur due to the following rules.        if s is a class type:            if t is a class type  then either |s| &lt;: |t|  or |t| &lt;: |s|. otherwise  a compile-time error occurs.            furthermore  if there exists a supertype x of t  and a supertype y of s  such that both x and y are provably distinct parameterized types (?4.5)  and that the erasures of x and y are the same  a compile-time error occurs.             if t is an interface type:              if s is not a final class (?8.1.1)  then  if there exists a supertype x of t  and a supertype y of s  such that both x and y are provably distinct parameterized types  and that the erasures of x and y are the same  a compile-time error occurs.          otherwise  the cast is always legal at compile time (because even if s does not implement t  a subclass of s might).                 in your case  a   will be thrown at runtime since   cannot be converted to  . but until the program executes  the compiler allows the cast because there may be a subclass of   that implements  .  "
          }
        ],
        "parent": "q_1995",
        "author": "manouti"
      },
      {
        "date": "Dec 25, 2014 10:36:24 AM",
        "commentid": "a_2362",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3999999999999999,
            "sent": " actually  in java it is  perfectly valid  to cast one  related type  to another (even if the casting makes little sense). you will get an error during runtime if the types are not  compatible .    for example :         compiles fine but gives   during runtime  "
          }
        ],
        "parent": "q_1995",
        "author": "TheLostMind"
      },
      {
        "date": "Dec 25, 2014 10:37:03 AM",
        "commentid": "a_2363",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3999999999999999,
            "sent": " line 1 will throw a runtime exception. the compiler does not check at compile time if the cast can succesfully be done. this is why it is sometimes advisable to check this first with the   operator.    in general  you can always cast a variable   of type   to any interface    because there may exist a class    or a class      the same is not true for casting a variable   of class   to any class    because there cannot exist a subclass    because a class may not extend multiple classes.  "
          }
        ],
        "parent": "q_1995",
        "author": "popovitsj"
      },
      {
        "date": "Dec 25, 2014 10:47:14 AM",
        "commentid": "a_2364",
        "sent": [
          {
            "topicid": 7,
            "systemtopicid": 7,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Inheritance",
            "linePolarity": -0.3999999999999999,
            "sent": " practically  on line one the code explicitly \\\"tells\\\" the compiler that the object on the right side is an object of type on the left side (this is called  ). this is valid against any two types in java in compile time  but if types are unrelated  there is going to be a runtime error (an   will be thrown).    on line two  what is done is valid in both compile time and runtime  because   is of whatever type it extends and/or implements  so practically  phone is a   (and a  ).  "
          }
        ],
        "parent": "q_1995",
        "author": "Nick L."
      }
    ],
    "sent": [
      {
        "topicid": 7,
        "systemtopicid": 7,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Inheritance",
        "linePolarity": -0.3999999999999999,
        "sent": " i am not sure i understand code on line 1 below?         i understand line2 since phone implements talkable  but device and talkable are unrelated  how can line1 be legal?  "
      }
    ]
  },
  {
    "author": "PeterMmm",
    "parent": "",
    "title": "\"When is length used as a method and when as property in Java?\"",
    "commentid": "q_6196",
    "date": "Dec 16, 2014 11:00:32 AM",
    "children": [
      {
        "date": "Dec 16, 2014 11:08:15 AM",
        "commentid": "a_7511",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.768,
            "sent": "     how would java developers select which of [the methods] to implement in their classes?      when you implement classes that contain other objects  it's almost always going to be      the method provided by the  interface.    as far as other choices go  you should avoid exposing member variables  even final ones  because they cannot be accessed through an interface. java gets away with it for arrays because of some jvm trickery  but you cannot do the same. hence    should be out: it remains in java because it's not possible to change something that fundamental that has been in the language from day one  but it's definitely not something one should consider when designing new classes.    when you implement your own type that has length (say  a rectangle or a line segment) you should prefer   to   because of java beans naming conventions.  "
          }
        ],
        "parent": "q_6196",
        "author": "dasblinkenlight"
      },
      {
        "date": "Dec 16, 2014 11:05:33 AM",
        "commentid": "a_7512",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.768,
            "sent": "     obviously it depends of the object type and the api is there for read...      you already have answered your question yourself: look in the api documentation of whatever class you are using.        but the point is how the java development select which of that implements in their classes.      the classes in java's standard library have been developed over a long period of time by different people  which do not always make the same choice for the name of methods  so there are inconsistencies and unfortunately you'll just have to live with that.  "
          }
        ],
        "parent": "q_6196",
        "author": "Jesper"
      },
      {
        "date": "Dec 16, 2014 11:17:35 AM",
        "commentid": "a_7513",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.768,
            "sent": " there is no clear rule  otherwise we wouldn't see such a mixup in the jdk itself. but here are some things to consider when making such a design decision.       don't worry to much. it is a minor thing and won't make to much of a difference. so when you think longer then 5 minutes about this  you are probably wasting money already.     use getters when a frameworks need them. many frameworks depend on the getter style. if you need or want such frameworks to work nicely with your class it might be beneficial to use that style.     shorter is better. the 'get' part doesn't increase clarity. it just generates to characters of noise to the source code  so if you don't need it for some reason  don't use it.     methods are easier to evolve. length is often a quantity that is not set directly but somehow computed. if you hide that behind a method it gives you the flexibility to change that implementation later on  without changing the api.     direct field accesses should be a tiny bit faster  but if you aren't working on high volume online trading or something  the difference isn't even worth thinking about. and if you do you should do your own measurements before making a decision. the hotspot compiler will almost for sure inline the method call anyways.       so if there aren't any external forces driving you in a different direction i would go with 'length()'  "
          }
        ],
        "parent": "q_6196",
        "author": "Jens Schauder"
      },
      {
        "date": "Dec 16, 2014 11:16:25 AM",
        "commentid": "a_7514",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.768,
            "sent": " according to oops principles  length should be attribute and getlength() should be method. also length attribute should be encapsulated should be exposed through methods  so getlength() sounds more appropriate.     unfortunately not all java library classes follow standards. there are some exceptions and this is one among them.  "
          }
        ],
        "parent": "q_6196",
        "author": "Pranalee"
      },
      {
        "date": "Dec 16, 2014 11:06:07 AM",
        "commentid": "a_7515",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": -0.768,
            "sent": " in a pure oo language it should be probably always a method like  . so in a class hierarchy you can override the attribute length.    but java is not pure oo. and the main reason for fields (.length) vs method (length()) is/was performance issues.     and even sun/oracle programmers did some bad class design.  "
          }
        ],
        "parent": "q_6196",
        "author": "PeterMmm"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": -0.768,
        "sent": " i've seen some legacy code that uses  property on some objects and others that uses   method. currently i'm working with a   from the   package and i found that it have the   method to get the numbers of elements.     my question is how as java developer i can know how to determine when to use           ? obviously it depends of the object type and the api is there for read... but the point is how the java development select which of that implements in their classes.     note:  in the question  when to use .length vs .length()  makoto answer's indicates that   is a property on arrays. that isn't a method call  and   is a method call on string. but  why is the reason? why not use ever a method or ever a property for maintain the consistency around all the api.  "
      }
    ]
  },
  {
    "author": "Marv",
    "parent": "",
    "title": "\"Is multiplying by 0.0000001 the same as dividing by 10000000?\"",
    "commentid": "q_6872",
    "date": "Dec 15, 2014 8:22:29 AM",
    "children": [
      {
        "date": "Dec 15, 2014 8:24:53 AM",
        "commentid": "a_8427",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " no it's not the same for the reason you mentioned. here's an example:         using a    we can see the difference between the two values:         ouput:       "
          }
        ],
        "parent": "q_6872",
        "author": "manouti"
      },
      {
        "date": "Dec 15, 2014 8:26:43 AM",
        "commentid": "a_8428",
        "sent": [
          {
            "topicid": 5,
            "systemtopicid": 5,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Variables",
            "linePolarity": 0.6680000000000001,
            "sent": " it's not only not the same because of the double representation but also if you multiply an integer by a double the result is a double. if you devide an integer by an integer  the result is an integer:                  prints        "
          }
        ],
        "parent": "q_6872",
        "author": "Marv"
      }
    ],
    "sent": [
      {
        "topicid": 5,
        "systemtopicid": 5,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Variables",
        "linePolarity": 0.6680000000000001,
        "sent": " in java  is multiplying a double by 0.0000001 the same as dividing it by 10000000? my intuition is that there could be a difference because 0.0000001 cannot be represented exactly in a double.  "
      }
    ]
  },
  {
    "author": "EJK",
    "parent": "",
    "title": "\"Is there a way to &quot;alias&quot; one class in Java for another?\"",
    "commentid": "q_8623",
    "date": "Dec 11, 2014 3:05:05 PM",
    "children": [
      {
        "date": "Dec 11, 2014 3:14:50 PM",
        "commentid": "a_10588",
        "sent": [
          {
            "topicid": 9,
            "systemtopicid": 9,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Eclipse",
            "linePolarity": -1.284,
            "sent": " i suggest using  jarjar .  this is a tool that will allow you to repackage classes in a library.  you can run this against the latest version of the tool and move the someenum class to a package that does not conflict with the plugins.    the  getting started  doc has an example  with jaxen.jar  that looks to be relevant to your situation.  "
          }
        ],
        "parent": "q_8623",
        "author": "EJK"
      }
    ],
    "sent": [
      {
        "topicid": 9,
        "systemtopicid": 9,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Eclipse",
        "linePolarity": -1.284,
        "sent": " i have the following api compatibility problem and looking for the ways to solve it.      tl;dr is there a way to create an \\\"alias\\\" for a class in java? i.e. some trick to make   to be an alias for  ?     long story. (i'm anonimizing it a bit to avoid fingerpointing.)    i'm working with a powerful java tool which also supports plugins. there's no strictly defined  public  api (in a sense of what you can and can't touch)  just usual private/protected/package/public classes  methods and fields. there are defined extension points (like  extend   class)  but then you'll have access to a wide area of the tool's internals.    in the recent minor version update (like    ->  ) tool developers have moved one enum class to another package -   became  . i think it was thought just as a trivial refactoring  not as a serios restructuring.    this class  however  seems to have been used by a number of plugins. the result is that these plugins are now incompatible with the latest version. most of the plugins are quite useful  but not actively maintained. people wrote them years ago - and they just worked over the years  with dozens of the version updates. so this has a potential of negative implact on the tool's plugin ecosystem.       my question is  if there's some way in java to create an \\\"alias\\\"   for  ? this would allow old plugins to continute working with the new version of the tool.    some classloader trick? in javascript that would've been trivial to shim  but in java?    why  i  am asking this. i'm an author of the maven plugin which wraps the tool in question. so i could easily add my sugar to this coffee  like classloaders and so on. if there is a technical way to make this work  i'd be in position to save most of the tool's plugins ecosystem - at least for maven users.    i've contacted the tool's vendor on this  but not sure of the success.    just to make it clear -  i  am  not  the vendor of the tool in question. i (a) write plugins for the tools (and have no big trouble updating  my  plugins) and (b) am an author of the   which allows execution the tool in the maven builds. i also consult a lot on the tool and care about its ecosystem (there are a lot of very useful plugins).        update     end of story: developers of the tool took my points into account  and decided to revert the change. kudos for that!  "
      }
    ]
  },
  {
    "author": "hgrey",
    "parent": "",
    "title": "\"Abstract methods and the varargs annotation\"",
    "commentid": "q_21608",
    "date": "Nov 20, 2014 5:51:20 PM",
    "children": [
      {
        "date": "Nov 26, 2014 3:02:51 PM",
        "commentid": "a_26196",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": -0.8759999999999999,
            "sent": " i don't know    but in java varargs is just an array.     so in java it will work this way  with one warning:         may be if you can fool   the same way  you will overcome the bug.  "
          }
        ],
        "parent": "q_21608",
        "author": "Suzan Cioc"
      },
      {
        "date": "Nov 28, 2014 10:02:46 AM",
        "commentid": "a_26197",
        "sent": [
          {
            "topicid": 6,
            "systemtopicid": 6,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "JSON",
            "linePolarity": -0.8759999999999999,
            "sent": " one way to achieve what you want is to define interface in java as follows         and then to define the implementation normally in scala as follows         scala compiler will recognize that it needs to generate vargs style method and will generate both varargs and seq implementation.    javap output is as you would expect       "
          }
        ],
        "parent": "q_21608",
        "author": "hgrey"
      }
    ],
    "sent": [
      {
        "topicid": 6,
        "systemtopicid": 6,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "JSON",
        "linePolarity": -0.8759999999999999,
        "sent": " scala provides a    annotation  that generates a java varargs forwarder method  which makes it possible to write something like this:         and then to call this method from java without needing to create a  :         which is pretty nice.    unfortunately the forwarding part doesn't seem to happen when the method is abstract:         now if we have this java code:         we get this exception (at runtime):         we can confirm the problem with  :         so nope  the forwarder definitely isn't getting implemented.    putting the annotation on both   methods fails to compile:         and of course putting the annotation only on the   in   means we can't use the forwarder from a   instance.    this seems like it must be a bug  but it also seems extremely easy to run into  and i'm not seeing anything in the issue tracker. am i using   correctly? if so  is there a workaround that would make it do what you'd expect here?  "
      }
    ]
  },
  {
    "author": "shikjohari",
    "parent": "",
    "title": "\"How a jar file gets executed\"",
    "commentid": "q_30220",
    "date": "Nov 7, 2014 12:11:55 AM",
    "children": [
      {
        "date": "Nov 7, 2014 12:15:40 AM",
        "commentid": "a_36364",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -0.6680000000000001,
            "sent": " the jvm is capable of loading classes or files from a jar file without extracting the jar to temp files.    this functionality is also available to you in the standard library  see the     for more information.    so no  the jvm does not extract a jar to temp files  classes (and resources) are simply loaded on demand.    a jar file is basically a zip file with a predefined entry   (this is only mandatory in case of an executable jar). this   entry (file) contains some information read by the jvm. more on the manifest files:     working with manifest files: the basics     in case of an executable jar the manifest file also contains the main class that should be loaded and whose   method to be called in order to start the application. the   manifest entry specifies the main class:       "
          }
        ],
        "parent": "q_30220",
        "author": "icza"
      },
      {
        "date": "Nov 7, 2014 1:23:19 AM",
        "commentid": "a_36365",
        "sent": [
          {
            "topicid": 10,
            "systemtopicid": 10,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Network",
            "linePolarity": -0.6680000000000001,
            "sent": " no the jvm extracts the jar file to the memory and not to the file. it reads the manifest.mf inside meta-inf which has an entry for the main class. jvm looks for the public static void main class inside this main class. this is how jvm finds the main class and executes the executable jar files  "
          }
        ],
        "parent": "q_30220",
        "author": "shikjohari"
      }
    ],
    "sent": [
      {
        "topicid": 10,
        "systemtopicid": 10,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Network",
        "linePolarity": -0.6680000000000001,
        "sent": " how does the jvm execute classes inside the jar? does it extract the contents of jar to a temp location and then executes the classes?  "
      }
    ]
  },
  {
    "author": "Evgeniy Dorofeev",
    "parent": "",
    "title": "\"No ClassCastException is thrown inside Java generics\"",
    "commentid": "q_33423",
    "date": "Nov 2, 2014 6:21:41 AM",
    "children": [
      {
        "date": "Nov 2, 2014 6:35:38 AM",
        "commentid": "a_40126",
        "sent": [
          {
            "topicid": 1,
            "systemtopicid": 1,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Android",
            "linePolarity": -0.28400000000000003,
            "sent": "   is effectively erased during compilation. see  here :        generics were introduced to the java language to provide tighter type   checks at compile time and to support generic programming. to   implement generics  the java compiler applies type erasure to:            replace all type parameters in generic types with their bounds or   object if the type parameters are unbounded. the produced bytecode    therefore  contains only ordinary classes  interfaces  and methods.     insert type casts if necessary to preserve type safety. generate   bridge methods to preserve polymorphism in extended generic types.     type erasure ensures that no new classes are created for parameterized   types; consequently  generics incur no runtime overhead.          so your   gets   erased into ca. the following:         which does obviously not produce any  .      is a different story  it results into the following:         which produces the   when trying to cast   to  .    please see the  type erasure  part of the  generics tutorial .  "
          }
        ],
        "parent": "q_33423",
        "author": "lexicore"
      },
      {
        "date": "Nov 2, 2014 6:35:03 AM",
        "commentid": "a_40127",
        "sent": [
          {
            "topicid": 1,
            "systemtopicid": 1,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Android",
            "linePolarity": -0.28400000000000003,
            "sent": " well  since the compiler erases the generic type parameters  the casting inside the method is essentially equivalent to :         which is not a problem  regardless of what you pass to your method (since any object can be cast to object).    however  when you try to assign that object to a string  in your main method  the   occurs  since   cannot be cast to a  .  "
          }
        ],
        "parent": "q_33423",
        "author": "Eran"
      },
      {
        "date": "Nov 2, 2014 6:35:29 AM",
        "commentid": "a_40128",
        "sent": [
          {
            "topicid": 1,
            "systemtopicid": 1,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Android",
            "linePolarity": -0.28400000000000003,
            "sent": " all the generic types are erased in the compiled code. as far as the compiled code is concerned    just returns an  . however  your   method tries to assign it to a    and it is not a    so that produces a  .     http://en.wikipedia.org/wiki/type_erasure   "
          }
        ],
        "parent": "q_33423",
        "author": "khelwood"
      },
      {
        "date": "Nov 2, 2014 6:40:23 AM",
        "commentid": "a_40129",
        "sent": [
          {
            "topicid": 1,
            "systemtopicid": 1,
            "sentimentwords": [],
            "sentid": "",
            "systemlabel": "Android",
            "linePolarity": -0.28400000000000003,
            "sent": " this is because of generic type erasure          is translated by compiler into       "
          }
        ],
        "parent": "q_33423",
        "author": "Evgeniy Dorofeev"
      }
    ],
    "sent": [
      {
        "topicid": 1,
        "systemtopicid": 1,
        "sentimentwords": [],
        "sentid": "",
        "systemlabel": "Android",
        "linePolarity": -0.28400000000000003,
        "sent": " below is the first java generics i've ever written :         the result is  \\\"exception outside casttoanothertype()\\\" . why did the exception not occur inside the generic method?  "
    }
    ]
  }
]
}